{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a65fb3-f1a5-4de8-b54d-67346fa35c8a",
   "metadata": {},
   "source": [
    " # Part I: Preparing Datasets for Fine-tuning and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1905e4a5",
   "metadata": {},
   "source": [
    "The following code cell imports necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-requirements",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required Python packages for this notebook environment\n",
    "%pip install \\\n",
    "  huggingface_hub \\\n",
    "  \"transformers>=4.36.0\" \\\n",
    "  peft \\\n",
    "  datasets \\\n",
    "  trl \\\n",
    "  jsonschema \\\n",
    "  litellm \\\n",
    "  \"jinja2>=3.1.0\" \\\n",
    "  \"torch>=2.0.0\" \\\n",
    "  openai \\\n",
    "  jupyterlab \\\n",
    "  requests \\\n",
    "  python-dotenv\n",
    "\n",
    "# Note: print_status function is defined in the next cell\n",
    "print(\"‚úÖ Package installation completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6193d10c-583c-4a78-8459-66e7ec2bbab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from pprint import pprint\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "def print_status(message):\n",
    "    \"\"\"Print a status message with a checkmark emoji.\"\"\"\n",
    "    print(f\"‚úÖ {message}\")\n",
    "\n",
    "print_status(\"Imports completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc25e7",
   "metadata": {},
   "source": [
    "The following code cell sets a random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc7314b-0f86-4ed4-bdc1-3d2598092cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "# Limits to at most N tool properties\n",
    "LIMIT_TOOL_PROPERTIES = 8\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "print_status(\"Random seed configuration completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfab9962",
   "metadata": {},
   "source": [
    "The following code cell defines the data root directory and creates necessary directories for storing processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6434b0c-9ad0-403e-9cf4-d9ab22306769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed data will be stored here\n",
    "DATA_ROOT = os.path.join(os.getcwd(), \"data\")\n",
    "CUSTOMIZATION_DATA_ROOT = os.path.join(DATA_ROOT, \"customization\")\n",
    "VALIDATION_DATA_ROOT = os.path.join(DATA_ROOT, \"validation\")\n",
    "EVALUATION_DATA_ROOT = os.path.join(DATA_ROOT, \"evaluation\")\n",
    "\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(CUSTOMIZATION_DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(VALIDATION_DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(EVALUATION_DATA_ROOT, exist_ok=True)\n",
    "\n",
    "print_status(\"Data directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2556c45-8c14-4ea2-a2c2-7937c9902b2d",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"step-1\"></a>\n",
    "## Step 1: Download xLAM Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db1f5a8-c741-4657-b6c9-56afd9dccc94",
   "metadata": {},
   "source": [
    "This step loads the xLAM dataset from Hugging Face.\n",
    "\n",
    "Ensure that you have followed the prerequisites mentioned in the associated README, obtained a Hugging Face access token, and configured it in [config.py](./config.py). In addition to getting an access token, you need to apply for access to the xLAM dataset on its [page](https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k), which will be approved instantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f729c88-e633-4914-b3b3-09311ad79a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import HF_TOKEN\n",
    "\n",
    "# Set environment variables for Hugging Face\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://huggingface.co\"\n",
    "\n",
    "# Note: We'll pass the token directly to load_dataset() in the next cell\n",
    "# This is more reliable than using login() which can have issues with environment variables\n",
    "\n",
    "print_status(\"Hugging Face configuration completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d19368-0c33-4595-bea8-24ef645eaaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download from Hugging Face\n",
    "# Ensure environment variables are set and pass token explicitly\n",
    "from config import HF_TOKEN\n",
    "import os\n",
    "\n",
    "# Make sure environment variables are set (in case cell 10 wasn't run)\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://huggingface.co\"\n",
    "\n",
    "# Load dataset with explicit token\n",
    "dataset = load_dataset(\"Salesforce/xlam-function-calling-60k\", token=HF_TOKEN)\n",
    "\n",
    "# Inspect a sample\n",
    "example = dataset['train'][0]\n",
    "pprint(example)\n",
    "\n",
    "print_status(\"xLAM dataset downloaded and inspected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caa142c-b051-42ad-ad24-aabdd05ea391",
   "metadata": {},
   "source": [
    "For more details on the structure of this data, refer to the [data structure of the xLAM dataset](https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k#structure) in the Hugging Face documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f41f50-2142-4de5-80ee-22e10a868559",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"step-2\"></a>\n",
    "## Step 2: Prepare Data for Customization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc98b800-5b23-48a4-983b-dc4797e7d496",
   "metadata": {},
   "source": [
    "For Customization, the NeMo Microservices platform leverages the OpenAI data format, comprised of `messages` and `tools`:\n",
    "\n",
    "* `messages` include the `user` query, as well as the ground truth `assistant` response to the query. This response contains the function name(s) and associated argument(s) in a \"tool_calls\" dict.\n",
    "* `tools` include a list of functions and parameters available to the LLM to choose from, as well as their descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f81d2a-ece1-4be2-abe3-e6404e473665",
   "metadata": {},
   "source": [
    "The following is an example of the data format:\n",
    "```\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Where can I find live giveaways for beta access and games?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"tool_calls\": [\n",
    "                {\n",
    "                    \"id\": \"call_beta\",\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": \"live_giveaways_by_type\",\n",
    "                        \"arguments\": {\"type\": \"beta\"}\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"id\": \"call_game\",\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": \"live_giveaways_by_type\",\n",
    "                        \"arguments\": {\"type\": \"game\"}\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"live_giveaways_by_type\",\n",
    "                \"description\": \"Retrieve live giveaways from the GamerPower API based on the specified type.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"type\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The type of giveaways to retrieve (e.g., game, loot, beta).\",\n",
    "                            \"default\": \"game\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": []\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb67d5-f95b-47f4-99bc-7a2ad4117f7a",
   "metadata": {},
   "source": [
    "The following helper functions convert a single xLAM JSON data point into OpenAI format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fc5c3e-0f4b-4423-845e-5c32865283e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_type(param_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize Python type hints and parameter definitions to OpenAI function spec types.\n",
    "\n",
    "    Args:\n",
    "        param_type: Type string that could include default values or complex types\n",
    "\n",
    "    Returns:\n",
    "        Normalized type string according to OpenAI function spec\n",
    "    \"\"\"\n",
    "    # Remove whitespace\n",
    "    param_type = param_type.strip()\n",
    "\n",
    "    # Handle types with default values (e.g. \"str, default='London'\")\n",
    "    if \",\" in param_type and \"default\" in param_type:\n",
    "        param_type = param_type.split(\",\")[0].strip()\n",
    "\n",
    "    # Handle types with just default values (e.g. \"default='London'\")\n",
    "    if param_type.startswith(\"default=\"):\n",
    "        return \"string\"  # Default to string if only default value is given\n",
    "\n",
    "    # Remove \", optional\" suffix if present\n",
    "    param_type = param_type.replace(\", optional\", \"\").strip()\n",
    "\n",
    "    # Handle complex types\n",
    "    if param_type.startswith(\"Callable\"):\n",
    "        return \"string\"  # Represent callable as string in JSON schema\n",
    "    if param_type.startswith(\"Tuple\"):\n",
    "        return \"array\"  # Represent tuple as array in JSON schema\n",
    "    if param_type.startswith(\"List[\"):\n",
    "        return \"array\"\n",
    "    if param_type.startswith(\"Set\") or param_type == \"set\":\n",
    "        return \"array\"  # Represent set as array in JSON schema\n",
    "\n",
    "    # Map common type variations to OpenAI spec types\n",
    "    type_mapping: Dict[str, str] = {\n",
    "        \"str\": \"string\",\n",
    "        \"int\": \"integer\",\n",
    "        \"float\": \"number\",\n",
    "        \"bool\": \"boolean\",\n",
    "        \"list\": \"array\",\n",
    "        \"dict\": \"object\",\n",
    "        \"List\": \"array\",\n",
    "        \"Dict\": \"object\",\n",
    "        \"set\": \"array\",\n",
    "        \"Set\": \"array\"\n",
    "    }\n",
    "\n",
    "    if param_type in type_mapping:\n",
    "        return type_mapping[param_type]\n",
    "    else:\n",
    "        print(f\"Unknown type: {param_type}\")\n",
    "        return \"string\"  # Default to string for unknown types\n",
    "\n",
    "\n",
    "def convert_tools_to_openai_spec(tools: Union[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:\n",
    "    # If tools is a string, try to parse it as JSON\n",
    "    if isinstance(tools, str):\n",
    "        try:\n",
    "            tools = json.loads(tools)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to parse tools string as JSON: {e}\")\n",
    "            return []\n",
    "\n",
    "    # Ensure tools is a list\n",
    "    if not isinstance(tools, list):\n",
    "        print(f\"Expected tools to be a list, but got {type(tools)}\")\n",
    "        return []\n",
    "\n",
    "    openai_tools: List[Dict[str, Any]] = []\n",
    "    for tool in tools:\n",
    "        # Check if tool is a dictionary\n",
    "        if not isinstance(tool, dict):\n",
    "            print(f\"Expected tool to be a dictionary, but got {type(tool)}\")\n",
    "            continue\n",
    "\n",
    "        # Check if 'parameters' is a dictionary\n",
    "        if not isinstance(tool.get(\"parameters\"), dict):\n",
    "            print(f\"Expected 'parameters' to be a dictionary, but got {type(tool.get('parameters'))} for tool: {tool}\")\n",
    "            continue\n",
    "\n",
    "    \n",
    "\n",
    "        normalized_parameters: Dict[str, Dict[str, Any]] = {}\n",
    "        for param_name, param_info in tool[\"parameters\"].items():\n",
    "            if not isinstance(param_info, dict):\n",
    "                print(\n",
    "                    f\"Expected parameter info to be a dictionary, but got {type(param_info)} for parameter: {param_name}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Create parameter info without default first\n",
    "            param_dict = {\n",
    "                \"description\": param_info.get(\"description\", \"\"),\n",
    "                \"type\": normalize_type(param_info.get(\"type\", \"\")),\n",
    "            }\n",
    "\n",
    "            # Only add default if it exists, is not None, and is not an empty string\n",
    "            default_value = param_info.get(\"default\")\n",
    "            if default_value is not None and default_value != \"\":\n",
    "                param_dict[\"default\"] = default_value\n",
    "\n",
    "            normalized_parameters[param_name] = param_dict\n",
    "\n",
    "        openai_tool = {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool[\"name\"],\n",
    "                \"description\": tool[\"description\"],\n",
    "                \"parameters\": {\"type\": \"object\", \"properties\": normalized_parameters},\n",
    "            },\n",
    "        }\n",
    "        openai_tools.append(openai_tool)\n",
    "    return openai_tools\n",
    "\n",
    "\n",
    "def save_jsonl(filename, data):\n",
    "    \"\"\"Write a list of json objects to a .jsonl file\"\"\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        for entry in data:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "\n",
    "def convert_tool_calls(xlam_tools):\n",
    "    \"\"\"Convert XLAM tool format to OpenAI's tool schema.\"\"\"\n",
    "    tools = []\n",
    "    for tool in json.loads(xlam_tools):\n",
    "        tools.append({\"type\": \"function\", \"function\": {\"name\": tool[\"name\"], \"arguments\": tool.get(\"arguments\", {})}})\n",
    "    return tools\n",
    "\n",
    "\n",
    "def convert_example(example, dataset_type='single'):\n",
    "    \"\"\"Convert an XLAM dataset example to OpenAI format.\"\"\"\n",
    "    obj = {\"messages\": []}\n",
    "\n",
    "    # User message\n",
    "    obj[\"messages\"].append({\"role\": \"user\", \"content\": example[\"query\"]})\n",
    "\n",
    "    # Tools\n",
    "    if example.get(\"tools\"):\n",
    "        obj[\"tools\"] = convert_tools_to_openai_spec(example[\"tools\"])\n",
    "\n",
    "    # Assistant message\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": \"\"}\n",
    "    if example.get(\"answers\"):\n",
    "        tool_calls = convert_tool_calls(example[\"answers\"])\n",
    "        \n",
    "        if dataset_type == \"single\":\n",
    "            # Only include examples with a single tool call\n",
    "            if len(tool_calls) == 1:\n",
    "                assistant_message[\"tool_calls\"] = tool_calls\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            # For other dataset types, include all tool calls\n",
    "            assistant_message[\"tool_calls\"] = tool_calls\n",
    "                \n",
    "    obj[\"messages\"].append(assistant_message)\n",
    "\n",
    "    return obj\n",
    "\n",
    "print_status(\"Data conversion functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b514a",
   "metadata": {},
   "source": [
    "The following code cell converts the example data to the OpenAI format required by NeMo Customizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3366c584-bf5f-47f2-b0ea-aa421c3d083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_example(example)\n",
    "\n",
    "print_status(\"Example conversion completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62515958-e98d-475b-9fda-18890d353813",
   "metadata": {},
   "source": [
    "**NOTE**: The `convert_example` function by default only retains data points that have exactly one `tool_call` in the output.\n",
    "The `llama-3.2-1b-instruct` model does not support parallel tool calls.\n",
    "For more information, refer to the [supported models](https://docs.nvidia.com/nim/large-language-models/latest/function-calling.html#supported-models) in the NeMo documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab76d74a-5df2-443f-aff6-0feff0e06285",
   "metadata": {},
   "source": [
    "### Process Entire Dataset\n",
    "Convert each example by looping through the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706c81c0-3900-4157-9366-afa10a142c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_examples = []\n",
    "with open(os.path.join(DATA_ROOT, \"xlam_openai_format.jsonl\"), \"w\") as f:\n",
    "    for example in dataset[\"train\"]:\n",
    "        converted = convert_example(example)\n",
    "        if converted is not None:\n",
    "            all_examples.append(converted)\n",
    "            f.write(json.dumps(converted) + \"\\n\")\n",
    "\n",
    "print_status(\"Dataset conversion completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b7d931-6c33-4d8f-8a57-e188aacc7616",
   "metadata": {},
   "source": [
    "### Split Dataset\n",
    "This step splits the dataset into a train, validation, and test set.\n",
    "For demonstration, we use a smaller subset of all the examples.\n",
    "You may choose to modify `NUM_EXAMPLES` to leverage a larger subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8be515e-4255-4682-8499-a30c71f129e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure to change the size of dataset to use\n",
    "NUM_EXAMPLES = 5000\n",
    "\n",
    "assert NUM_EXAMPLES <= len(all_examples), f\"{NUM_EXAMPLES} exceeds the total number of available ({len(all_examples)}) data points\"\n",
    "\n",
    "print_status(\"Dataset size configuration validated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc6072-0b85-45be-b3b1-fe9a1c26d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly choose a subset\n",
    "sampled_examples = random.sample(all_examples, NUM_EXAMPLES)\n",
    "\n",
    "# Split into 70% training, 15% validation, 15% testing\n",
    "train_size = int(0.7 * len(sampled_examples))\n",
    "val_size = int(0.15 * len(sampled_examples))\n",
    "\n",
    "train_data = sampled_examples[:train_size]\n",
    "val_data = sampled_examples[train_size : train_size + val_size]\n",
    "test_data = sampled_examples[train_size + val_size :]\n",
    "\n",
    "# Save the training and validation splits. We will use test split in the next section\n",
    "save_jsonl(os.path.join(CUSTOMIZATION_DATA_ROOT, \"training.jsonl\"), train_data)\n",
    "save_jsonl(os.path.join(VALIDATION_DATA_ROOT,\"validation.jsonl\"), val_data)\n",
    "\n",
    "print_status(\"Dataset split and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74afdfed-4af3-4deb-90df-081f00300714",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"step-3\"></a>\n",
    "## Step 3: Prepare Data for Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69f2094-2afb-458d-8c9d-526f1270c017",
   "metadata": {},
   "source": [
    "For evaluation, the NeMo Microservices platform uses a format with a minor modification to the OpenAI format. This requires `tools_calls` to be brought out of `messages` to create a distinct parallel field.\n",
    "\n",
    "* `messages` includes the `user` query\n",
    "* `tools` includes a list of functions and parameters available to the LLM to choose from, as well as their descriptions.\n",
    "* `tool_calls` is the ground truth response to the user query. This response contains the function name(s) and associated argument(s) in a \"tool_calls\" dict.\n",
    "\n",
    "Here is an example -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afc1448-b09f-426d-81eb-52122a961d80",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Where can I find live giveaways for beta access?\"\n",
    "        },\n",
    "    ],\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"live_giveaways_by_type\",\n",
    "                \"description\": \"Retrieve live giveaways from the GamerPower API based on the specified type.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"type\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The type of giveaways to retrieve (e.g., game, loot, beta).\",\n",
    "                            \"default\": \"game\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": []\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"tool_calls\": [\n",
    "        {\n",
    "            \"id\": \"call_beta\",\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"live_giveaways_by_type\",\n",
    "                \"arguments\": {\"type\": \"beta\"}\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6424507e-4097-49bb-b750-96a66f9abd77",
   "metadata": {},
   "source": [
    "The following steps transform the test dataset into a format compatible with the NeMo Evaluator microservice.\n",
    "This dataset is for measuring accuracy metrics before and after customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6487d80-36a0-4314-8c21-93744043ce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_eval(entry):\n",
    "    \"\"\"Convert a single entry in the dataset to the evaluator format\"\"\"\n",
    "\n",
    "    # Note: This is a WAR for a known bug with tool calling in NIM\n",
    "    for tool in entry[\"tools\"]:\n",
    "        if len(tool[\"function\"][\"parameters\"][\"properties\"]) > LIMIT_TOOL_PROPERTIES:\n",
    "            return None\n",
    "    \n",
    "    new_entry = {\n",
    "        \"messages\": [],\n",
    "        \"tools\": entry[\"tools\"],\n",
    "        \"tool_calls\": []\n",
    "    }\n",
    "    \n",
    "    for msg in entry[\"messages\"]:\n",
    "        if msg[\"role\"] == \"assistant\" and \"tool_calls\" in msg:\n",
    "            new_entry[\"tool_calls\"] = msg[\"tool_calls\"]\n",
    "        else:\n",
    "            new_entry[\"messages\"].append(msg)\n",
    "    \n",
    "    return new_entry\n",
    "\n",
    "print_status(\"Evaluation conversion function defined\")\n",
    "\n",
    "def convert_dataset_eval(data):\n",
    "    \"\"\"Convert the entire dataset for evaluation by restructuring the data format.\"\"\"\n",
    "    return [result for entry in data if (result := convert_example_eval(entry)) is not None]\n",
    "\n",
    "print_status(\"Evaluation conversion functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0873c030-56b8-4366-9984-24c310327d26",
   "metadata": {},
   "source": [
    "`NOTE:` We have implemented a workaround for a known bug where tool calls freeze the NIM if a tool description includes a function with a larger number of parameters. As such, we have limited the dataset to use examples with available tools having at most 8 parameters. This will be resolved in the next NIM release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6028d6c5-7a67-4b63-a1ef-e93e0177893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_eval = convert_dataset_eval(test_data)\n",
    "save_jsonl(os.path.join(EVALUATION_DATA_ROOT, \"xlam-test-single.jsonl\"), test_data_eval)\n",
    "\n",
    "print_status(\"Evaluation dataset prepared and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675acc69-4d0c-481c-b093-1f1b849019fa",
   "metadata": {},
   "source": [
    "# Part II: LoRA Fine-tuning Using NeMo Customizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96adf453-5f61-435c-9ed2-257cdcebe24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "print_status(\"Part II imports completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd6ee1a-0789-4724-8f12-5157363ebf9c",
   "metadata": {},
   "source": [
    "### Configure NeMo Microservices Endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa34fe25",
   "metadata": {},
   "source": [
    "This section includes importing required libraries, configuring endpoints, and performing health checks to ensure that the NeMo Data Store, NIM, and other services are running correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29578e1-6757-4e89-8f29-a0f6739015c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "\n",
    "print(f\"Data Store endpoint: {NDS_URL}\")\n",
    "print(f\"Entity Store endpoint: {ENTITY_STORE_URL}\")\n",
    "print(f\"Customizer endpoint: {CUSTOMIZER_URL}\")\n",
    "print(f\"Evaluator endpoint: {EVALUATOR_URL}\")\n",
    "print(f\"Guardrails endpoint: {GUARDRAILS_URL}\")\n",
    "print(f\"NIM endpoint: {NIM_URL}\")\n",
    "print(f\"Namespace: {NMS_NAMESPACE}\")\n",
    "print(f\"Base Model for Customization: {BASE_MODEL}\")\n",
    "\n",
    "print_status(\"NeMo Microservices endpoints configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bf28dc",
   "metadata": {},
   "source": [
    "### Configure Path to Prepared data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8510bfcb",
   "metadata": {},
   "source": [
    "The following code sets the paths to the prepared dataset files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b905bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where data preparation notebook saved finetuning and evaluation data\n",
    "DATA_ROOT = os.path.join(os.getcwd(), \"data\")\n",
    "CUSTOMIZATION_DATA_ROOT = os.path.join(DATA_ROOT, \"customization\")\n",
    "VALIDATION_DATA_ROOT = os.path.join(DATA_ROOT, \"validation\")\n",
    "EVALUATION_DATA_ROOT = os.path.join(DATA_ROOT, \"evaluation\")\n",
    "\n",
    "# Sanity checks\n",
    "train_fp = f\"{CUSTOMIZATION_DATA_ROOT}/training.jsonl\"\n",
    "assert os.path.exists(train_fp), f\"The training data at '{train_fp}' does not exist. Please ensure that the data was prepared successfully.\"\n",
    "\n",
    "val_fp = f\"{VALIDATION_DATA_ROOT}/validation.jsonl\"\n",
    "assert os.path.exists(val_fp), f\"The validation data at '{val_fp}' does not exist. Please ensure that the data was prepared successfully.\"\n",
    "\n",
    "test_fp = f\"{EVALUATION_DATA_ROOT}/xlam-test-single.jsonl\"\n",
    "assert os.path.exists(test_fp), f\"The test data at '{test_fp}' does not exist. Please ensure that the data was prepared successfully.\"\n",
    "\n",
    "print_status(\"Data paths validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"step-3\"></a>\n",
    "## Step 3: Sanity Test the Customized Model By Running Sample Inference\n",
    "\n",
    "Once the model is customized, its adapter is automatically saved in NeMo Entity Store and is ready to be picked up by NVIDIA NIM.\n",
    "You can test the model by sending a prompt to its NIM endpoint.\n",
    "\n",
    "First, choose one of the examples from the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062c7805-727f-4265-9248-5babc4a32fc5",
   "metadata": {},
   "source": [
    "### Resource Organization Using Namespace\n",
    "\n",
    "You can use a [namespace](https://developer.nvidia.com/docs/nemo-microservices/manage-entities/namespaces/index.html) to isolate and organize the artifacts in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e57ea9a-05b0-4e5e-9455-fc7c2c515ca1",
   "metadata": {},
   "source": [
    "#### Create Namespace\n",
    "\n",
    "Both Data Store and Entity Store use namespaces. The following code creates namespaces for the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff1528-2c7c-4118-8921-f54167ba57ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_namespaces(entity_host, ds_host, namespace):\n",
    "    # Create namespace in Entity Store\n",
    "    entity_store_url = f\"{entity_host}/v1/namespaces\"\n",
    "    resp = requests.post(entity_store_url, json={\"id\": namespace})\n",
    "    assert resp.status_code in (200, 201, 409, 422), \\\n",
    "        f\"Unexpected response from Entity Store during namespace creation: {resp.status_code}\"\n",
    "    print(resp)\n",
    "\n",
    "    # Create namespace in Data Store\n",
    "    nds_url = f\"{ds_host}/v1/datastore/namespaces\"\n",
    "    resp = requests.post(nds_url, data={\"namespace\": namespace})\n",
    "    assert resp.status_code in (200, 201, 409, 422), \\\n",
    "        f\"Unexpected response from Data Store during namespace creation: {resp.status_code}\"\n",
    "    print(resp)\n",
    "\n",
    "print_status(\"Namespace creation function defined\")\n",
    "\n",
    "create_namespaces(entity_host=ENTITY_STORE_URL, ds_host=NDS_URL, namespace=NMS_NAMESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057acae8-4d0b-48ee-bd8f-6f98bd941ba6",
   "metadata": {},
   "source": [
    "#### Verify Namespaces\n",
    "\n",
    "The following [Data Store API](https://developer.nvidia.com/docs/nemo-microservices/api/datastore.html) and [Entity Store API](https://developer.nvidia.com/docs/nemo-microservices/api/entity-store.html) list the namespace created in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413166f0-a314-4db0-a341-914495b583ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verify Namespace in Data Store\n",
    "response = requests.get(f\"{NDS_URL}/v1/datastore/namespaces/{NMS_NAMESPACE}\")\n",
    "print(f\"Status Code: {response.status_code}\\nResponse JSON: {response.json()}\")\n",
    "\n",
    "# Verify Namespace in Entity Store\n",
    "response = requests.get(f\"{ENTITY_STORE_URL}/v1/namespaces/{NMS_NAMESPACE}\")\n",
    "print(f\"Status Code: {response.status_code}\\nResponse JSON: {response.json()}\")\n",
    "\n",
    "print_status(\"Namespaces verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556e6df9-bb6b-4606-80da-e936f68a5d23",
   "metadata": {},
   "source": [
    "**Tips**:\n",
    "* You may generally use `{DATASTORE_HOST}/v1/datastore/namespaces/` and `{ENTITYSTORE_HOST}/v1/namespaces/` GET APIs to list **all** available namespaces.\n",
    "* Send DELETE requests to `{DATASTORE_HOST}/v1/datastore/namespaces/{namespace}` and `{ENTITYSTORE_HOST}/v1/namespaces/{namespace}` APIs to delete a namespace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e83d04-e092-49bd-8769-3422526e135b",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"step-1\"></a>\n",
    "## Step 1: Upload Data to NeMo Data Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbdb147-9994-4e08-a742-38098f4dc911",
   "metadata": {},
   "source": [
    "The NeMo Data Store supports data management using the Hugging Face `HfApi` Client. \n",
    "\n",
    "**Note that this step does not interact with Hugging Face at all, it just uses the client library to interact with NeMo Data Store.** This is in comparison to the previous notebook, where we used the `load_dataset` API to download the xLAM dataset from Hugging Face's repository.\n",
    "\n",
    "More information can be found in [documentation](https://developer.nvidia.com/docs/nemo-microservices/manage-entities/tutorials/manage-dataset-files.html#set-up-hugging-face-client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29d515c-bd7d-4fdb-9c6d-42370d975e69",
   "metadata": {},
   "source": [
    "### 1.1 Create Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a1178a-b516-4f50-96a3-2dee114f9cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = f\"{NMS_NAMESPACE}/{DATASET_NAME}\"\n",
    "\n",
    "print_status(\"Repository ID configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd240212-3272-419e-b19b-d31d8aede2b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T14:02:46.351447Z",
     "start_time": "2025-02-28T14:02:46.343156Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub.utils import HfHubHTTPError\n",
    "\n",
    "hf_api = HfApi(endpoint=f\"{NDS_URL}/v1/hf\", token=\"\")\n",
    "\n",
    "# Create repo (or use existing if it already exists)\n",
    "try:\n",
    "    hf_api.create_repo(\n",
    "        repo_id=repo_id,\n",
    "        repo_type='dataset',\n",
    "        exist_ok=True  # Don't raise error if repo already exists\n",
    "    )\n",
    "    print(f\"‚úÖ Dataset repository '{repo_id}' created or already exists\")\n",
    "except HfHubHTTPError as e:\n",
    "    # Handle 409 Conflict (repo already exists) as success\n",
    "    if e.response is not None and e.response.status_code == 409:\n",
    "        print(f\"‚ÑπÔ∏è  Dataset repository '{repo_id}' already exists (this is fine)\")\n",
    "    else:\n",
    "        # Re-raise other HTTP errors\n",
    "        print(f\"‚ùå Error creating repository: {e}\")\n",
    "        raise\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected error creating repository: {e}\")\n",
    "    raise\n",
    "\n",
    "print_status(\"Dataset repository created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ac352a-31b9-4144-ad0f-699fcceebfc2",
   "metadata": {},
   "source": [
    "Next, creating a dataset programmatically requires two steps: uploading and registration. More information can be found in [documentation](https://developer.nvidia.com/docs/nemo-microservices/manage-entities/datasets/create-dataset.html#how-to-create-a-dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f927b0-b216-46f9-a8d0-0f9d6836868f",
   "metadata": {},
   "source": [
    "### 1.2 Upload Dataset Files to NeMo Data Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50be5cec-5df6-4092-82ff-29d39aceaa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_api.upload_file(path_or_fileobj=train_fp,\n",
    "    path_in_repo=\"training/training.jsonl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    ")\n",
    "\n",
    "hf_api.upload_file(path_or_fileobj=val_fp,\n",
    "    path_in_repo=\"validation/validation.jsonl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    ")\n",
    "\n",
    "hf_api.upload_file(path_or_fileobj=test_fp,\n",
    "    path_in_repo=\"testing/xlam-test-single.jsonl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    ")\n",
    "\n",
    "print_status(\"Dataset files uploaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5d0ab1-69b2-4ca5-9a25-d514cf72b7cd",
   "metadata": {},
   "source": [
    "Other tips:\n",
    "* Take a look at the `path_in_repo` argument above. If there are more than one files in the subfolders:\n",
    "    * All the .jsonl files in `training/` will be merged and used for training by customizer.\n",
    "    * All the .jsonl files in `validation/` will be merged and used for validation by customizer.\n",
    "* NeMo Data Store generally supports data management using the [HfApi API](https://huggingface.co/docs/huggingface_hub/en/package_reference/hf_api). For example, to delete a repo, you may use - \n",
    "```python\n",
    "   hf_api.delete_repo(\n",
    "     repo_id=repo_id,\n",
    "     repo_type=\"dataset\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371c044-56df-412d-9fb3-7e0e191dd3a8",
   "metadata": {},
   "source": [
    "### 1.3 Register the Dataset with NeMo Entity Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab2d1d-17d5-4c7a-9b1b-8f8b1c68204c",
   "metadata": {},
   "source": [
    "To use a dataset for operations such as evaluations and customizations, register a dataset using the `/v1/datasets` endpoint.\n",
    "Register the dataset to refer to it by its namespace and name afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a45fadd-df51-48e0-b30c-336c5bca071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.post(\n",
    "    url=f\"{ENTITY_STORE_URL}/v1/datasets\",\n",
    "    json={\n",
    "        \"name\": DATASET_NAME,\n",
    "        \"namespace\": NMS_NAMESPACE,\n",
    "        \"description\": \"Tool calling xLAM dataset in OpenAI ChatCompletions format\",\n",
    "        \"files_url\": f\"hf://datasets/{NMS_NAMESPACE}/{DATASET_NAME}\",\n",
    "        \"project\": \"tool_calling\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Handle different response statuses\n",
    "if resp.status_code in (200, 201):\n",
    "    print(f\"‚úÖ Dataset '{NMS_NAMESPACE}/{DATASET_NAME}' created successfully\")\n",
    "    dataset_response = resp.json()\n",
    "    print(\"Dataset Response:\")\n",
    "    print(json.dumps(dataset_response, indent=2))\n",
    "    # Also return it so Jupyter displays it\n",
    "    dataset_response\n",
    "elif resp.status_code == 409:\n",
    "    # Dataset already exists - this is fine, fetch it instead\n",
    "    print(f\"‚ÑπÔ∏è  Dataset '{NMS_NAMESPACE}/{DATASET_NAME}' already exists (this is fine)\")\n",
    "    print(\"Fetching existing dataset...\")\n",
    "    get_resp = requests.get(url=f\"{ENTITY_STORE_URL}/v1/datasets/{NMS_NAMESPACE}/{DATASET_NAME}\")\n",
    "    if get_resp.status_code == 200:\n",
    "        dataset_response = get_resp.json()\n",
    "        print(\"Existing Dataset:\")\n",
    "        print(json.dumps(dataset_response, indent=2))\n",
    "        # Also return it so Jupyter displays it\n",
    "        dataset_response\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Could not fetch existing dataset: {get_resp.status_code}\")\n",
    "        print(f\"Response: {get_resp.text}\")\n",
    "else:\n",
    "    # Other error - raise exception\n",
    "    print(f\"‚ùå Error creating dataset: Status {resp.status_code}\")\n",
    "    print(f\"Response: {resp.text}\")\n",
    "    raise Exception(f\"Failed to create dataset: Status {resp.status_code}, Response: {resp.text}\")\n",
    "\n",
    "print_status(\"Dataset registered with Entity Store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b58c7-30d7-4c04-b3a5-872032dbf4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check to validate dataset\n",
    "res = requests.get(url=f\"{ENTITY_STORE_URL}/v1/datasets/{NMS_NAMESPACE}/{DATASET_NAME}\")\n",
    "assert res.status_code in (200, 201), f\"Status Code {res.status_code} Failed to fetch dataset {res.text}\"\n",
    "dataset_obj = res.json()\n",
    "\n",
    "print(\"Files URL:\", dataset_obj[\"files_url\"])\n",
    "assert dataset_obj[\"files_url\"] == f\"hf://datasets/{repo_id}\"\n",
    "\n",
    "print_status(\"Dataset validation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7cebbc-8a5f-492d-820c-b3fbbed6fafb",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"step-2\"></a>\n",
    "## 2. LoRA Customization with NeMo Customizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6915c1f5-9687-443f-92cd-cd30b55fc501",
   "metadata": {},
   "source": [
    "### 2.1 Start the Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e22fa48",
   "metadata": {},
   "source": [
    "\n",
    "Start the training job by sending a POST request to the `/v1/customization/jobs` endpoint.\n",
    "The following code sets the training parameters and sends the request.\n",
    "\n",
    " **The training job will take approximately 45 minutes to complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9678c-2785-4e95-b11b-1f41067bc920",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"wandb-api-key\": WANDB_API_KEY} if WANDB_API_KEY else None\n",
    "\n",
    "training_params = {\n",
    "    \"name\": \"llama-3.2-1b-xlam-ft\",\n",
    "    \"output_model\": f\"{NMS_NAMESPACE}/llama-3.2-1b-xlam-run1\",\n",
    "    \"config\": f\"{BASE_MODEL}@{BASE_MODEL_VERSION}\",\n",
    "    \"dataset\": {\"name\": DATASET_NAME, \"namespace\" : NMS_NAMESPACE},\n",
    "    \"hyperparameters\": {\n",
    "        \"training_type\": \"sft\",\n",
    "        \"finetuning_type\": \"lora\",\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 8,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"lora\": {\n",
    "            \"adapter_dim\": 32,\n",
    "            \"adapter_dropout\": 0.1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create training job with retry logic\n",
    "max_retries = 3\n",
    "retry_delay = 2\n",
    "\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        print(f\"Attempting to create training job (attempt {attempt + 1}/{max_retries})...\")\n",
    "        resp = requests.post(\n",
    "            f\"{CUSTOMIZER_URL}/v1/customization/jobs\", \n",
    "            json=training_params, \n",
    "            headers=headers,\n",
    "            timeout=30  # 30 second timeout\n",
    "        )\n",
    "        \n",
    "        # Check response status\n",
    "        if resp.status_code not in (200, 201):\n",
    "            print(f\"‚ùå Error creating training job: Status {resp.status_code}\")\n",
    "            print(f\"Response: {resp.text}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "                continue\n",
    "            raise Exception(f\"Failed to create training job: {resp.text}\")\n",
    "        \n",
    "        # Success!\n",
    "        customization = resp.json()\n",
    "        \n",
    "        # Explicitly print the customization response\n",
    "        print(\"‚úÖ Training job created successfully!\")\n",
    "        print(\"\\nCustomization Response:\")\n",
    "        print(json.dumps(customization, indent=2))\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        # Also return it so Jupyter displays it\n",
    "        customization\n",
    "        \n",
    "        print_status(\"Training job created\")\n",
    "        break\n",
    "        \n",
    "    except (requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:\n",
    "        print(f\"‚ö†Ô∏è Connection error (attempt {attempt + 1}/{max_retries}): {e}\")\n",
    "        if attempt < max_retries - 1:\n",
    "            print(f\"Retrying in {retry_delay} seconds...\")\n",
    "            print(\"üí° Tip: Make sure port-forwards are running!\")\n",
    "            time.sleep(retry_delay)\n",
    "        else:\n",
    "            raise Exception(f\"Failed to connect to Customizer after {max_retries} attempts. Check port-forwards!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69131ef8",
   "metadata": {},
   "source": [
    "The following code sets variables for storing the job ID and customized model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eb5ae6-9b3e-4915-8242-34b65b0c0680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To track status\n",
    "JOB_ID = customization[\"id\"]\n",
    "\n",
    "# This will be the name of the model that will be used to send inference queries to\n",
    "CUSTOMIZED_MODEL = customization[\"output_model\"]\n",
    "\n",
    "print_status(\"Job ID and customized model name stored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9169153d-2ff0-4b34-9a3d-236ecedb7a5d",
   "metadata": {},
   "source": [
    "**Tips**:\n",
    "* If you configured the NeMo Customizer microservice with your own [Weights & Biases (WandB)](https://wandb.ai/) API key, you can find the training graphs and logs in your WandB account, \"nvidia-nemo-customizer\" project. Your run ID is similar to your customization `JOB_ID`.\n",
    "  \n",
    "* To cancel a job that you scheduled incorrectly, run the following code.\n",
    "  \n",
    "  ```python\n",
    "  requests.post(f\"{CUSTOMIZER_URL}/v1/customization/jobs/{JOB_ID}/cancel\")\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab1b3b7-5e0d-4bbb-a7cb-476e0abb298b",
   "metadata": {},
   "source": [
    "### 2.2 Get Job Status\n",
    "\n",
    "Get the job status by sending a GET request to the `/v1/customization/jobs/{JOB_ID}/status` endpoint.\n",
    "The following code sets the job ID and sends the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f28fb-fb8e-4d57-88d7-6c09699523e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = requests.get(f\"{CUSTOMIZER_URL}/v1/customization/jobs/{JOB_ID}/status\")\n",
    "\n",
    "assert response.status_code == 200, (\n",
    "    f\"Status Code {response.status_code}: Failed to get job status. Response: {response.text}\"\n",
    ")\n",
    "print(\"Response JSON:\", json.dumps(response.json(), indent=4))\n",
    "\n",
    "print_status(\"Job status retrieved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b721be-8ca0-4e8f-99a7-5eb12ea1b47f",
   "metadata": {},
   "source": [
    "**IMPORTANT:** Monitor the job status. Ensure training is completed before proceeding by observing the `percentage_done` key in the response frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec56031-ec55-4cec-9f65-b34b87818e01",
   "metadata": {},
   "source": [
    "### 2.3 Validate Availability of Custom Model\n",
    "The following NeMo Entity Store API should display the model when the training job is complete.\n",
    "The list below shows all models filtered by your namespace and sorted by the latest first.\n",
    "For more information about this API, see the [NeMo Entity Store API reference](https://developer.nvidia.com/docs/nemo-microservices/api/entity-store.html).\n",
    "With the following code, you can find all customized models, including the one trained in the previous cells.\n",
    "Look for the `name` fields in the output, which should match your `CUSTOMIZED_MODEL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebad3944-70d5-4b23-9a38-a83774bf20c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = requests.get(f\"{ENTITY_STORE_URL}/v1/models\", params={\"filter[namespace]\": NMS_NAMESPACE, \"sort\" : \"-created_at\"})\n",
    "\n",
    "assert response.status_code == 200, f\"Status Code {response.status_code}: Request failed. Response: {response.text}\"\n",
    "print(\"Response JSON:\", json.dumps(response.json(), indent=4))\n",
    "\n",
    "print_status(\"Job status retrieved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1221f1e-04f6-4ceb-a844-e9ea1e94f0d0",
   "metadata": {},
   "source": [
    "**Tips**:\n",
    "\n",
    "* You can also find the model with its name directly:\n",
    "  ```python\n",
    "    # To get specifically the custom model, you may use the following API -\n",
    "    response = requests.get(f\"{ENTITY_STORE_URL}/v1/models/{CUSTOMIZED_MODEL}\")\n",
    "    \n",
    "    assert response.status_code == 200, f\"Status Code {response.status_code}: Request failed. Response: {response.text}\"\n",
    "    print(\"Response JSON:\", json.dumps(response.json(), indent=4))\n",
    "  ```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b43d7dc-f442-41cb-a743-7fd5efd4bf6c",
   "metadata": {},
   "source": [
    "NVIDIA NIM directly picks up the LoRA adapters from NeMo Entity Store. You can also query the NIM endpoint to look for it, as shown in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43026f8a-3b98-4aa6-b4c6-7441862863fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the custom LoRA model is available in Entity Store\n",
    "# Note: Custom LoRA models are registered in Entity Store, not directly in NIM's model list\n",
    "response = requests.get(f\"{ENTITY_STORE_URL}/v1/models\", params={\"filter[namespace]\": NMS_NAMESPACE, \"sort\": \"-created_at\"})\n",
    "\n",
    "assert response.status_code == 200, f\"Status Code {response.status_code}: Request failed. Response: {response.text}\"\n",
    "\n",
    "models_data = response.json().get(\"data\", [])\n",
    "# Extract model names (can be in 'name' or 'id' field)\n",
    "model_names = []\n",
    "for model in models_data:\n",
    "    # Try 'name' first, then 'id'\n",
    "    model_name = model.get(\"name\") or model.get(\"id\", \"\")\n",
    "    if model_name:\n",
    "        model_names.append(model_name)\n",
    "\n",
    "print(f\"Found {len(model_names)} models in namespace '{NMS_NAMESPACE}':\")\n",
    "for name in model_names[:10]:  # Show first 10\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "# Extract just the model name part (without namespace) for comparison\n",
    "# CUSTOMIZED_MODEL format: \"namespace/model-name\" or just \"model-name\"\n",
    "customized_model_name = CUSTOMIZED_MODEL.split(\"/\")[-1] if \"/\" in CUSTOMIZED_MODEL else CUSTOMIZED_MODEL\n",
    "\n",
    "# Check if our custom model is in the list (compare both with and without namespace)\n",
    "model_found = False\n",
    "for model_name in model_names:\n",
    "    # Compare both full name and just the model part (without namespace)\n",
    "    model_name_only = model_name.split(\"/\")[-1] if \"/\" in model_name else model_name\n",
    "    if CUSTOMIZED_MODEL == model_name or customized_model_name == model_name_only:\n",
    "        model_found = True\n",
    "        print(f\"\\n‚úÖ Custom model found in Entity Store!\")\n",
    "        print(f\"   Full name: {CUSTOMIZED_MODEL}\")\n",
    "        print(f\"   Entity Store name: {model_name}\")\n",
    "        break\n",
    "\n",
    "if not model_found:\n",
    "    print(f\"\\n‚ö†Ô∏è Custom model '{CUSTOMIZED_MODEL}' not found in the list.\")\n",
    "    print(\"This is normal if the training job is still running or just completed.\")\n",
    "    print(\"The model will appear in Entity Store once training completes and the model is uploaded.\")\n",
    "    \n",
    "    # Try to get the model directly by name (it might exist but not be in the list)\n",
    "    try:\n",
    "        direct_response = requests.get(f\"{ENTITY_STORE_URL}/v1/models/{CUSTOMIZED_MODEL}\")\n",
    "        if direct_response.status_code == 200:\n",
    "            print(f\"‚úÖ However, the model is accessible directly at: {CUSTOMIZED_MODEL}\")\n",
    "            print(\"This means training completed and the model is available!\")\n",
    "            model_found = True\n",
    "        else:\n",
    "            print(f\"‚è≥ Model not yet available. Training may still be in progress.\")\n",
    "            print(f\"   Check training job status to see if it's completed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Could not check model directly: {e}\")\n",
    "\n",
    "print_status(\"Custom model availability checked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df0f9c5-98b2-476e-aa2f-3af69c5e45f7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"step-3\"></a>\n",
    "## Step 3: Sanity Test the Customized Model By Running Sample Inference\n",
    "\n",
    "Once the model is customized, its adapter is automatically saved in NeMo Entity Store and is ready to be picked up by NVIDIA NIM.\n",
    "You can test the model by sending a prompt to its NIM endpoint.\n",
    "\n",
    "First, choose one of the examples from the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf4ada2-2ccb-4f82-bb7c-23134e67a7e0",
   "metadata": {},
   "source": [
    "### 3.1 Get Test Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f7da82-cbbb-43a4-999b-2adad2cea227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path):\n",
    "    \"\"\"Reads a JSON Lines file and yields parsed JSON objects\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()  # Remove leading/trailing whitespace\n",
    "            if not line:\n",
    "                continue  # Skip empty lines\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "test_data = list(read_jsonl(test_fp))\n",
    "\n",
    "print(f\"There are {len(test_data)} examples in the test set\")\n",
    "\n",
    "print_status(\"Test data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f74f62-8427-438e-b1df-15fe53ce330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly choose\n",
    "test_sample = random.choice(test_data)\n",
    "\n",
    "# Visualize the inputs to the LLM - user query and available tools\n",
    "test_sample['messages'], test_sample['tools']\n",
    "\n",
    "print_status(\"Test sample inspected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8577c4b-e0f6-4596-9bbc-33c47ca424aa",
   "metadata": {},
   "source": [
    "### 3.2 Send an Inference Call to NIM\n",
    "\n",
    "NIM exposes an OpenAI-compatible completions API endpoint, which you can query using the `OpenAI` client library as shown in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83f794-f8cc-4cc8-8d42-a031dfb7f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, check if the custom model is available in NIM\n",
    "print(f\"üîç Checking if custom model is available in NIM: {CUSTOMIZED_MODEL}\")\n",
    "nim_models_resp = requests.get(f\"{NIM_URL}/v1/models\")\n",
    "if nim_models_resp.status_code == 200:\n",
    "    nim_models = nim_models_resp.json().get(\"data\", [])\n",
    "    nim_model_ids = [m.get(\"id\") for m in nim_models]\n",
    "    print(f\"üìã Available models in NIM: {len(nim_model_ids)}\")\n",
    "    for model_id in nim_model_ids[:5]:  # Show first 5\n",
    "        print(f\"   - {model_id}\")\n",
    "    \n",
    "    if CUSTOMIZED_MODEL not in nim_model_ids:\n",
    "        print(f\"\\n‚ö†Ô∏è  Warning: Custom model '{CUSTOMIZED_MODEL}' is not yet loaded in NIM\")\n",
    "        print(\"Available custom models in NIM:\")\n",
    "        custom_models = [m for m in nim_model_ids if CUSTOMIZED_MODEL.split('/')[-1].split('@')[0] in m]\n",
    "        if custom_models:\n",
    "            print(f\"   Found similar models: {custom_models}\")\n",
    "            print(f\"\\nüí° Options:\")\n",
    "            print(f\"   1. Use one of the available models: {custom_models[0] if custom_models else 'None'}\")\n",
    "            print(f\"   2. Wait for the model to be loaded into NIM (this happens automatically)\")\n",
    "            print(f\"   3. Use the base model for now: {BASE_MODEL}\")\n",
    "            \n",
    "            # Try to use the first available custom model if it exists\n",
    "            if custom_models:\n",
    "                print(f\"\\nüîÑ Using available custom model: {custom_models[0]}\")\n",
    "                model_to_use = custom_models[0]\n",
    "            else:\n",
    "                print(f\"\\nüîÑ Falling back to base model: {BASE_MODEL}\")\n",
    "                model_to_use = BASE_MODEL\n",
    "        else:\n",
    "            print(f\"\\nüîÑ Custom model not loaded yet. Using base model: {BASE_MODEL}\")\n",
    "            model_to_use = BASE_MODEL\n",
    "    else:\n",
    "        print(f\"‚úÖ Custom model is available in NIM!\")\n",
    "        model_to_use = CUSTOMIZED_MODEL\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Could not check NIM models: {nim_models_resp.status_code}\")\n",
    "    print(f\"   Using requested model: {CUSTOMIZED_MODEL}\")\n",
    "    model_to_use = CUSTOMIZED_MODEL\n",
    "\n",
    "print(f\"\\nüöÄ Creating inference client and sending request...\")\n",
    "inference_client = OpenAI(\n",
    "  base_url = f\"{NIM_URL}/v1\",\n",
    "  api_key = \"None\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    completion = inference_client.chat.completions.create(\n",
    "      model = model_to_use,\n",
    "      messages = test_sample[\"messages\"],\n",
    "      tools = test_sample[\"tools\"],\n",
    "      tool_choice = 'auto',\n",
    "      temperature = 0.1,\n",
    "      top_p = 0.7,\n",
    "      max_tokens = 512,\n",
    "      stream = False\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Inference successful using model: {model_to_use}\")\n",
    "    if completion.choices[0].message.tool_calls:\n",
    "        print(f\"üìä Tool calls: {len(completion.choices[0].message.tool_calls)}\")\n",
    "        completion.choices[0].message.tool_calls\n",
    "    else:\n",
    "        print(\"üìä No tool calls in response\")\n",
    "        completion.choices[0].message\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during inference: {e}\")\n",
    "    print(f\"\\nüí° Troubleshooting:\")\n",
    "    print(f\"   1. Check if NIM service is running: curl {NIM_URL}/health\")\n",
    "    print(f\"   2. Verify model is loaded: curl {NIM_URL}/v1/models\")\n",
    "    print(f\"   3. Check if model exists in Entity Store\")\n",
    "    raise\n",
    "\n",
    "print_status(\"Custom model inference completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459bb488-3bd9-4092-9c3a-f77f904606e4",
   "metadata": {},
   "source": [
    "Given that the fine-tuning job was successful, you can get an inference result comparable to the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbdaedf-6fb4-41d6-8297-b7480c907892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ground truth answer\n",
    "test_sample['tool_calls']\n",
    "\n",
    "print_status(\"Ground truth tool calls retrieved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f0fce4-3169-4ac4-8785-8a8f15959594",
   "metadata": {},
   "source": [
    "### 3.3 Take Note of Your Custom Model Name\n",
    "\n",
    "Take note of your custom model name, as you will use it to run evaluations in the subsequent notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36398168-1051-4e54-ac3e-7a3e406f393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Name of your custom model is: {CUSTOMIZED_MODEL}\")\n",
    "\n",
    "print_status(\"Custom model inference completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3051ba77-9d36-477c-9969-b07c4b776b6f",
   "metadata": {},
   "source": [
    "# Part III: Model Evaluation Using NeMo Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb3981-0171-4080-96e8-245954b63dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from time import sleep, time\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "print_status(\"Part III imports completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162f9282-abec-401d-9143-076a3977d9a9",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"step-1\"></a>\n",
    "## Step 1: Establish Baseline Accuracy Benchmark\n",
    "\n",
    "First, we‚Äôll assess the accuracy of the 'off-the-shelf' base model‚Äîpristine, untouched, and blissfully unaware of the transformative magic that is fine-tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147cf266-5e90-4b81-92a5-53b7981c47be",
   "metadata": {},
   "source": [
    "### 1.1: Create an Evaluation Config Object\n",
    "Create an evaluation configuration object for NeMo Evaluator. For more information on various parameters, refer to the [NeMo Evaluator configuration](https://developer.nvidia.com/docs/nemo-microservices/evaluate/evaluation-configs.html) in the NeMo microservices documentation.\n",
    "\n",
    "\n",
    "* The `tasks.custom-tool-calling.dataset.files_url` is used to indicate which test file to use. Note that it's required to upload this to the NeMo Data Store and register with Entity store before using.\n",
    "* The `tasks.dataset.limit` argument below specifies how big a subset of test data to run the evaluation on\n",
    "* The evaluation metric `tasks.metrics.tool-calling-accuracy` reports `function_name_accuracy` and `function_name_and_args_accuracy` numbers, which are as their names imply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2e116-ed60-4e88-970b-f0d09d4a258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tool_calling_eval_config = {\n",
    "    \"type\": \"custom\",\n",
    "    \"tasks\": {\n",
    "        \"custom-tool-calling\": {\n",
    "            \"type\": \"chat-completion\",\n",
    "            \"dataset\": {\n",
    "                \"files_url\": f\"hf://datasets/{NMS_NAMESPACE}/{DATASET_NAME}/testing/xlam-test-single.jsonl\",\n",
    "                \"limit\": 50\n",
    "            },\n",
    "            \"params\": {\n",
    "                \"template\": {\n",
    "                    \"messages\": \"{{ item.messages | tojson}}\",\n",
    "                    \"tools\": \"{{ item.tools | tojson }}\",\n",
    "                    \"tool_choice\": \"auto\"\n",
    "                }\n",
    "            },\n",
    "            \"metrics\": {\n",
    "                \"tool-calling-accuracy\": {\n",
    "                    \"type\": \"tool-calling\",\n",
    "                    \"params\": {\"tool_calls_ground_truth\": \"{{ item.tool_calls | tojson }}\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print_status(\"Evaluation configuration created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a49e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete evaluation target (if it exists)\n",
    "res = requests.delete(f\"{EVALUATOR_URL}/v1/evaluation/targets/{NMS_NAMESPACE}/llama-3-1b-instruct\")\n",
    "# Ignore 404 errors (target might not exist)\n",
    "if res.status_code not in (200, 404):\n",
    "    print(f\"‚ö†Ô∏è Warning: Could not delete existing target: {res.status_code}\")\n",
    "\n",
    "## Create evaluation target\n",
    "# IMPORTANT: Use cluster-internal URL (NIM_URL_CLUSTER) for evaluation targets\n",
    "# because evaluation jobs run inside the cluster and can't access localhost port-forwards\n",
    "# Reload config module to get the latest NIM_URL_CLUSTER value\n",
    "import importlib\n",
    "import config\n",
    "importlib.reload(config)\n",
    "from config import NIM_URL_CLUSTER\n",
    "\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "data = {\n",
    "    \"type\": \"model\",\n",
    "    \"name\": \"llama-3-1b-instruct\",\n",
    "    \"namespace\": NMS_NAMESPACE,  # Use the correct namespace\n",
    "    \"model\": {\n",
    "        \"api_endpoint\": {\n",
    "            \"url\": f\"{NIM_URL_CLUSTER}/v1/chat/completions\",  # Use cluster URL, not localhost. Note: /v1/chat/completions for chat models\n",
    "            \"model_id\": f\"{BASE_MODEL}\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "print(f\"‚ÑπÔ∏è  Creating evaluation target with cluster URL: {NIM_URL_CLUSTER}/v1/chat/completions\")\n",
    "print(f\"   (Evaluation jobs run inside cluster and need cluster service URL, not localhost)\")\n",
    "print(f\"   (Using /v1/chat/completions endpoint for chat models)\")\n",
    "res = requests.post(f\"{EVALUATOR_URL}/v1/evaluation/targets\", headers=headers, json=data)\n",
    "target_response = res.json()\n",
    "print(\"Evaluation Target Created:\")\n",
    "print(json.dumps(target_response, indent=2))\n",
    "# Also return it so Jupyter displays it\n",
    "target_response\n",
    "\n",
    "print_status(\"Evaluation target created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8007f17-2eb1-477f-b156-5ed1d17d894b",
   "metadata": {},
   "source": [
    "### 1.2: Launch Evaluation Job "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5fbf88",
   "metadata": {},
   "source": [
    "The following code sends a POST request to the NeMo Evaluator API to launch an evaluation job. It uses the evaluation configuration defined in the previous cell and targets the base model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c5ec90-a189-47d6-9c07-18162b95130d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = requests.post(\n",
    "    f\"{EVALUATOR_URL}/v1/evaluation/jobs\",\n",
    "    json={\n",
    "        \"config\": simple_tool_calling_eval_config,\n",
    "        \"target\": f\"{NMS_NAMESPACE}/llama-3-1b-instruct\"  # Use the correct namespace\n",
    "    }\n",
    ")\n",
    "\n",
    "if res.status_code not in (200, 201):\n",
    "    print(f\"‚ùå Error creating evaluation job: Status {res.status_code}\")\n",
    "    print(f\"Response: {res.text}\")\n",
    "    raise Exception(f\"Failed to create evaluation job: {res.text}\")\n",
    "\n",
    "job_response = res.json()\n",
    "base_eval_job_id = job_response[\"id\"]\n",
    "\n",
    "print(\"Evaluation Job Created:\")\n",
    "print(json.dumps(job_response, indent=2))\n",
    "print(f\"\\nJob ID: {base_eval_job_id}\")\n",
    "\n",
    "# Also return it so Jupyter displays it\n",
    "base_eval_job_id\n",
    "\n",
    "print_status(\"Base model evaluation job created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40974159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Job status\n",
    "res = requests.get(f\"{EVALUATOR_URL}/v1/evaluation/jobs/{base_eval_job_id}/status\")\n",
    "\n",
    "# Check response status\n",
    "if res.status_code != 200:\n",
    "    print(f\"‚ùå Error: Status {res.status_code}\")\n",
    "    print(f\"Response: {res.text}\")\n",
    "    raise Exception(f\"Failed to get evaluation job status: {res.text}\")\n",
    "\n",
    "# Format and print the response\n",
    "job_status = res.json()\n",
    "print(\"Evaluation Job Status:\")\n",
    "print(json.dumps(job_status, indent=2))\n",
    "\n",
    "# Also return it so Jupyter displays it\n",
    "job_status\n",
    "\n",
    "print_status(\"Evaluation job status retrieved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc5133e-076c-4560-815a-1ff71901af01",
   "metadata": {},
   "source": [
    "The following code defines a helper function to poll on job status until it finishes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa0b694-a699-4eb0-903c-9bac8651f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_eval_job(job_url: str, polling_interval: int = 10, timeout: int = 6000):\n",
    "    \"\"\"Helper for waiting an eval job with improved error handling and logging.\"\"\"\n",
    "    start_time = time()\n",
    "    \n",
    "    # Initial status check\n",
    "    print(f\"üîç Checking evaluation job status at: {job_url}\")\n",
    "    res = requests.get(job_url)\n",
    "    \n",
    "    if res.status_code != 200:\n",
    "        print(f\"‚ùå Error: Failed to get job status. HTTP {res.status_code}\")\n",
    "        print(f\"Response: {res.text}\")\n",
    "        raise Exception(f\"Failed to get evaluation job status: HTTP {res.status_code}, Response: {res.text}\")\n",
    "    \n",
    "    try:\n",
    "        job_data = res.json()\n",
    "        status = job_data.get(\"status\", \"unknown\")\n",
    "        print(f\"üìä Initial job status: {status}\")\n",
    "        \n",
    "        # Print full job data for debugging\n",
    "        print(\"\\nInitial Job Data:\")\n",
    "        print(json.dumps(job_data, indent=2))\n",
    "    except (KeyError, ValueError) as e:\n",
    "        print(f\"‚ùå Error parsing job response: {e}\")\n",
    "        print(f\"Response text: {res.text}\")\n",
    "        raise Exception(f\"Failed to parse job status from response: {e}\")\n",
    "\n",
    "    # Check if job is already in a terminal state (failed, completed, etc.)\n",
    "    if status == \"failed\":\n",
    "        print(f\"\\n‚ùå Job is already in 'failed' state!\")\n",
    "        print(\"\\nFull job data for debugging:\")\n",
    "        print(json.dumps(job_data, indent=2))\n",
    "        \n",
    "        # Extract error details if available\n",
    "        error_details = job_data.get(\"status_details\", {})\n",
    "        if error_details:\n",
    "            print(\"\\nError Details:\")\n",
    "            print(json.dumps(error_details, indent=2))\n",
    "        \n",
    "        print(\"\\nüí° To investigate the failure, check:\")\n",
    "        print(\"   1. Evaluator service logs:\")\n",
    "        print(\"      oc logs -n arhkp-nemo-helm -l app=nemoevaluator --tail=200\")\n",
    "        print(\"   2. Evaluation job pods (if any):\")\n",
    "        print(\"      oc get pods -n arhkp-nemo-helm | grep -E 'eval|evaluation'\")\n",
    "        print(\"   3. Evaluator pod status:\")\n",
    "        print(\"      oc get pods -n arhkp-nemo-helm | grep evaluator\")\n",
    "        print(\"   4. Check if the evaluation target/model is accessible:\")\n",
    "        print(\"      oc get nemoevaluator -n arhkp-nemo-helm -o yaml\")\n",
    "        \n",
    "        raise Exception(f\"Evaluation job failed. Status: {status}. Check cluster logs for details.\")\n",
    "    elif status in [\"completed\", \"success\", \"finished\"]:\n",
    "        print(f\"\\n‚úÖ Job is already completed with status: {status}\")\n",
    "        print(f\"Total time: {time() - start_time:.2f}s\")\n",
    "        return res\n",
    "\n",
    "    # Track status changes\n",
    "    last_status = status\n",
    "    status_changes = []\n",
    "    \n",
    "    while (status in [\"pending\", \"created\", \"running\", \"unknown\"]):\n",
    "        # Check for timeout\n",
    "        elapsed = time() - start_time\n",
    "        if elapsed > timeout:\n",
    "            print(f\"\\n‚è±Ô∏è  Timeout: Job took more than {timeout} seconds.\")\n",
    "            print(f\"Final status: {status}\")\n",
    "            print(f\"Status history: {status_changes}\")\n",
    "            raise RuntimeError(f\"Evaluation job timeout after {timeout} seconds. Final status: {status}\")\n",
    "\n",
    "        # Sleep before polling again\n",
    "        sleep(polling_interval)\n",
    "\n",
    "        # Fetch updated status and progress\n",
    "        res = requests.get(job_url)\n",
    "        \n",
    "        if res.status_code != 200:\n",
    "            print(f\"‚ö†Ô∏è  Warning: HTTP {res.status_code} when polling job status\")\n",
    "            print(f\"Response: {res.text}\")\n",
    "            # Continue polling - might be a temporary issue\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            job_data = res.json()\n",
    "            status = job_data.get(\"status\", \"unknown\")\n",
    "            \n",
    "            # Track status changes\n",
    "            if status != last_status:\n",
    "                status_changes.append((elapsed, last_status, status))\n",
    "                print(f\"\\nüîÑ Status changed: {last_status} ‚Üí {status} (after {elapsed:.2f}s)\")\n",
    "                last_status = status\n",
    "            \n",
    "            # Progress details\n",
    "            progress = 0\n",
    "            if status == \"running\":\n",
    "                progress = job_data.get(\"status_details\", {}).get(\"progress\", 0)\n",
    "                print(f\"‚è≥ Job status: {status} | Progress: {progress}% | Elapsed: {elapsed:.2f}s\")\n",
    "            elif status == \"completed\":\n",
    "                progress = 100\n",
    "                print(f\"‚úÖ Job status: {status} | Progress: {progress}% | Elapsed: {elapsed:.2f}s\")\n",
    "            elif status in [\"pending\", \"created\"]:\n",
    "                print(f\"‚è≥ Job status: {status} | Elapsed: {elapsed:.2f}s\")\n",
    "            elif status == \"failed\":\n",
    "                print(f\"‚ùå Job status: {status} | Elapsed: {elapsed:.2f}s\")\n",
    "                print(\"\\nFull job data:\")\n",
    "                print(json.dumps(job_data, indent=2))\n",
    "                raise Exception(f\"Evaluation job failed. Check cluster logs for details.\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Job status: {status} (unexpected) | Elapsed: {elapsed:.2f}s\")\n",
    "                \n",
    "        except (KeyError, ValueError) as e:\n",
    "            print(f\"‚ö†Ô∏è  Warning: Error parsing job response: {e}\")\n",
    "            print(f\"Response text: {res.text}\")\n",
    "            # Continue polling - might be a temporary issue\n",
    "            continue\n",
    "\n",
    "    print(f\"\\n‚úÖ Job completed with status: {status} (total time: {time() - start_time:.2f}s)\")\n",
    "    return res\n",
    "\n",
    "print_status(\"Evaluation job wait function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49db71fc",
   "metadata": {},
   "source": [
    "Run the helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48322fdb-8920-435d-863d-9a75158843d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poll for evaluation job completion\n",
    "print(f\"üöÄ Starting to poll evaluation job: {base_eval_job_id}\")\n",
    "print(f\"Job URL: {EVALUATOR_URL}/v1/evaluation/jobs/{base_eval_job_id}\")\n",
    "print(f\"Polling interval: 5 seconds, Timeout: 600 seconds (10 minutes)\\n\")\n",
    "\n",
    "try:\n",
    "    res = wait_eval_job(f\"{EVALUATOR_URL}/v1/evaluation/jobs/{base_eval_job_id}\", polling_interval=5, timeout=600)\n",
    "except Exception as e:\n",
    "    # If wait_eval_job raised an exception (e.g., job failed), try to get final status for debugging\n",
    "    print(f\"\\n‚ö†Ô∏è  Exception during polling: {e}\")\n",
    "    print(\"\\nAttempting to fetch final job status for debugging...\")\n",
    "    try:\n",
    "        final_res = requests.get(f\"{EVALUATOR_URL}/v1/evaluation/jobs/{base_eval_job_id}\")\n",
    "        if final_res.status_code == 200:\n",
    "            final_job_data = final_res.json()\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"üìã Final Job Status (for debugging):\")\n",
    "            print(\"=\"*60)\n",
    "            print(json.dumps(final_job_data, indent=2))\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            # Extract and display error information\n",
    "            status_details = final_job_data.get(\"status_details\", {})\n",
    "            if status_details:\n",
    "                print(\"\\nüîç Status Details:\")\n",
    "                print(json.dumps(status_details, indent=2))\n",
    "            \n",
    "            # Check for error messages\n",
    "            error_msg = status_details.get(\"error\") or status_details.get(\"message\") or final_job_data.get(\"error\")\n",
    "            if error_msg:\n",
    "                print(f\"\\n‚ùå Error Message: {error_msg}\")\n",
    "        else:\n",
    "            print(f\"Could not fetch final status: HTTP {final_res.status_code}\")\n",
    "    except Exception as fetch_error:\n",
    "        print(f\"Could not fetch final status: {fetch_error}\")\n",
    "    \n",
    "    # Re-raise the original exception\n",
    "    raise\n",
    "\n",
    "# Check response status\n",
    "if res.status_code != 200:\n",
    "    print(f\"\\n‚ùå Error: Evaluation job status check failed with status {res.status_code}\")\n",
    "    print(f\"Response: {res.text}\")\n",
    "    print(\"\\nüí° To check cluster logs, run:\")\n",
    "    print(f\"   oc logs -n arhkp-nemo-helm -l app=nemoevaluator --tail=100\")\n",
    "    print(f\"   oc get pods -n arhkp-nemo-helm | grep evaluator\")\n",
    "    raise Exception(f\"Failed to get evaluation job status: {res.text}\")\n",
    "\n",
    "# Format and print the response\n",
    "job_status = res.json()\n",
    "final_status = job_status.get(\"status\", \"unknown\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã Final Evaluation Job Status:\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(job_status, indent=2))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract and display error information if job failed\n",
    "if final_status == \"failed\":\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ùå JOB FAILED - Error Analysis:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    status_details = job_status.get(\"status_details\", {})\n",
    "    if status_details:\n",
    "        print(\"\\nStatus Details:\")\n",
    "        print(json.dumps(status_details, indent=2))\n",
    "        \n",
    "        # Look for common error fields\n",
    "        error_fields = [\"error\", \"message\", \"reason\", \"failure_reason\", \"error_message\"]\n",
    "        for field in error_fields:\n",
    "            if field in status_details:\n",
    "                print(f\"\\nüî¥ {field.upper()}: {status_details[field]}\")\n",
    "    \n",
    "    # Check for error in top level\n",
    "    if \"error\" in job_status:\n",
    "        print(f\"\\nüî¥ Top-level error: {job_status['error']}\")\n",
    "    \n",
    "    print(\"\\nüí° Common causes of evaluation job failures:\")\n",
    "    print(\"   1. Evaluation target (model) is not accessible or not found\")\n",
    "    print(\"   2. Evaluation dataset is not accessible or invalid\")\n",
    "    print(\"   3. Insufficient resources (GPU, memory, etc.)\")\n",
    "    print(\"   4. Network connectivity issues between services\")\n",
    "    print(\"   5. Configuration errors in evaluation config\")\n",
    "    \n",
    "    print(\"\\nüí° To investigate:\")\n",
    "    print(\"   1. Check evaluator service logs:\")\n",
    "    print(\"      oc logs -n arhkp-nemo-helm -l app=nemoevaluator --tail=200\")\n",
    "    print(\"   2. Check evaluation job pods:\")\n",
    "    print(\"      oc get pods -n arhkp-nemo-helm | grep -E 'eval|evaluation'\")\n",
    "    print(\"   3. Verify evaluation target exists:\")\n",
    "    print(f\"      requests.get(f'{EVALUATOR_URL}/v1/evaluation/targets/{NMS_NAMESPACE}/llama-3-1b-instruct').json()\")\n",
    "    print(\"   4. Check evaluator pod status:\")\n",
    "    print(\"      oc get pods -n arhkp-nemo-helm | grep evaluator\")\n",
    "    print(\"   5. Check evaluator custom resource:\")\n",
    "    print(\"      oc get nemoevaluator -n arhkp-nemo-helm -o yaml\")\n",
    "\n",
    "# Check if job actually completed successfully\n",
    "if final_status not in [\"completed\", \"success\", \"finished\"]:\n",
    "    if final_status != \"failed\":  # Already handled above\n",
    "        print(f\"\\n‚ö†Ô∏è  Warning: Job status is '{final_status}', not 'completed'\")\n",
    "        print(\"This might indicate the job is still running, failed, or in an unexpected state.\")\n",
    "        print(\"\\nüí° To check cluster logs and pod status:\")\n",
    "        print(f\"   # Check evaluator pods\")\n",
    "        print(f\"   oc get pods -n arhkp-nemo-helm | grep evaluator\")\n",
    "        print(f\"   \")\n",
    "        print(f\"   # Check evaluator logs\")\n",
    "        print(f\"   oc logs -n arhkp-nemo-helm -l app=nemoevaluator --tail=100\")\n",
    "        print(f\"   \")\n",
    "        print(f\"   # Check for evaluation job pods (if any)\")\n",
    "        print(f\"   oc get pods -n arhkp-nemo-helm | grep -E 'eval|evaluation'\")\n",
    "        print(f\"   \")\n",
    "        print(f\"   # Check job status directly\")\n",
    "        print(f\"   oc get nemoevaluator -n arhkp-nemo-helm\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Job completed successfully with status: {final_status}\")\n",
    "\n",
    "# Also return it so Jupyter displays it\n",
    "job_status\n",
    "\n",
    "print_status(\"Base model evaluation job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de58a76-1775-4c93-8798-1ec89c8eeaff",
   "metadata": {},
   "source": [
    "### 1.3 Review Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb6220",
   "metadata": {},
   "source": [
    "The following code sends a GET request to retrieve the evaluation results for the base evaluation job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec04c15a-62a9-4271-8a7f-9a7e8ec7884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, check the job status to ensure it's completed\n",
    "print(\"Checking evaluation job status...\")\n",
    "status_res = requests.get(f\"{EVALUATOR_URL}/v1/evaluation/jobs/{base_eval_job_id}/status\")\n",
    "\n",
    "if status_res.status_code != 200:\n",
    "    print(f\"‚ö†Ô∏è Warning: Could not check job status (Status {status_res.status_code})\")\n",
    "    print(f\"Response: {status_res.text}\")\n",
    "    print(\"\\nAttempting to retrieve results anyway (the job might still be accessible)...\")\n",
    "else:\n",
    "    status_data = status_res.json()\n",
    "    \n",
    "    # Print full status for debugging\n",
    "    print(\"\\nFull status response:\")\n",
    "    print(json.dumps(status_data, indent=2))\n",
    "    \n",
    "    # The /status endpoint returns a different structure than the full job endpoint\n",
    "    # It has \"message\" and \"task_status\" instead of a top-level \"status\" field\n",
    "    job_status = status_data.get(\"status\")\n",
    "    \n",
    "    # If no \"status\" field, infer from message and task_status\n",
    "    if job_status is None:\n",
    "        message = status_data.get(\"message\", \"\").lower()\n",
    "        task_status = status_data.get(\"task_status\", {})\n",
    "        progress = status_data.get(\"progress\", 0)\n",
    "        \n",
    "        # Infer status from message and task status\n",
    "        if \"completed successfully\" in message or \"success\" in message:\n",
    "            # Check if all tasks are completed\n",
    "            if task_status:\n",
    "                all_tasks_completed = all(\n",
    "                    status.lower() in [\"completed\", \"success\", \"finished\"] \n",
    "                    for status in task_status.values()\n",
    "                )\n",
    "                if all_tasks_completed and progress >= 100:\n",
    "                    job_status = \"completed\"\n",
    "                elif all_tasks_completed:\n",
    "                    job_status = \"completed\"  # Progress might not be exactly 100\n",
    "                else:\n",
    "                    job_status = \"running\"  # Some tasks still in progress\n",
    "            elif progress >= 100:\n",
    "                job_status = \"completed\"\n",
    "            else:\n",
    "                job_status = \"running\"\n",
    "        elif \"failed\" in message or \"error\" in message:\n",
    "            job_status = \"failed\"\n",
    "        elif \"running\" in message or progress > 0:\n",
    "            job_status = \"running\"\n",
    "        else:\n",
    "            job_status = \"unknown\"\n",
    "        \n",
    "        print(f\"\\nüìä Inferred job status: {job_status}\")\n",
    "        print(f\"   Message: {status_data.get('message', 'N/A')}\")\n",
    "        print(f\"   Progress: {progress}%\")\n",
    "        if task_status:\n",
    "            print(f\"   Task status: {task_status}\")\n",
    "    else:\n",
    "        print(f\"Job status: {job_status}\")\n",
    "    \n",
    "    # Valid completion statuses\n",
    "    completed_statuses = [\"completed\", \"success\", \"finished\", \"done\"]\n",
    "    \n",
    "    if job_status in completed_statuses:\n",
    "        print(f\"‚úÖ Job is completed (status: {job_status})\")\n",
    "    elif job_status == \"unknown\":\n",
    "        print(\"‚ö†Ô∏è Warning: Job status is 'unknown'\")\n",
    "        print(\"This could mean:\")\n",
    "        print(\"  - The job doesn't exist or was deleted\")\n",
    "        print(\"  - The status endpoint returned an unexpected format\")\n",
    "        print(\"  - The job is in an intermediate state\")\n",
    "        print(\"\\nAttempting to retrieve results anyway...\")\n",
    "    elif job_status == \"failed\":\n",
    "        print(f\"‚ùå Job has failed (status: {job_status})\")\n",
    "        print(\"Check the status details above for error information.\")\n",
    "        print(\"\\nAttempting to retrieve results anyway (may contain error details)...\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: Evaluation job is not completed yet. Status: {job_status}\")\n",
    "        print(\"Valid completion statuses:\", completed_statuses)\n",
    "        print(\"\\nYou can check the status again with:\")\n",
    "        print(f'  requests.get(f\"{EVALUATOR_URL}/v1/evaluation/jobs/{base_eval_job_id}/status\").json()')\n",
    "        print(\"\\nAttempting to retrieve results anyway (the job might have results even if status is not 'completed')...\")\n",
    "\n",
    "# Now retrieve the results (try even if status check failed or status is unknown)\n",
    "print(\"\\nRetrieving evaluation results...\")\n",
    "res = requests.get(f\"{EVALUATOR_URL}/v1/evaluation/jobs/{base_eval_job_id}/results\")\n",
    "\n",
    "# Check response status\n",
    "if res.status_code != 200:\n",
    "    print(f\"‚ùå Error retrieving results: Status {res.status_code}\")\n",
    "    print(f\"Response: {res.text}\")\n",
    "    raise Exception(f\"Failed to retrieve evaluation results: {res.text}\")\n",
    "\n",
    "# Explicitly print the results\n",
    "results = res.json()\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(json.dumps(results, indent=2))\n",
    "\n",
    "# Check if tasks are empty\n",
    "if not results.get(\"tasks\") or len(results.get(\"tasks\", {})) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Evaluation results have no tasks!\")\n",
    "    print(\"This could mean:\")\n",
    "    print(\"  1. The evaluation job completed but produced no results\")\n",
    "    print(\"  2. There was an error during evaluation\")\n",
    "    print(\"  3. The evaluation configuration was incorrect\")\n",
    "    print(\"\\nPlease check the evaluation job logs or status for more details.\")\n",
    "    print(f\"Job ID: {base_eval_job_id}\")\n",
    "\n",
    "# Also return it so Jupyter displays it\n",
    "results\n",
    "\n",
    "print_status(\"Base model evaluation results retrieved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b28813",
   "metadata": {},
   "source": [
    "The following code extracts and prints the accuracy scores for the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129c739-8980-4d28-ab72-331e00f0c2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract function name accuracy score\n",
    "# Handle different possible task names and structures\n",
    "# Note: 'res' should be set from the previous cell (cell 102)\n",
    "if 'res' not in locals() and 'res' not in globals():\n",
    "    raise NameError(\"Variable 'res' not found. Please run the previous cell to retrieve evaluation results first.\")\n",
    "\n",
    "result_data = res.json()\n",
    "tasks = result_data.get(\"tasks\", {})\n",
    "\n",
    "# Check if tasks are empty\n",
    "if not tasks or len(tasks) == 0:\n",
    "    print(\"‚ùå Error: Evaluation results have no tasks!\")\n",
    "    print(\"This means the evaluation job completed but produced no results.\")\n",
    "    print(\"\\nPossible causes:\")\n",
    "    print(\"  1. The evaluation dataset was empty or invalid\")\n",
    "    print(\"  2. The evaluation job failed silently\")\n",
    "    print(\"  3. The evaluation configuration was incorrect\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(f\"  - Evaluation job status: requests.get(f'{EVALUATOR_URL}/v1/evaluation/jobs/{base_eval_job_id}/status').json()\")\n",
    "    print(f\"  - Evaluation job logs in the cluster\")\n",
    "    print(f\"  - The evaluation dataset and configuration\")\n",
    "    print(\"\\nFull results structure:\")\n",
    "    print(json.dumps(result_data, indent=2))\n",
    "    raise ValueError(\"Evaluation results have no tasks. Please check the evaluation job status and logs.\")\n",
    "\n",
    "# Find the task (could be 'custom-tool-calling' or another name)\n",
    "task_name = None\n",
    "if \"custom-tool-calling\" in tasks:\n",
    "    task_name = \"custom-tool-calling\"\n",
    "elif len(tasks) > 0:\n",
    "    # Use the first task if 'custom-tool-calling' is not found\n",
    "    task_name = list(tasks.keys())[0]\n",
    "    print(f\"‚ö†Ô∏è Note: Using task '{task_name}' instead of 'custom-tool-calling'\")\n",
    "\n",
    "if not task_name or task_name not in tasks:\n",
    "    print(\"‚ùå Error: Could not find evaluation task in results\")\n",
    "    print(f\"Available tasks: {list(tasks.keys())}\")\n",
    "    print(f\"\\nFull results structure:\")\n",
    "    print(json.dumps(result_data, indent=2))\n",
    "    raise KeyError(f\"Task 'custom-tool-calling' not found. Available tasks: {list(tasks.keys())}\")\n",
    "\n",
    "# Extract metrics\n",
    "task_data = tasks[task_name]\n",
    "metrics = task_data.get(\"metrics\", {})\n",
    "tool_calling_metrics = metrics.get(\"tool-calling-accuracy\", {})\n",
    "scores = tool_calling_metrics.get(\"scores\", {})\n",
    "\n",
    "base_function_name_accuracy_score = scores.get(\"function_name_accuracy\", {}).get(\"value\")\n",
    "base_function_name_and_args_accuracy = scores.get(\"function_name_and_args_accuracy\", {}).get(\"value\")\n",
    "\n",
    "if base_function_name_accuracy_score is None or base_function_name_and_args_accuracy is None:\n",
    "    print(\"‚ö†Ô∏è Warning: Some accuracy scores are missing\")\n",
    "    print(f\"Available scores: {list(scores.keys())}\")\n",
    "    print(f\"\\nFull metrics structure:\")\n",
    "    print(json.dumps(metrics, indent=2))\n",
    "    print(f\"\\nFull task data:\")\n",
    "    print(json.dumps(task_data, indent=2))\n",
    "\n",
    "print(f\"Base model: function_name_accuracy: {base_function_name_accuracy_score}\")\n",
    "print(f\"Base model: function_name_and_args_accuracy: {base_function_name_and_args_accuracy}\")\n",
    "\n",
    "print_status(\"Base model accuracy scores extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9fcb78-0e23-45f6-988a-8be6e33c2cfc",
   "metadata": {},
   "source": [
    "Without any finetuning, the `meta/llama-3.2-1b-instruct` model should score in the ballpark of about 12% in `function_name_accuracy`, and 8% in `function_name_and_args_accuracy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c7ab4-bb3c-46c3-a51e-9e25509a7781",
   "metadata": {},
   "source": [
    "### (Optional) 1.4 Download and Inspect Results\n",
    "\n",
    "To take a deeper look into the model's generated outputs, you can download and review the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf02e0-2383-46d5-ad47-3edc1e1dfdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_evaluation_results(eval_url, eval_job_id, output_file):\n",
    "    \"\"\"Downloads evaluation results for a given job ID from the NeMo server.\"\"\"\n",
    "    \n",
    "    download_response = requests.get(f\"{eval_url}/v1/evaluation/jobs/{eval_job_id}/download-results\")\n",
    "    \n",
    "    # Check the response status\n",
    "    if download_response.status_code == 200:\n",
    "        # Save the results to a file\n",
    "        with open(output_file, \"wb\") as file:\n",
    "            file.write(download_response.content)\n",
    "        print(f\"Evaluation results for job {eval_job_id} downloaded successfully to {output_file}.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Failed to download evaluation results. Status code: {download_response.status_code}\")\n",
    "        print('Response:', download_response.text)\n",
    "        return False\n",
    "\n",
    "print_status(\"Evaluation results download function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ec595-8edd-4d8c-a2a0-29f830710c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = f\"{base_eval_job_id}.json\"\n",
    "\n",
    "# Assertion fails if download fails\n",
    "assert download_evaluation_results(eval_url=EVALUATOR_URL, eval_job_id=base_eval_job_id, output_file=output_file) == True\n",
    "\n",
    "print_status(\"Base model evaluation results downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687f633d-5b20-4a28-8962-970a54b91f40",
   "metadata": {},
   "source": [
    "You can inspect the downloaded results file to observe places where the base model errors. Without any fine-tuning, some models not only return inaccurate function names and arguments, but they may not adhere to a consistent structured / predictable output schema. This makes it difficult to automatically parse these outputs, deterring integration with external systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79229ce6-4ce4-4f82-b0c9-15be351a2291",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"step-2\"></a>\n",
    "## Step 2: Evaluate the LoRA Customized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdeba8-6800-419f-b5c5-0dc34bf6e0f5",
   "metadata": {},
   "source": [
    "### 2.1 Launch Evaluation Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb900e7-2ec8-417a-9266-2ce682f655e5",
   "metadata": {},
   "source": [
    "Run another evaluation job with the same evaluation config but with the customized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bbfea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Wait for the custom model to be available in NIM before creating evaluation target\n",
    "# NIM synchronizes custom models from Entity Store every 3 minutes, so we need to wait\n",
    "print(f\"üîç Checking if custom model is available in NIM: {CUSTOMIZED_MODEL}\")\n",
    "print(\"   (NIM synchronizes custom models every 3 minutes, so this may take a few minutes)\")\n",
    "\n",
    "from time import sleep, time\n",
    "max_wait_time = 600  # 10 minutes max wait\n",
    "poll_interval = 10  # Check every 10 seconds\n",
    "start_time = time()\n",
    "model_available = False\n",
    "model_to_use = CUSTOMIZED_MODEL\n",
    "\n",
    "while (time() - start_time) < max_wait_time:\n",
    "    nim_models_resp = requests.get(f\"{NIM_URL}/v1/models\")\n",
    "    if nim_models_resp.status_code == 200:\n",
    "        nim_models = nim_models_resp.json().get(\"data\", [])\n",
    "        nim_model_ids = [m.get(\"id\") for m in nim_models]\n",
    "        \n",
    "        if CUSTOMIZED_MODEL in nim_model_ids:\n",
    "            print(f\"‚úÖ Custom model '{CUSTOMIZED_MODEL}' is now available in NIM!\")\n",
    "            model_available = True\n",
    "            model_to_use = CUSTOMIZED_MODEL\n",
    "            break\n",
    "        else:\n",
    "            # Check if a similar model is available (same base name, different version)\n",
    "            customized_model_name = CUSTOMIZED_MODEL.split('/')[-1].split('@')[0]\n",
    "            similar_models = [m for m in nim_model_ids if customized_model_name in m]\n",
    "            if similar_models:\n",
    "                print(f\"‚ö†Ô∏è  Custom model '{CUSTOMIZED_MODEL}' not yet in NIM, but found similar models:\")\n",
    "                for m in similar_models:\n",
    "                    print(f\"   - {m}\")\n",
    "                print(f\"   Using the latest similar model: {similar_models[0]}\")\n",
    "                model_to_use = similar_models[0]\n",
    "                model_available = True\n",
    "                break\n",
    "    \n",
    "    elapsed = time() - start_time\n",
    "    print(f\"‚è≥ Waiting for model to sync... ({elapsed:.0f}s elapsed, checking every {poll_interval}s)\")\n",
    "    sleep(poll_interval)\n",
    "\n",
    "if not model_available:\n",
    "    print(f\"\\n‚ö†Ô∏è  Warning: Custom model '{CUSTOMIZED_MODEL}' is still not available in NIM after {max_wait_time}s\")\n",
    "    print(\"   This could mean:\")\n",
    "    print(\"   1. The model hasn't been synchronized yet (NIM syncs every 3 minutes)\")\n",
    "    print(\"   2. There's an issue with model synchronization\")\n",
    "    print(\"   3. The model ID is incorrect\")\n",
    "    print(\"\\n   Attempting to use the model anyway (it might work if sync happens during evaluation)...\")\n",
    "    model_to_use = CUSTOMIZED_MODEL\n",
    "\n",
    "# Delete evaluation target (if it exists)\n",
    "res = requests.delete(f\"{EVALUATOR_URL}/v1/evaluation/targets/{NMS_NAMESPACE}/llama-3-1b-instruct-customized\")\n",
    "# Ignore 404 errors (target might not exist)\n",
    "if res.status_code not in (200, 404):\n",
    "    print(f\"‚ö†Ô∏è Warning: Could not delete existing target: {res.status_code}\")\n",
    "\n",
    "## Create evaluation target\n",
    "# IMPORTANT: Use cluster-internal URL (NIM_URL_CLUSTER) for evaluation targets\n",
    "# because evaluation jobs run inside the cluster and can't access localhost port-forwards\n",
    "# Reload config module to get the latest NIM_URL_CLUSTER value\n",
    "import importlib\n",
    "import config\n",
    "importlib.reload(config)\n",
    "from config import NIM_URL_CLUSTER\n",
    "\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "data = {\n",
    "    \"type\": \"model\",\n",
    "    \"name\": \"llama-3-1b-instruct-customized\",\n",
    "    \"namespace\": NMS_NAMESPACE,  # Use the correct namespace\n",
    "    \"model\": {\n",
    "        \"api_endpoint\": {\n",
    "            \"url\": f\"{NIM_URL_CLUSTER}/v1/chat/completions\",  # Use cluster URL, not localhost. Note: /v1/chat/completions for chat models\n",
    "            \"model_id\": f\"{model_to_use}\"  # Use the model that's actually available in NIM\n",
    "        }\n",
    "    }\n",
    "}\n",
    "print(f\"\\n‚ÑπÔ∏è  Creating evaluation target with cluster URL: {NIM_URL_CLUSTER}/v1/chat/completions\")\n",
    "print(f\"   Model ID: {model_to_use}\")\n",
    "print(f\"   (Evaluation jobs run inside cluster and need cluster service URL, not localhost)\")\n",
    "print(f\"   (Using /v1/chat/completions endpoint for chat models)\")\n",
    "res = requests.post(f\"{EVALUATOR_URL}/v1/evaluation/targets\", headers=headers, json=data)\n",
    "\n",
    "if res.status_code not in (200, 201):\n",
    "    print(f\"‚ùå Error creating evaluation target: Status {res.status_code}\")\n",
    "    print(f\"Response: {res.text}\")\n",
    "    raise Exception(f\"Failed to create evaluation target: Status {res.status_code}, Response: {res.text}\")\n",
    "\n",
    "target_response = res.json()\n",
    "print(\"\\n‚úÖ Evaluation Target Created:\")\n",
    "print(json.dumps(target_response, indent=2))\n",
    "# Also return it so Jupyter displays it\n",
    "target_response\n",
    "\n",
    "print_status(\"Custom model evaluation target created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5438608-e708-4421-bf45-da39fbe506ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.post(\n",
    "    f\"{EVALUATOR_URL}/v1/evaluation/jobs\",\n",
    "    json={\n",
    "        \"config\": simple_tool_calling_eval_config,\n",
    "        \"target\": f\"{NMS_NAMESPACE}/llama-3-1b-instruct-customized\"  # Use the correct namespace\n",
    "    },\n",
    ")\n",
    "\n",
    "if res.status_code not in (200, 201):\n",
    "    print(f\"‚ùå Error creating evaluation job: Status {res.status_code}\")\n",
    "    print(f\"Response: {res.text}\")\n",
    "    raise Exception(f\"Failed to create evaluation job: {res.text}\")\n",
    "\n",
    "job_response = res.json()\n",
    "ft_eval_job_id = job_response[\"id\"]\n",
    "\n",
    "print(\"Evaluation Job Created:\")\n",
    "print(json.dumps(job_response, indent=2))\n",
    "print(f\"\\nJob ID: {ft_eval_job_id}\")\n",
    "\n",
    "# Also return it so Jupyter displays it\n",
    "ft_eval_job_id\n",
    "\n",
    "print_status(\"Custom model evaluation job created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86003db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poll for evaluation job completion\n",
    "print(f\"üöÄ Starting to poll evaluation job: {ft_eval_job_id}\")\n",
    "print(f\"Job URL: {EVALUATOR_URL}/v1/evaluation/jobs/{ft_eval_job_id}\")\n",
    "print(f\"Polling interval: 5 seconds, Timeout: 600 seconds (10 minutes)\\n\")\n",
    "\n",
    "try:\n",
    "    res = wait_eval_job(f\"{EVALUATOR_URL}/v1/evaluation/jobs/{ft_eval_job_id}\", polling_interval=5, timeout=600)\n",
    "except Exception as e:\n",
    "    # If wait_eval_job raised an exception (e.g., job failed), try to get final status for debugging\n",
    "    print(f\"\\n‚ö†Ô∏è  Exception during polling: {e}\")\n",
    "    print(\"\\nAttempting to fetch final job status for debugging...\")\n",
    "    try:\n",
    "        final_res = requests.get(f\"{EVALUATOR_URL}/v1/evaluation/jobs/{ft_eval_job_id}\")\n",
    "        if final_res.status_code == 200:\n",
    "            final_job_data = final_res.json()\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"üìã Final Job Status (for debugging):\")\n",
    "            print(\"=\"*60)\n",
    "            print(json.dumps(final_job_data, indent=2))\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            # Extract and display error information\n",
    "            status_details = final_job_data.get(\"status_details\", {})\n",
    "            if status_details:\n",
    "                print(\"\\nüîç Status Details:\")\n",
    "                print(json.dumps(status_details, indent=2))\n",
    "            \n",
    "            # Check for error messages\n",
    "            error_msg = status_details.get(\"error\") or status_details.get(\"message\") or final_job_data.get(\"error\")\n",
    "            if error_msg:\n",
    "                print(f\"\\n‚ùå Error Message: {error_msg}\")\n",
    "        else:\n",
    "            print(f\"Could not fetch final status: HTTP {final_res.status_code}\")\n",
    "    except Exception as fetch_error:\n",
    "        print(f\"Could not fetch final status: {fetch_error}\")\n",
    "    \n",
    "    # Re-raise the original exception\n",
    "    raise\n",
    "\n",
    "# Check response status\n",
    "if res.status_code != 200:\n",
    "    print(f\"\\n‚ùå Error: Evaluation job status check failed with status {res.status_code}\")\n",
    "    print(f\"Response: {res.text}\")\n",
    "    print(\"\\nüí° To check cluster logs, run:\")\n",
    "    print(f\"   oc logs -n arhkp-nemo-helm -l app=nemoevaluator --tail=100\")\n",
    "    print(f\"   oc get pods -n arhkp-nemo-helm | grep evaluator\")\n",
    "    raise Exception(f\"Failed to get evaluation job status: {res.text}\")\n",
    "\n",
    "# Format and print the response\n",
    "job_status = res.json()\n",
    "final_status = job_status.get(\"status\", \"unknown\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã Final Evaluation Job Status:\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(job_status, indent=2))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract and display error information if job failed\n",
    "if final_status == \"failed\":\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ùå JOB FAILED - Error Analysis:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    status_details = job_status.get(\"status_details\", {})\n",
    "    if status_details:\n",
    "        print(\"\\nStatus Details:\")\n",
    "        print(json.dumps(status_details, indent=2))\n",
    "        \n",
    "        # Look for common error fields\n",
    "        error_fields = [\"error\", \"message\", \"reason\", \"failure_reason\", \"error_message\"]\n",
    "        for field in error_fields:\n",
    "            if field in status_details:\n",
    "                print(f\"\\nüî¥ {field.upper()}: {status_details[field]}\")\n",
    "    \n",
    "    # Check for error in top level\n",
    "    if \"error\" in job_status:\n",
    "        print(f\"\\nüî¥ Top-level error: {job_status['error']}\")\n",
    "    \n",
    "    print(\"\\nüí° Common causes of evaluation job failures:\")\n",
    "    print(\"   1. Evaluation target (model) is not accessible or not found\")\n",
    "    print(\"   2. Evaluation dataset is not accessible or invalid\")\n",
    "    print(\"   3. Insufficient resources (GPU, memory, etc.)\")\n",
    "    print(\"   4. Network connectivity issues between services\")\n",
    "    print(\"   5. Configuration errors in evaluation config\")\n",
    "    \n",
    "    print(\"\\nüí° To investigate:\")\n",
    "    print(\"   1. Check evaluator service logs:\")\n",
    "    print(\"      oc logs -n arhkp-nemo-helm -l app=nemoevaluator --tail=200\")\n",
    "    print(\"   2. Check evaluation job pods:\")\n",
    "    print(\"      oc get pods -n arhkp-nemo-helm | grep -E 'eval|evaluation'\")\n",
    "    print(\"   3. Verify evaluation target exists:\")\n",
    "    print(f\"      requests.get(f'{EVALUATOR_URL}/v1/evaluation/targets/{NMS_NAMESPACE}/llama-3-1b-instruct-customized').json()\")\n",
    "    print(\"   4. Check evaluator pod status:\")\n",
    "    print(\"      oc get pods -n arhkp-nemo-helm | grep evaluator\")\n",
    "    print(\"   5. Check evaluator custom resource:\")\n",
    "    print(\"      oc get nemoevaluator -n arhkp-nemo-helm -o yaml\")\n",
    "\n",
    "# Check if job actually completed successfully\n",
    "if final_status not in [\"completed\", \"success\", \"finished\"]:\n",
    "    if final_status != \"failed\":  # Already handled above\n",
    "        print(f\"\\n‚ö†Ô∏è  Warning: Job status is '{final_status}', not 'completed'\")\n",
    "        print(\"This might indicate the job is still running, failed, or in an unexpected state.\")\n",
    "        print(\"\\nüí° To check cluster logs and pod status:\")\n",
    "        print(f\"   # Check evaluator pods\")\n",
    "        print(f\"   oc get pods -n arhkp-nemo-helm | grep evaluator\")\n",
    "        print(f\"   \")\n",
    "        print(f\"   # Check evaluator logs\")\n",
    "        print(f\"   oc logs -n arhkp-nemo-helm -l app=nemoevaluator --tail=100\")\n",
    "        print(f\"   \")\n",
    "        print(f\"   # Check for evaluation job pods (if any)\")\n",
    "        print(f\"   oc get pods -n arhkp-nemo-helm | grep -E 'eval|evaluation'\")\n",
    "        print(f\"   \")\n",
    "        print(f\"   # Check job status directly\")\n",
    "        print(f\"   oc get nemoevaluator -n arhkp-nemo-helm\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Job completed successfully with status: {final_status}\")\n",
    "\n",
    "# Also return it so Jupyter displays it\n",
    "job_status\n",
    "\n",
    "print_status(\"Custom model evaluation job completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7409f25-23c1-4574-9392-051f3da0498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poll for evaluation job completion\n",
    "# NOTE: Using localhost (port-forward) is CORRECT for the notebook\n",
    "# The notebook runs locally and accesses services via port-forward\n",
    "print(f\"üöÄ Starting to poll evaluation job: {ft_eval_job_id}\")\n",
    "print(f\"Job URL: {EVALUATOR_URL}/v1/evaluation/jobs/{ft_eval_job_id}\")\n",
    "print(f\"Polling interval: 5 seconds, Timeout: 600 seconds (10 minutes)\\n\")\n",
    "\n",
    "try:\n",
    "    res = wait_eval_job(f\"{EVALUATOR_URL}/v1/evaluation/jobs/{ft_eval_job_id}\", polling_interval=5, timeout=600)\n",
    "except Exception as e:\n",
    "    # If wait_eval_job raised an exception (e.g., job failed), fetch final status for analysis\n",
    "    # Don't re-raise - instead, fetch the job status and continue with error analysis\n",
    "    print(f\"\\n‚ö†Ô∏è  Job failed or exception during polling: {e}\")\n",
    "    print(\"\\nFetching final job status for analysis...\")\n",
    "    try:\n",
    "        final_res = requests.get(f\"{EVALUATOR_URL}/v1/evaluation/jobs/{ft_eval_job_id}\")\n",
    "        if final_res.status_code == 200:\n",
    "            # Use this response as 'res' so the rest of the cell can process it\n",
    "            res = final_res\n",
    "            print(\"‚úÖ Fetched final job status - continuing with error analysis...\\n\")\n",
    "        else:\n",
    "            print(f\"‚ùå Could not fetch final status: HTTP {final_res.status_code}\")\n",
    "            print(f\"Response: {final_res.text}\")\n",
    "            raise Exception(f\"Failed to fetch job status: HTTP {final_res.status_code}\")\n",
    "    except Exception as fetch_error:\n",
    "        print(f\"‚ùå Could not fetch final status: {fetch_error}\")\n",
    "        raise\n",
    "\n",
    "# Check response status\n",
    "if res.status_code != 200:\n",
    "    print(f\"\\n‚ùå Error: Evaluation job status check failed with status {res.status_code}\")\n",
    "    print(f\"Response: {res.text}\")\n",
    "    print(\"\\nüí° To check cluster logs, run:\")\n",
    "    print(f\"   oc logs -n arhkp-nemo-helm -l app=nemoevaluator --tail=100\")\n",
    "    print(f\"   oc get pods -n arhkp-nemo-helm | grep evaluator\")\n",
    "    raise Exception(f\"Failed to get evaluation job status: {res.text}\")\n",
    "\n",
    "# Format and print the response\n",
    "job_status = res.json()\n",
    "final_status = job_status.get(\"status\", \"unknown\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã Final Evaluation Job Status:\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(job_status, indent=2))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract and display error information if job failed\n",
    "if final_status == \"failed\":\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ùå JOB FAILED - Error Analysis:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    status_details = job_status.get(\"status_details\", {})\n",
    "    if status_details:\n",
    "        print(\"\\nStatus Details:\")\n",
    "        print(json.dumps(status_details, indent=2))\n",
    "        \n",
    "        # Look for common error fields\n",
    "        error_fields = [\"error\", \"message\", \"reason\", \"failure_reason\", \"error_message\"]\n",
    "        for field in error_fields:\n",
    "            if field in status_details:\n",
    "                print(f\"\\nüî¥ {field.upper()}: {status_details[field]}\")\n",
    "    \n",
    "    # Check for error in top level\n",
    "    if \"error\" in job_status:\n",
    "        print(f\"\\nüî¥ Top-level error: {job_status['error']}\")\n",
    "    \n",
    "    print(\"\\nüí° Common causes of evaluation job failures:\")\n",
    "    print(\"   1. Evaluation target (model) is not accessible or not found\")\n",
    "    print(\"   2. Evaluation dataset is not accessible or invalid\")\n",
    "    print(\"   3. Insufficient resources (GPU, memory, etc.)\")\n",
    "    print(\"   4. Network connectivity issues between services\")\n",
    "    print(\"   5. Configuration errors in evaluation config\")\n",
    "    \n",
    "    print(\"\\nüí° To investigate:\")\n",
    "    print(\"   1. Check evaluator service logs:\")\n",
    "    print(\"      oc logs -n arhkp-nemo-helm -l app=nemoevaluator --tail=200\")\n",
    "    print(\"   2. Check evaluation job pods:\")\n",
    "    print(\"      oc get pods -n arhkp-nemo-helm | grep -E 'eval|evaluation'\")\n",
    "    print(\"   3. Verify evaluation target exists:\")\n",
    "    print(f\"      requests.get(f'{EVALUATOR_URL}/v1/evaluation/targets/{NMS_NAMESPACE}/llama-3-1b-instruct-customized').json()\")\n",
    "    print(\"   4. Check evaluator pod status:\")\n",
    "    print(\"      oc get pods -n arhkp-nemo-helm | grep evaluator\")\n",
    "    print(\"   5. Check evaluator custom resource:\")\n",
    "    print(\"      oc get nemoevaluator -n arhkp-nemo-helm -o yaml\")\n",
    "\n",
    "# Check if job actually completed successfully\n",
    "if final_status not in [\"completed\", \"success\", \"finished\"]:\n",
    "    print(f\"\\n‚ö†Ô∏è  Warning: Job status is '{final_status}', not 'completed'\")\n",
    "    print(\"This might indicate the job is still running, failed, or in an unexpected state.\")\n",
    "    print(\"\\nüí° To check cluster logs and pod status:\")\n",
    "    print(f\"   # Check evaluator pods\")\n",
    "    print(f\"   oc get pods -n arhkp-nemo-helm | grep evaluator\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   # Check evaluator logs\")\n",
    "    print(f\"   oc logs -n arhkp-nemo-helm -l app=nemoevaluator --tail=100\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   # Check for evaluation job pods (if any)\")\n",
    "    print(f\"   oc get pods -n arhkp-nemo-helm | grep -E 'eval|evaluation'\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   # Check job status directly\")\n",
    "    print(f\"   oc get nemoevaluator -n arhkp-nemo-helm\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Job completed successfully with status: {final_status}\")\n",
    "\n",
    "# Also return it so Jupyter displays it\n",
    "job_status\n",
    "\n",
    "print_status(\"Custom model evaluation job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3706c060-8b37-40e2-b0e6-636d130260e7",
   "metadata": {},
   "source": [
    "### 2.2 Review Evaluation Metrics\n",
    "The following code sends a GET request to retrieve the evaluation results for the fine-tuned model evaluation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2ffdb8-8e8e-403e-a836-8bcd93778814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, check the job status to ensure it's completed\n",
    "print(\"Checking evaluation job status...\")\n",
    "status_res = requests.get(f\"{EVALUATOR_URL}/v1/evaluation/jobs/{ft_eval_job_id}/status\")\n",
    "\n",
    "if status_res.status_code != 200:\n",
    "    print(f\"‚ö†Ô∏è Warning: Could not check job status (Status {status_res.status_code})\")\n",
    "    print(f\"Response: {status_res.text}\")\n",
    "    print(\"\\nAttempting to retrieve results anyway (the job might still be accessible)...\")\n",
    "else:\n",
    "    status_data = status_res.json()\n",
    "    \n",
    "    # Print full status for debugging\n",
    "    print(\"\\nFull status response:\")\n",
    "    print(json.dumps(status_data, indent=2))\n",
    "    \n",
    "    # The /status endpoint returns a different structure than the full job endpoint\n",
    "    # It has \"message\" and \"task_status\" instead of a top-level \"status\" field\n",
    "    job_status = status_data.get(\"status\")\n",
    "    \n",
    "    # If no \"status\" field, infer from message and task_status\n",
    "    if job_status is None:\n",
    "        message = status_data.get(\"message\", \"\").lower()\n",
    "        task_status = status_data.get(\"task_status\", {})\n",
    "        progress = status_data.get(\"progress\", 0)\n",
    "        \n",
    "        # Infer status from message and task status\n",
    "        if \"completed successfully\" in message or \"success\" in message:\n",
    "            # Check if all tasks are completed\n",
    "            if task_status:\n",
    "                all_tasks_completed = all(\n",
    "                    status.lower() in [\"completed\", \"success\", \"finished\"] \n",
    "                    for status in task_status.values()\n",
    "                )\n",
    "                if all_tasks_completed and progress >= 100:\n",
    "                    job_status = \"completed\"\n",
    "                elif all_tasks_completed:\n",
    "                    job_status = \"completed\"  # Progress might not be exactly 100\n",
    "                else:\n",
    "                    job_status = \"running\"  # Some tasks still in progress\n",
    "            elif progress >= 100:\n",
    "                job_status = \"completed\"\n",
    "            else:\n",
    "                job_status = \"running\"\n",
    "        elif \"failed\" in message or \"error\" in message:\n",
    "            job_status = \"failed\"\n",
    "        elif \"running\" in message or progress > 0:\n",
    "            job_status = \"running\"\n",
    "        else:\n",
    "            job_status = \"unknown\"\n",
    "        \n",
    "        print(f\"\\nüìä Inferred job status: {job_status}\")\n",
    "        print(f\"   Message: {status_data.get('message', 'N/A')}\")\n",
    "        print(f\"   Progress: {progress}%\")\n",
    "        if task_status:\n",
    "            print(f\"   Task status: {task_status}\")\n",
    "    else:\n",
    "        print(f\"Job status: {job_status}\")\n",
    "    \n",
    "    # Valid completion statuses\n",
    "    completed_statuses = [\"completed\", \"success\", \"finished\", \"done\"]\n",
    "    \n",
    "    if job_status in completed_statuses:\n",
    "        print(f\"‚úÖ Job is completed (status: {job_status})\")\n",
    "    elif job_status == \"unknown\":\n",
    "        print(\"‚ö†Ô∏è Warning: Job status is 'unknown'\")\n",
    "        print(\"This could mean:\")\n",
    "        print(\"  - The job doesn't exist or was deleted\")\n",
    "        print(\"  - The status endpoint returned an unexpected format\")\n",
    "        print(\"  - The job is in an intermediate state\")\n",
    "        print(\"\\nAttempting to retrieve results anyway...\")\n",
    "    elif job_status == \"failed\":\n",
    "        print(f\"‚ùå Job has failed (status: {job_status})\")\n",
    "        print(\"Check the status details above for error information.\")\n",
    "        print(\"\\nAttempting to retrieve results anyway (may contain error details)...\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: Evaluation job is not completed yet. Status: {job_status}\")\n",
    "        print(\"Valid completion statuses:\", completed_statuses)\n",
    "        print(\"\\nYou can check the status again with:\")\n",
    "        print(f'  requests.get(f\"{EVALUATOR_URL}/v1/evaluation/jobs/{ft_eval_job_id}/status\").json()')\n",
    "        print(\"\\nAttempting to retrieve results anyway (the job might have results even if status is not 'completed')...\")\n",
    "\n",
    "# Now retrieve the results (try even if status check failed or status is unknown)\n",
    "print(\"\\nRetrieving evaluation results...\")\n",
    "res = requests.get(f\"{EVALUATOR_URL}/v1/evaluation/jobs/{ft_eval_job_id}/results\")\n",
    "\n",
    "# Check response status\n",
    "if res.status_code != 200:\n",
    "    print(f\"‚ùå Error retrieving results: Status {res.status_code}\")\n",
    "    print(f\"Response: {res.text}\")\n",
    "    raise Exception(f\"Failed to retrieve evaluation results: {res.text}\")\n",
    "\n",
    "# Explicitly print the results\n",
    "results = res.json()\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(json.dumps(results, indent=2))\n",
    "\n",
    "# Check if tasks are empty\n",
    "if not results.get(\"tasks\") or len(results.get(\"tasks\", {})) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Evaluation results have no tasks!\")\n",
    "    print(\"This could mean:\")\n",
    "    print(\"  1. The evaluation job completed but produced no results\")\n",
    "    print(\"  2. There was an error during evaluation\")\n",
    "    print(\"  3. The evaluation configuration was incorrect\")\n",
    "    print(\"\\nPlease check the evaluation job logs or status for more details.\")\n",
    "    print(f\"Job ID: {ft_eval_job_id}\")\n",
    "\n",
    "# Also return it so Jupyter displays it\n",
    "results\n",
    "\n",
    "print_status(\"Custom model evaluation results retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb03140-21ad-445b-b173-7aebdfb0b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract function name accuracy score\n",
    "# Handle different possible task names and structures (same as base model extraction)\n",
    "result_data = res.json()\n",
    "tasks = result_data.get(\"tasks\", {})\n",
    "\n",
    "# Find the task (could be 'custom-tool-calling' or another name)\n",
    "task_name = None\n",
    "if \"custom-tool-calling\" in tasks:\n",
    "    task_name = \"custom-tool-calling\"\n",
    "elif len(tasks) > 0:\n",
    "    # Use the first task if 'custom-tool-calling' is not found\n",
    "    task_name = list(tasks.keys())[0]\n",
    "    print(f\"‚ö†Ô∏è Note: Using task '{task_name}' instead of 'custom-tool-calling'\")\n",
    "\n",
    "if not task_name or task_name not in tasks:\n",
    "    print(\"‚ùå Error: Could not find evaluation task in results\")\n",
    "    print(f\"Available tasks: {list(tasks.keys())}\")\n",
    "    print(f\"\\nFull results structure:\")\n",
    "    print(json.dumps(result_data, indent=2))\n",
    "    raise KeyError(f\"Task 'custom-tool-calling' not found. Available tasks: {list(tasks.keys())}\")\n",
    "\n",
    "# Extract metrics\n",
    "task_data = tasks[task_name]\n",
    "metrics = task_data.get(\"metrics\", {})\n",
    "tool_calling_metrics = metrics.get(\"tool-calling-accuracy\", {})\n",
    "scores = tool_calling_metrics.get(\"scores\", {})\n",
    "\n",
    "ft_function_name_accuracy_score = scores.get(\"function_name_accuracy\", {}).get(\"value\")\n",
    "ft_function_name_and_args_accuracy = scores.get(\"function_name_and_args_accuracy\", {}).get(\"value\")\n",
    "\n",
    "if ft_function_name_accuracy_score is None or ft_function_name_and_args_accuracy is None:\n",
    "    print(\"‚ö†Ô∏è Warning: Some accuracy scores are missing\")\n",
    "    print(f\"Available scores: {list(scores.keys())}\")\n",
    "    print(f\"\\nFull metrics structure:\")\n",
    "    print(json.dumps(metrics, indent=2))\n",
    "\n",
    "print(f\"Custom model: function_name_accuracy: {ft_function_name_accuracy_score}\")\n",
    "print(f\"Custom model: function_name_and_args_accuracy: {ft_function_name_and_args_accuracy}\")\n",
    "\n",
    "print_status(\"Custom model accuracy scores extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568a204-ad01-4a04-8cfa-602816b8937c",
   "metadata": {},
   "source": [
    "A successfully fine-tuned `meta/llama-3.2-1b-instruct` results in a significant increase in tool calling accuracy with \n",
    "\n",
    "In this case you should observe roughly the following improvements -\n",
    "* function_name_accuracy: 12% to 92%\n",
    "* function_name_and_args_accuracy: 8% to 72%\n",
    "\n",
    "Since this evaluation was on a limited number of samples for demonstration purposes, you may choose to increase `tasks.dataset.limit` in your evaluation config `simple_tool_calling_eval_config`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcc53b6-677b-4b44-bf6e-bcdadef21a73",
   "metadata": {},
   "source": [
    "## (Optional) Next Steps\n",
    "\n",
    "\n",
    "\n",
    "* You may also run the same evaluation on a base `meta/llama-3.1-70B` model for comparison.\n",
    "For this, first you will need to deploy the corresponding NIM using instructions [here](https://build.nvidia.com/meta/llama-3_1-70b-instruct/deploy). After your NIM is deployed, set that endpoint as your evaluation target like so -\n",
    "\n",
    "``` python\n",
    "# Create an evaluation target\n",
    "NIM_URL = \"http://0.0.0.0:8000\"\n",
    "EVAL_TARGET = {\n",
    "    \"type\": \"model\", \n",
    "    \"model\": {\n",
    "       \"api_endpoint\": {\n",
    "         \"url\": f\"{NIM_URL}/v1/completions\",\n",
    "         \"model_id\": \"meta/llama-3.1-70b-instruct\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Start eval job\n",
    "res = requests.post(\n",
    "    f\"{EVALUATOR_URL}/v1/evaluation/jobs\",\n",
    "    json={\n",
    "        \"config\": simple_tool_calling_eval_config,\n",
    "        \"target\": EVAL_TARGET\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "Running evaluation using the default config in this notebook, you should observe `meta/llama-3.1-70B` performance similar to -\n",
    "* function_name_accuracy: 98%\n",
    "* function_name_and_args_accuracy: 66%\n",
    "\n",
    "Remarkably, a LoRA-tuned `meta/llama-3.2-1B` achieves accuracy that is close to a model 70 times its size, even outperforming it in the combined `function_name_and_args_accuracy` score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406f95b-4fc5-462a-ad6c-43196b70896d",
   "metadata": {},
   "source": [
    "You can now proceed with the same processes to fine-tune other NIM for LLMs and evaluate the accuracies between the base model and the fine-tuned model. By doing so, you can produce more accurate models for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24397111-1ab2-460f-bcc6-f510002c8ceb",
   "metadata": {},
   "source": [
    "# Part IV. Adding Safety Guardrails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd80618-8c23-4dd2-b397-28c74294f466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from time import sleep, time\n",
    "from openai import OpenAI\n",
    "\n",
    "print_status(\"Part IV imports completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bab1e8-9aae-4cbd-bdf9-d5c4f4f361ee",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"step-1\"></a>\n",
    "## Step 1: Adding a Guardrails Configuration to the Microservice\n",
    "\n",
    "Start by running the following command which creates a `config.yml` file with the model deployed in the guardrails microservice "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb49932b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce022951-1477-48bd-9f53-fc4eaeb43816",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n",
    "data = {\n",
    "    \"name\": \"demo-self-check-input-output\",\n",
    "    \"namespace\": \"default\",\n",
    "    \"description\": \"demo streaming self-check input and output\",\n",
    "    \"data\": {\n",
    "        \"prompts\": [\n",
    "            {\n",
    "                \"task\": \"self_check_input\",\n",
    "                \"content\": \"Your task is to check if the user message below contains any explicit content or abusive language\"\n",
    "            },\n",
    "            {\n",
    "                \"task\": \"self_check_output\",\n",
    "                \"content\": \"Your task is to check if the bot message below contains any explicit content or abusive language.\"\n",
    "            }\n",
    "        ],\n",
    "        \"instructions\": [\n",
    "            {\n",
    "                \"type\": \"general\",\n",
    "                \"content\": \"Below is a conversation between a user and a bot called the ABC Bot.\\nThe bot is designed to answer employee questions about the ABC Company.\\nThe bot is knowledgeable about the employee handbook and company policies.\\nIf the bot does not know the answer to a question, it truthfully says it does not know.\"\n",
    "            }\n",
    "        ],\n",
    "        \"sample_conversation\": \"user \\\"Hi there. Can you help me with some questions I have about the company?\\\"\\n  express greeting and ask for assistance\\nbot express greeting and confirm and offer assistance\\n  \\\"Hi there! I am here to help answer any questions you may have about the ABC Company. What would you like to know?\\\"\\nuser \\\"What is the company policy on paid time off?\\\"\\n  ask question about benefits\\nbot respond to question about benefits\\n  \\\"The ABC Company provides eligible employees with up to two weeks of paid vacation time per year, as well as five paid sick days per year. Please refer to the employee handbook for more information.\\\"\",\n",
    "        \"models\": [],\n",
    "        \"rails\": {\n",
    "            \"input\": {\n",
    "                \"flows\": [\n",
    "                    \"self check input\"\n",
    "                ]\n",
    "            },\n",
    "            \"output\": {\n",
    "                \"flows\": [\n",
    "                    \"self check output\"\n",
    "                ],\n",
    "                \"streaming\": {\n",
    "                    \"enabled\": \"True\",\n",
    "                    \"chunk_size\": 200,\n",
    "                    \"context_size\": 50,\n",
    "                    \"stream_first\": \"True\"\n",
    "                }\n",
    "            },\n",
    "            \"dialog\": {\n",
    "                \"single_call\": {\n",
    "                    \"enabled\": \"False\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "response = requests.post(f\"{GUARDRAILS_URL}/v1/guardrail/configs\", headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=2))\n",
    "\n",
    "print_status(\"Guardrails configurations listed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335af47a-21b0-47cb-9c39-1a9883a01758",
   "metadata": {},
   "source": [
    "The following REST API call lists the available guardrails configurations. You should be able to see the `toolcalling` configuration - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab0592-2396-4f3e-8e9e-fa056c4e0bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(f\"{GUARDRAILS_URL}/v1/guardrail/configs?page=1&page_size=10&sort=-created_at\")\n",
    "print(json.dumps(response.json(), indent=2))\n",
    "\n",
    "print_status(\"Guardrails test with unsafe query completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2adb5ff-9806-4df6-9fca-85c9077c6c1f",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"step-2\"></a>\n",
    "## Step 2: Evaluate the Safety guardrails\n",
    "\n",
    "With the above guardrails configuration in place, we can now send an example query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc38a1fd-022e-40fa-8833-dc988ed1558a",
   "metadata": {},
   "source": [
    "Now Let's try with Guardrails ON. NeMo Guardrail should not respond to the unsafe user query.\n",
    "\n",
    "### 2.2: Unsafe User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d1090-033a-497b-96fa-6933fd7d850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"{GUARDRAILS_URL}/v1/guardrail/chat/completions\"\n",
    "\n",
    "headers = {\"Accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta/llama-3.2-1b-instruct\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"You are stupid\"}\n",
    "    ],\n",
    "    \"guardrails\": {\n",
    "        \"config_id\": \"demo-self-check-input-output\",\n",
    "    },\n",
    "    \"top_p\": 1\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=2))\n",
    "\n",
    "print_status(\"Guardrails test with unsafe query completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c81e5f-17c6-40e9-a23c-9e2536b57fee",
   "metadata": {},
   "source": [
    "Let's try the safe user query. \n",
    "\n",
    "### 2.3: Safe User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189142fc-8102-4aea-a4e8-49355cbe3a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "url = f\"{GUARDRAILS_URL}/v1/guardrail/completions\"\n",
    "\n",
    "headers = {\"Accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta/llama-3.2-1b-instruct\",\n",
    "    \"prompt\": \"Tell me about Cape Hatteras National Seashore in 50 words or less.\",\n",
    "    \"guardrails\": {\n",
    "      \"config_id\": \"demo-self-check-input-output\"\n",
    "    },\n",
    "    \"temperature\": 1,\n",
    "    \"max_tokens\": 100,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=2))\n",
    "\n",
    "print_status(\"Guardrails test with safe query completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
