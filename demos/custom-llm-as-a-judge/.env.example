# NeMo Microservices Configuration
# Copy this file to .env and fill in your values
# DO NOT commit .env to git (it's in .gitignore)

# Required
NMS_NAMESPACE=anemo-rhoai

# NIM Configuration
# The notebook uses meta-llama3-1b-instruct service by default (standard NIM service)
# This service serves meta/llama-3.2-1b-instruct model
STANDARD_NIM_SERVICE=meta-llama3-1b-instruct  # Default service (can override if needed)

# Optional - Only needed if using external APIs (not used in this tutorial)
# OPENAI_API_KEY=your-openai-api-key
# NVIDIA_API_KEY=your-nvidia-api-key
# HF_TOKEN=your-huggingface-token

# Optional - Data Store token (defaults to "token" if not set)
# NDS_TOKEN=token

# Optional - Service account token (NOT needed - service mesh handles authentication)
# NIM_SERVICE_ACCOUNT_TOKEN=

# Optional
RUN_LOCALLY=true  # Set to true for local development with port-forwards
DATASET_NAME=custom-llm-as-a-judge-eval-data

# Legacy/Alternative NIM Configuration (not recommended - has URL stripping issues with Evaluator v25.06)
# Only use if you must use Knative InferenceService instead of standard NIM service
# EXTERNAL_NIM_SERVICE=anemo-rhoai-predictor-00002
# EXTERNAL_NIM_NAMESPACE=anemo-rhoai
# EXTERNAL_NIM_PORT=80
