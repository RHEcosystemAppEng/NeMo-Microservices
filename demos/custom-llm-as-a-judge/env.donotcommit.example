# NeMo Microservices Configuration
# Copy this file to env.donotcommit and fill in your values
# env.donotcommit is git-ignored and will NOT be committed

# REQUIRED: Namespace for cluster services
# Replace with your actual OpenShift namespace/project name
# Find your namespace: oc projects
NMS_NAMESPACE=your-namespace

# REQUIRED: Set to "false" when running in Workbench (uses cluster URLs)
# Set to "true" only if you're running locally with port-forwards
RUN_LOCALLY=false

# REQUIRED: NIM Model Serving Configuration
# Replace with your actual InferenceService name
# Find your service: oc get inferenceservice -n <your-namespace>
NIM_MODEL_SERVING_SERVICE=your-inferenceservice-name
NIM_MODEL_SERVING_MODEL=meta/llama-3.2-1b-instruct

# REQUIRED: External URL for NIM Model Serving (HTTPS)
# This is the external URL from the InferenceService status
# Find your URL: oc get inferenceservice <name> -n <namespace> -o jsonpath='{.status.url}'
# Format: https://<service-name>-<namespace>.apps.<cluster-domain>
# Example: https://my-model-my-namespace.apps.my-cluster.example.com
NIM_MODEL_SERVING_URL_EXTERNAL=https://your-service-name-your-namespace.apps.your-cluster-domain.com

# Configuration flags
USE_NIM_MODEL_SERVING=true
USE_EXTERNAL_URL=true

# OPTIONAL: Dataset name for evaluation data
DATASET_NAME=custom-llm-as-a-judge-eval-data

# REQUIRED: NIM Service Account Token (for LlamaStack and fallback)
# Kubernetes service account token (JWT) for authenticating with KServe InferenceService
# This token is REQUIRED for LlamaStack to work with authenticated KServe services
# Get your token: oc create token <service-account-name> -n <your-namespace> --duration=8760h
# Example: oc create token my-model-sa -n my-namespace --duration=8760h
# The service account name is typically: <inferenceservice-name>-sa
# Find your service account: oc get sa -n <your-namespace> | grep model
NIM_SERVICE_ACCOUNT_TOKEN=

# OPTIONAL: NeMo Data Store token
# Default is "token" - update if your deployment uses a different token
NDS_TOKEN=token

# OPTIONAL: API Keys (only needed if using external APIs as fallback)
# OPENAI_API_KEY=
# NVIDIA_API_KEY=
# HF_TOKEN=
