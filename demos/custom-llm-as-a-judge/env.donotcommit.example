# NeMo Microservices Configuration
# Copy this file to env.donotcommit and fill in your values
# env.donotcommit is git-ignored and will NOT be committed

# REQUIRED: Namespace for cluster services
# Replace with your actual OpenShift namespace/project name
NMS_NAMESPACE=your-namespace

# REQUIRED: Set to "false" when running in Workbench (uses cluster URLs)
# Set to "true" only if you're running locally with port-forwards
RUN_LOCALLY=false

# OPTIONAL: NIM Model Serving Configuration
# Replace with your actual InferenceService name (find with: oc get inferenceservice -n <namespace>)
NIM_MODEL_SERVING_SERVICE=your-nim-service-name
NIM_MODEL_SERVING_MODEL=meta/llama-3.2-1b-instruct
# Replace with your actual external URL (HTTPS)
# Format: https://<service-name>-<namespace>.apps.<cluster-domain>
# Find with: oc get route -n <namespace> | grep <service-name>
NIM_MODEL_SERVING_URL_EXTERNAL=https://your-service-name-your-namespace.apps.your-cluster-domain.com
USE_NIM_MODEL_SERVING=true
USE_EXTERNAL_URL=true

# REQUIRED: Service Account Token for NIM Model Serving authentication
# Get this token from your service account secret:
# oc get secret <service-account-name> -n <namespace> -o jsonpath='{.data.token}' | base64 -d
# Or from the service account directly:
# oc get sa <service-account-name> -n <namespace> -o jsonpath='{.secrets[0].name}' | xargs oc get secret -n <namespace> -o jsonpath='{.data.token}' | base64 -d
NIM_SERVICE_ACCOUNT_TOKEN=

# OPTIONAL: API Keys (only needed if using external APIs)
# OPENAI_API_KEY=
# NVIDIA_API_KEY=
# HF_TOKEN=
