{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fda40cc-5239-4387-963e-c09af7ebd671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "from time import sleep, time\n",
    "from openai import OpenAI\n",
    "import asyncio\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ffcd8-b75d-458f-9621-5e535bf1315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key for NVIDIA provider (required even for self-hosted services)\n",
    "os.environ[\"NVIDIA_API_KEY\"] = NDS_TOKEN\n",
    "\n",
    "# Metadata associated with Datasets and Customization Jobs\n",
    "os.environ[\"NVIDIA_DATASET_NAMESPACE\"] = NMS_NAMESPACE\n",
    "os.environ[\"NVIDIA_PROJECT_ID\"] = PROJECT_ID\n",
    "\n",
    "## Inference env vars\n",
    "os.environ[\"NVIDIA_BASE_URL\"] = NIM_URL\n",
    "\n",
    "# Data Store env vars\n",
    "os.environ[\"NVIDIA_DATASETS_URL\"] = ENTITY_STORE_URL\n",
    "\n",
    "## Customizer env vars\n",
    "os.environ[\"NVIDIA_CUSTOMIZER_URL\"] = CUSTOMIZER_URL\n",
    "os.environ[\"NVIDIA_OUTPUT_MODEL_DIR\"] = CUSTOMIZED_MODEL_DIR\n",
    "\n",
    "# Evaluator env vars\n",
    "os.environ[\"NVIDIA_EVALUATOR_URL\"] = EVALUATOR_URL\n",
    "\n",
    "# Guardrails env vars\n",
    "os.environ[\"GUARDRAILS_SERVICE_URL\"] = GUARDRAILS_URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeecaa8d-02b5-4fd4-9faa-6bf9c7daa830",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data Store endpoint: {DATA_STORE_URL}\")\n",
    "print(f\"Entity Store endpoint: {ENTITY_STORE_URL}\")\n",
    "print(f\"Customizer endpoint: {CUSTOMIZER_URL}\")\n",
    "print(f\"Evaluator endpoint: {EVALUATOR_URL}\")\n",
    "print(f\"NIM endpoint: {NIM_URL}\")\n",
    "print(f\"Namespace: {NMS_NAMESPACE}\")\n",
    "print(f\"Base Model for Customization: {BASE_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a67604-427b-4f41-ad63-3bc62c7dcad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack.core.library_client import LlamaStackAsLibraryClient\n",
    "\n",
    "client = LlamaStackAsLibraryClient(\"nvidia\")\n",
    "client.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e75c3-bdb2-4300-a6ab-ef97fd87cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack.apis.common.job_types import JobStatus\n",
    "from llama_stack.core.datatypes import Api\n",
    "import asyncio\n",
    "\n",
    "def wait_customization_job(job_id: str, polling_interval: int = 30, timeout: int = 5500):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Access post_training through impls\n",
    "    post_training = client.async_client.impls[Api.post_training]\n",
    "    \n",
    "    # Get initial status using async\n",
    "    loop = asyncio.get_event_loop()\n",
    "    res = loop.run_until_complete(post_training.get_training_job_status(job_uuid=job_id))\n",
    "    job_status = res.status\n",
    "\n",
    "    print(f\"Waiting for Customization job {job_id} to finish.\")\n",
    "    print(f\"Job status: {job_status} after {time.time() - start_time} seconds.\")\n",
    "\n",
    "    while job_status in [JobStatus.scheduled.value, JobStatus.in_progress.value]:\n",
    "        sleep(polling_interval)\n",
    "        res = loop.run_until_complete(post_training.get_training_job_status(job_uuid=job_id))\n",
    "        job_status = res.status\n",
    "\n",
    "        print(f\"Job status: {job_status} after {time.time() - start_time} seconds.\")\n",
    "\n",
    "        if time.time() - start_time > timeout:\n",
    "            raise RuntimeError(f\"Customization Job {job_id} took more than {timeout} seconds.\")\n",
    "\n",
    "    return job_status\n",
    "\n",
    "\n",
    "# When creating a customized model, NIM asynchronously loads the model in its model registry.\n",
    "# After this, we can run inference with the new model. This helper function waits for NIM to pick up the new model.\n",
    "def wait_nim_loads_customized_model(model_id: str, polling_interval: int = 10, timeout: int = 300):\n",
    "    found = False\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Checking if NIM has loaded customized model {model_id}.\")\n",
    "\n",
    "    while not found:\n",
    "        sleep(polling_interval)\n",
    "\n",
    "        res = requests.get(f\"{NIM_URL}/v1/models\")\n",
    "        if model_id in [model[\"id\"] for model in res.json()[\"data\"]]:\n",
    "            found = True\n",
    "            print(f\"Model {model_id} available after {time.time() - start_time} seconds.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Model {model_id} not available after {time.time() - start_time} seconds.\")\n",
    "\n",
    "    if not found:\n",
    "        raise RuntimeError(f\"Model {model_id} not available after {timeout} seconds.\")\n",
    "\n",
    "    assert found, f\"Could not find model {model_id} in the list of available models.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5a5d1a-eeda-4f9a-bd6a-977c07589306",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = f\"{NMS_NAMESPACE}/{DATASET_NAME}\"\n",
    "print(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0fce0f-0f2a-4006-9611-99b9f665322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.datasets.register(\n",
    "    purpose=\"post-training/messages\",\n",
    "    dataset_id=DATASET_NAME,\n",
    "    source={\n",
    "        \"type\": \"uri\",\n",
    "        \"uri\": f\"hf://datasets/{repo_id}\"\n",
    "    },\n",
    "    metadata={\n",
    "        \"format\": \"json\",\n",
    "        \"description\": \"Tool calling xLAM dataset in OpenAI ChatCompletions format\",\n",
    "        \"provider_id\": \"nvidia\"\n",
    "    }\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530839f6-d74e-4daa-a685-d3f87f93e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url=f\"{ENTITY_STORE_URL}/v1/datasets/{NMS_NAMESPACE}/{DATASET_NAME}\")\n",
    "assert res.status_code in (200, 201), f\"Status Code {res.status_code} Failed to fetch dataset {res.text}\"\n",
    "dataset_obj = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462dd67f-a129-432e-940a-611bcb7daf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Files URL:\", dataset_obj[\"files_url\"])\n",
    "assert dataset_obj[\"files_url\"] == f\"hf://datasets/{repo_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c0966-64b1-4b56-a76d-21ab3fc3ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack.apis.post_training import LoraFinetuningConfig\n",
    "import inspect\n",
    "\n",
    "print(\"LoraFinetuningConfig signature:\")\n",
    "print(inspect.signature(LoraFinetuningConfig))\n",
    "\n",
    "if hasattr(LoraFinetuningConfig, 'model_fields'):\n",
    "    print(\"\\nLoraFinetuningConfig fields:\")\n",
    "    for field_name, field_info in LoraFinetuningConfig.model_fields.items():\n",
    "        required = field_info.is_required()\n",
    "        print(f\"  {field_name}: required={required}, default={field_info.default}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e03e04-4697-4999-97e8-b1a58917e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from llama_stack.core.datatypes import Api\n",
    "from llama_stack.apis.post_training import (\n",
    "    TrainingConfig,\n",
    "    DataConfig,\n",
    "    OptimizerConfig,\n",
    "    LoraFinetuningConfig,\n",
    "    DatasetFormat,\n",
    "    OptimizerType,\n",
    ")\n",
    "\n",
    "unique_suffix = int(time.time())\n",
    "\n",
    "# Access post_training through impls\n",
    "post_training = client.async_client.impls[Api.post_training]\n",
    "\n",
    "# Create proper config objects with all required fields\n",
    "data_config = DataConfig(\n",
    "    batch_size=16,\n",
    "    dataset_id=DATASET_NAME,\n",
    "    shuffle=True,\n",
    "    data_format=DatasetFormat.instruct\n",
    ")\n",
    "\n",
    "optimizer_config = OptimizerConfig(\n",
    "    optimizer_type=OptimizerType.adamw,\n",
    "    lr=0.0001,\n",
    "    weight_decay=0.01,\n",
    "    num_warmup_steps=100\n",
    ")\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    n_epochs=2,\n",
    "    data_config=data_config,\n",
    "    optimizer_config=optimizer_config\n",
    ")\n",
    "\n",
    "# LoRA configuration with correct fields\n",
    "algorithm_config = LoraFinetuningConfig(\n",
    "    lora_attn_modules=[],\n",
    "    apply_lora_to_mlp=True,\n",
    "    apply_lora_to_output=False,\n",
    "    rank=8,\n",
    "    alpha=16,\n",
    "    use_dora=False,\n",
    "    quantize_base=False\n",
    ")\n",
    "\n",
    "# Convert to dict to work around the bug\n",
    "training_config_dict = training_config.model_dump()\n",
    "\n",
    "# Now call the supervised_fine_tune method with dict\n",
    "res = await post_training.supervised_fine_tune(\n",
    "    job_uuid=f\"finetune-{unique_suffix}\",\n",
    "    model=\"meta/llama-3.2-1b-instruct@v1.0.0\",\n",
    "    training_config=training_config_dict,  # Pass as dict\n",
    "    algorithm_config=algorithm_config,\n",
    "    hyperparam_search_config=None,\n",
    "    logger_config=None,\n",
    "    checkpoint_dir=\"\",\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eca4035-860a-45c0-9f17-f793e1d6b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = res.model_dump()\n",
    "\n",
    "# To job track status\n",
    "JOB_ID = job[\"id\"]\n",
    "\n",
    "# This will be the name of the model that will be used to send inference queries to\n",
    "CUSTOMIZED_MODEL = job[\"output_model\"]\n",
    "print(JOB_ID)\n",
    "print(CUSTOMIZED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abaf4e4-339e-47d5-b1c6-71aed99dac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_status = wait_customization_job(job_id=JOB_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138fcaa5-0c27-4cc2-bf1e-8eabb769c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(f\"{ENTITY_STORE_URL}/v1/models\", params={\"filter[namespace]\": NMS_NAMESPACE, \"sort\" : \"-created_at\"})\n",
    "\n",
    "assert response.status_code == 200, f\"Status Code {response.status_code}: Request failed. Response: {response.text}\"\n",
    "print(\"Response JSON:\", json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11095c5e-e2ca-420b-af5c-f3cd398949de",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_nim_loads_customized_model(model_id=CUSTOMIZED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d608db8-4dd0-4a0c-be1a-f88de37359b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the model is in NIM\n",
    "resp = requests.get(f\"{NIM_URL}/v1/models\")\n",
    "models = resp.json().get(\"data\", [])\n",
    "model_names = [model[\"id\"] for model in models]\n",
    "\n",
    "print(\"Available models in NIM:\")\n",
    "for name in model_names:\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "assert CUSTOMIZED_MODEL in model_names, f\"Model {CUSTOMIZED_MODEL} not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de32b72-2dd3-4519-af47-03c8a4ab31f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack.core.datatypes import Api\n",
    "\n",
    "# Get the NVIDIA inference provider directly\n",
    "inference_router = client.async_client.impls[Api.inference]\n",
    "nvidia_provider = inference_router.routing_table.impls_by_provider_id.get(\"nvidia\")\n",
    "\n",
    "if nvidia_provider:\n",
    "    # Get fresh list of models from NVIDIA provider\n",
    "    models_from_provider = await nvidia_provider.list_models()\n",
    "    print(\"Models from NVIDIA provider:\")\n",
    "    for model in models_from_provider:\n",
    "        print(f\"  - {model.provider_resource_id}\")\n",
    "    \n",
    "    # Now update the routing table with these models\n",
    "    models_routing_table = client.async_client.impls[Api.models]\n",
    "    await models_routing_table.update_registered_models(\n",
    "        provider_id=\"nvidia\",\n",
    "        models=models_from_provider\n",
    "    )\n",
    "    print(\"\\nRouting table updated!\")\n",
    "    \n",
    "else:\n",
    "    print(\"NVIDIA provider not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39217ee3-70cb-4124-b01b-65dfd600bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed data will be stored here\n",
    "DATA_ROOT = os.path.join(os.getcwd(), \"sample_data\")\n",
    "CUSTOMIZATION_DATA_ROOT = os.path.join(DATA_ROOT, \"customization\")\n",
    "VALIDATION_DATA_ROOT = os.path.join(DATA_ROOT, \"validation\")\n",
    "EVALUATION_DATA_ROOT = os.path.join(DATA_ROOT, \"evaluation\")\n",
    "\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(CUSTOMIZATION_DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(VALIDATION_DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(EVALUATION_DATA_ROOT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081a209c-6480-4a4a-98e9-bb25a01b67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fp = f\"{CUSTOMIZATION_DATA_ROOT}/training.jsonl\"\n",
    "assert os.path.exists(train_fp), f\"The training data at '{train_fp}' does not exist. Please ensure that the data was prepared successfully.\"\n",
    "\n",
    "val_fp = f\"{VALIDATION_DATA_ROOT}/validation.jsonl\"\n",
    "assert os.path.exists(val_fp), f\"The validation data at '{val_fp}' does not exist. Please ensure that the data was prepared successfully.\"\n",
    "\n",
    "test_fp = f\"{EVALUATION_DATA_ROOT}/xlam-test-single.jsonl\"\n",
    "assert os.path.exists(test_fp), f\"The test data at '{test_fp}' does not exist. Please ensure that the data was prepared successfully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f3d146-1774-4a1e-9542-81adadca8cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path):\n",
    "    \"\"\"Reads a JSON Lines file and yields parsed JSON objects\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()  # Remove leading/trailing whitespace\n",
    "            if not line:\n",
    "                continue  # Skip empty lines\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "test_data = list(read_jsonl(test_fp))\n",
    "\n",
    "print(f\"There are {len(test_data)} examples in the test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e32dfb4-e2e8-4fe2-8453-c7696338f441",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Randomly choose\n",
    "test_sample = random.choice(test_data)\n",
    "\n",
    "# Transform tools to format expected by Llama Stack client\n",
    "for i, tool in enumerate(test_sample['tools']):\n",
    "    # Extract properties we will map to the expected format\n",
    "    tool = tool.get('function', {})\n",
    "    tool_name = tool.get('name')\n",
    "    tool_description = tool.get('description')\n",
    "    tool_params = tool.get('parameters', {})\n",
    "    tool_params_properties = tool_params.get('properties', {})\n",
    "\n",
    "    # Create object of parameters for this tool\n",
    "    transformed_parameters = {}\n",
    "    for name, property in tool_params_properties.items():\n",
    "        transformed_param = {\n",
    "            'param_type': property.get('type'),\n",
    "            'description': property.get('description')\n",
    "        }\n",
    "        if 'default' in property:\n",
    "            transformed_param['default'] = property['default']\n",
    "        if 'required' in property:\n",
    "            transformed_param['required'] = property['required']\n",
    "\n",
    "        transformed_parameters[name] = transformed_param\n",
    "\n",
    "    # Update this tool in-place using the expected format\n",
    "    test_sample['tools'][i] = {\n",
    "        'tool_name': tool_name,\n",
    "        'description': tool_description,\n",
    "        'parameters': transformed_parameters\n",
    "    }\n",
    "\n",
    "# Visualize the inputs to the LLM - user query and available tools\n",
    "test_sample['messages']\n",
    "test_sample['tools']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ff4b7-c526-4bcc-87b9-639e1ccba9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack.core.datatypes import Api\n",
    "\n",
    "# Use the registered model ID\n",
    "REGISTERED_MODEL_ID = \"nvidia/nvidia-tool-calling-tutorial/test-llama-stack@v1\"\n",
    "\n",
    "# Transform tools back to OpenAI format\n",
    "openai_tools = []\n",
    "for tool in test_sample['tools']:\n",
    "    # Check if it's already in OpenAI format (has 'function' key)\n",
    "    if 'function' in tool:\n",
    "        openai_tools.append(tool)\n",
    "    else:\n",
    "        # Convert from Llama Stack format to OpenAI format\n",
    "        openai_tool = {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool.get('tool_name'),\n",
    "                \"description\": tool.get('description'),\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {},\n",
    "                    \"required\": []\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        # Convert parameters\n",
    "        for param_name, param_info in tool.get('parameters', {}).items():\n",
    "            openai_tool[\"function\"][\"parameters\"][\"properties\"][param_name] = {\n",
    "                \"type\": param_info.get('param_type'),\n",
    "                \"description\": param_info.get('description', '')\n",
    "            }\n",
    "            if param_info.get('default') is not None:\n",
    "                openai_tool[\"function\"][\"parameters\"][\"properties\"][param_name][\"default\"] = param_info['default']\n",
    "            if param_info.get('required', False):\n",
    "                openai_tool[\"function\"][\"parameters\"][\"required\"].append(param_name)\n",
    "        \n",
    "        openai_tools.append(openai_tool)\n",
    "\n",
    "print(\"OpenAI formatted tools:\")\n",
    "print(openai_tools[0] if openai_tools else \"No tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9e61d1-e731-42d2-a8ac-709fc1345d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack.core.datatypes import Api\n",
    "from llama_stack.apis.inference import OpenAIChatCompletionRequestWithExtraBody\n",
    "\n",
    "# Access inference through impls\n",
    "inference = client.async_client.impls[Api.inference]\n",
    "\n",
    "# Create request with OpenAI-formatted tools\n",
    "request = OpenAIChatCompletionRequestWithExtraBody(\n",
    "    model=REGISTERED_MODEL_ID,\n",
    "    messages=test_sample[\"messages\"],\n",
    "    tools=openai_tools,  # Use the converted tools\n",
    "    tool_choice=\"auto\",\n",
    "    stream=False,\n",
    "    max_tokens=512,\n",
    "    temperature=0.1,\n",
    "    top_p=0.7,\n",
    ")\n",
    "\n",
    "# Make the chat completion call\n",
    "completion = await inference.openai_chat_completion(params=request)\n",
    "\n",
    "print(\"Tool calls from model:\")\n",
    "if hasattr(completion, 'choices') and len(completion.choices) > 0:\n",
    "    print(completion.choices[0].message.tool_calls)\n",
    "else:\n",
    "    print(completion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c10a4d-3589-42d2-a3aa-f5a35aae739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample['tool_calls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5455c-e690-4e8d-9be4-35f9118baf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Name of your custom model is: {CUSTOMIZED_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a83600-9438-4029-b95d-81ed10c82f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
