{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6097a43a-7a49-4c0b-8289-0057a609d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "from time import sleep, time\n",
    "from openai import OpenAI\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f13eb21-8975-4f78-bede-4d0e0123959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables for LlamaStack\n",
    "os.environ[\"NVIDIA_DATASET_NAMESPACE\"] = NMS_NAMESPACE\n",
    "os.environ[\"NVIDIA_PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"NVIDIA_BASE_URL\"] = NIM_URL\n",
    "os.environ[\"NVIDIA_DATASETS_URL\"] = ENTITY_STORE_URL\n",
    "os.environ[\"NVIDIA_CUSTOMIZER_URL\"] = CUSTOMIZER_URL\n",
    "os.environ[\"NVIDIA_OUTPUT_MODEL_DIR\"] = CUSTOMIZED_MODEL_DIR\n",
    "os.environ[\"NVIDIA_EVALUATOR_URL\"] = EVALUATOR_URL\n",
    "os.environ[\"GUARDRAILS_SERVICE_URL\"] = GUARDRAILS_URL\n",
    "os.environ[\"NVIDIA_GUARDRAILS_CONFIG_ID\"] = \"demo-self-check-input-output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c1d1a-5ed6-4992-996c-2a9ca6c41943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack.core.library_client import LlamaStackAsLibraryClient\n",
    "\n",
    "client = LlamaStackAsLibraryClient(\"nvidia\")\n",
    "client.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bebf53-7279-4dd2-a4d0-fccce2e52998",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data Store endpoint: {NDS_URL}\")\n",
    "print(f\"Entity Store endpoint: {ENTITY_STORE_URL}\")\n",
    "print(f\"Customizer endpoint: {CUSTOMIZER_URL}\")\n",
    "print(f\"Evaluator endpoint: {EVALUATOR_URL}\")\n",
    "print(f\"Guardrails endpoint: {GUARDRAILS_URL}\")\n",
    "print(f\"NIM endpoint: {NIM_URL}\")\n",
    "print(f\"Namespace: {NMS_NAMESPACE}\")\n",
    "print(f\"Base Model: {BASE_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3db84c-a5d7-45a9-b3dc-2e047c43a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "simple_working_config = {\n",
    "    \"name\": \"demo-self-check-input-output\",\n",
    "    \"namespace\": \"default\",\n",
    "    \"description\": \"Simple content moderation without self-check flows\",\n",
    "    \"data\": {\n",
    "        \"models\": [\n",
    "            {\n",
    "                \"type\": \"main\",\n",
    "                \"engine\": \"openai\",\n",
    "                \"model\": BASE_MODEL\n",
    "            }\n",
    "        ],\n",
    "        \"rails\": {\n",
    "            \"input\": {\n",
    "                \"flows\": []  # Empty for now - no self-check\n",
    "            },\n",
    "            \"output\": {\n",
    "                \"flows\": []  # Empty for now - no self-check\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{GUARDRAILS_URL}/v1/guardrail/configs\",\n",
    "    headers=headers,\n",
    "    json=simple_working_config\n",
    ")\n",
    "\n",
    "if response.status_code in (200, 201):\n",
    "    print(\"✓ Simplified guardrails configuration created\")\n",
    "    print(json.dumps(response.json(), indent=2))\n",
    "else:\n",
    "    print(f\"❌ Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05952e4b-c4f9-41c6-a100-043657c4cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the configuration was created\n",
    "response = requests.get(f\"{GUARDRAILS_URL}/v1/guardrail/configs?page=1&page_size=10&sort=-created_at\")\n",
    "\n",
    "print(f\"Response status: {response.status_code}\\n\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    configs = response.json()\n",
    "    \n",
    "    # Handle different response formats\n",
    "    if isinstance(configs, list):\n",
    "        # Response is a direct list\n",
    "        config_list = configs\n",
    "    elif isinstance(configs, dict):\n",
    "        # Response is a dict, check for common keys\n",
    "        config_list = configs.get('items', configs.get('data', configs.get('configs', [])))\n",
    "    else:\n",
    "        config_list = []\n",
    "    \n",
    "    print(f\"Found {len(config_list)} guardrails configurations:\\n\")\n",
    "    \n",
    "    if len(config_list) > 0:\n",
    "        for config in config_list:\n",
    "            print(f\"  - {config.get('name', 'Unknown')}: {config.get('description', 'No description')}\")\n",
    "    else:\n",
    "        # If list is empty, try to get the specific config directly\n",
    "        print(\"List returned empty, trying direct access...\\n\")\n",
    "        direct_response = requests.get(f\"{GUARDRAILS_URL}/v1/guardrail/configs/default/demo-self-check-input-output\")\n",
    "        if direct_response.status_code == 200:\n",
    "            config = direct_response.json()\n",
    "            print(f\"✓ Found config directly:\")\n",
    "            print(f\"  - {config.get('name')}: {config.get('description')}\")\n",
    "        else:\n",
    "            print(f\"Could not retrieve config directly: {direct_response.status_code}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code} - {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee284aa-dd89-4981-b238-bbdbf1ee1c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"{GUARDRAILS_URL}/v1/guardrail/chat/completions\"\n",
    "headers = {\"Accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "unsafe_query = {\n",
    "    \"model\": BASE_MODEL,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"You are stupid and useless\"}\n",
    "    ],\n",
    "    \"guardrails\": {\n",
    "        \"config_id\": \"demo-self-check-input-output\"\n",
    "    },\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 100\n",
    "}\n",
    "\n",
    "print(\"Testing UNSAFE query with guardrails...\\n\")\n",
    "response = requests.post(url, headers=headers, json=unsafe_query)\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"Response:\\n{json.dumps(response.json(), indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84a4be2-1eeb-41b1-bffa-494b2390e7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_query = {\n",
    "    \"model\": BASE_MODEL,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ],\n",
    "    \"guardrails\": {\n",
    "        \"config_id\": \"demo-self-check-input-output\"\n",
    "    },\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 100\n",
    "}\n",
    "\n",
    "print(\"Testing SAFE query with guardrails...\\n\")\n",
    "response = requests.post(url, headers=headers, json=safe_query)\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"Response:\\n{json.dumps(response.json(), indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c65f0-557b-4f95-abae-0b6c68c3798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "completions_url = f\"{GUARDRAILS_URL}/v1/guardrail/completions\"\n",
    "\n",
    "completions_query = {\n",
    "    \"model\": BASE_MODEL,\n",
    "    \"prompt\": \"Tell me about machine learning in 50 words or less.\",\n",
    "    \"guardrails\": {\n",
    "        \"config_id\": \"demo-self-check-input-output\"\n",
    "    },\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 100,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "print(\"Testing with completions API...\\n\")\n",
    "response = requests.post(completions_url, headers=headers, json=completions_query)\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"Response:\\n{json.dumps(response.json(), indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c50f06-bfc1-4688-a1a9-f39fc53a7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OpenAI client for direct NIM access (without guardrails)\n",
    "nim_client = OpenAI(\n",
    "    base_url=f\"{NIM_URL}/v1\",\n",
    "    api_key=\"None\"\n",
    ")\n",
    "\n",
    "test_queries = [\n",
    "    \"You are completely useless\",  # Unsafe\n",
    "    \"What is the weather like today?\",  # Safe\n",
    "    \"Tell me about Python programming\"  # Safe\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARISON: Guardrails ON vs OFF\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test {i}: '{query}'\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Without guardrails (direct NIM)\n",
    "    print(\"[WITHOUT GUARDRAILS]\")\n",
    "    try:\n",
    "        response = nim_client.chat.completions.create(\n",
    "            model=BASE_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": query}],\n",
    "            max_tokens=100,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        print(f\"Response: {response.choices[0].message.content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    # With guardrails\n",
    "    print(\"\\n[WITH GUARDRAILS]\")\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{GUARDRAILS_URL}/v1/guardrail/chat/completions\",\n",
    "            headers={\"Accept\": \"application/json\", \"Content-Type\": \"application/json\"},\n",
    "            json={\n",
    "                \"model\": BASE_MODEL,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": query}],\n",
    "                \"guardrails\": {\"config_id\": \"demo-self-check-input-output\"},\n",
    "                \"temperature\": 0.7,\n",
    "                \"max_tokens\": 100\n",
    "            }\n",
    "        )\n",
    "        result = response.json()\n",
    "        if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "            print(f\"Response: {result['choices'][0]['message']['content']}\")\n",
    "        else:\n",
    "            print(f\"Blocked or modified response: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Comparison complete!\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7224e4d0-7fdc-41a6-ae28-e05ab080585e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
