---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llamastack-config
  labels:
    "app.kubernetes.io/name": "nemo-llamastack"
    "app.kubernetes.io/instance": "nemo-llamastack"
data:
  run.yaml: |
    version: 2
    image_name: nvidia
    apis:
      - agents
      - datasetio
      - eval
      - files
      - inference
      - post_training
      - safety
      - scoring
      - tool_runtime
      - vector_io
    providers:
      inference:
        - provider_id: nvidia
          provider_type: remote::nvidia
          config:
            url: ${env.NVIDIA_BASE_URL:=https://integrate.api.nvidia.com}
            api_key: ${env.NVIDIA_API_KEY}
            append_api_version: ${env.NVIDIA_APPEND_API_VERSION:=True}
      vector_io:
        - provider_id: faiss
          provider_type: inline::faiss
          config:
            persistence:
              backend: meta-reference
              namespace: default
      safety:
        - provider_id: nvidia
          provider_type: remote::nvidia
          config:
            guardrails_service_url: ${env.GUARDRAILS_SERVICE_URL:=http://nemoguardrails-sample.YOUR_NEMO_NAMESPACE.svc.cluster.local:8000}
            config_id: ${env.NVIDIA_GUARDRAILS_CONFIG_ID:=self-check}
            model: ${env.SAFETY_MODEL:=meta/llama-3.2-1b-instruct}
      agents:
        - provider_id: meta-reference
          provider_type: inline::meta-reference
          config:
            persistence:
              agent_state:
                backend: meta-reference
                namespace: default
                table_name: agent_state
              responses:
                backend: meta-reference-sql
                namespace: default
                table_name: agent_responses
      eval:
        - provider_id: nvidia
          provider_type: remote::nvidia
          config:
            evaluator_url: ${env.NVIDIA_EVALUATOR_URL:=http://nemoevaluator-sample.YOUR_NEMO_NAMESPACE.svc.cluster.local:8000}
      post_training:
        - provider_id: nvidia
          provider_type: remote::nvidia
          config:
            api_key: ${env.NVIDIA_API_KEY:=}
            dataset_namespace: ${env.NVIDIA_DATASET_NAMESPACE:=default}
            project_id: ${env.NVIDIA_PROJECT_ID:=test-project}
            customizer_url: ${env.NVIDIA_CUSTOMIZER_URL:=http://nemo.test}
      datasetio:
        - provider_id: nvidia
          provider_type: remote::nvidia
          config:
            api_key: ${env.NVIDIA_API_KEY:=}
            dataset_namespace: ${env.NVIDIA_DATASET_NAMESPACE:=xlam-tutorial-ns}
            project_id: ${env.NVIDIA_PROJECT_ID:=test-project}
            datasets_url: ${env.NVIDIA_ENTITY_STORE_URL:=http://nemoentitystore-sample.YOUR_NEMO_NAMESPACE.svc.cluster.local:8000}
        - provider_id: localfs
          provider_type: inline::localfs
          config:
            kvstore:
              backend: meta-reference
              namespace: default
              table_name: datasets
      files:
        - provider_id: localfs
          provider_type: inline::localfs
          config:
            storage_dir: /tmp/files
            metadata_store:
              backend: meta-reference-sql
              namespace: default
              table_name: files
      scoring:
        - provider_id: basic
          provider_type: inline::basic
      tool_runtime:
        - provider_id: rag-runtime
          provider_type: inline::rag-runtime
    storage:
      backends:
        meta-reference:
          type: kv_sqlite
          db_path: /tmp/storage_store.db
        meta-reference-sql:
          type: sql_sqlite
          db_path: /tmp/sql_storage_store.db
      stores:
        metadata:
          backend: meta-reference
          namespace: default
        conversations:
          backend: meta-reference-sql
          namespace: default
          table_name: conversations
        inference:
          backend: meta-reference-sql
          namespace: default
          table_name: inference
        prompts:
          backend: meta-reference
          namespace: default
    models:
      - metadata: { }
        model_id: ${env.INFERENCE_MODEL}
        provider_id: nvidia
        model_type: llm
      - metadata: { }
        model_id: ${env.SAFETY_MODEL}
        provider_id: nvidia
        model_type: llm
    shields:
      - shield_id: demo-self-check-input-output
        provider_id: nvidia
        provider_shield_id: demo-self-check-input-output
        params:
          model: ${env.SAFETY_MODEL:=meta/llama-3.2-1b-instruct}
    vector_dbs: [ ]
    datasets: [ ]
    scoring_fns: [ ]
    benchmarks: [ ]
    tool_groups:
      - toolgroup_id: builtin::rag
        provider_id: rag-runtime
    server:
      port: 8321