{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94783c11-9f06-4785-8b54-8d309b7bbf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from time import sleep, time\n",
    "from openai import OpenAI\n",
    "import asyncio\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a26df36-6d90-4631-bbe7-a97225d3a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NVIDIA_DATASET_NAMESPACE\"] = NMS_NAMESPACE\n",
    "os.environ[\"NVIDIA_PROJECT_ID\"] = PROJECT_ID\n",
    "\n",
    "## Inference env vars\n",
    "os.environ[\"NVIDIA_BASE_URL\"] = NIM_URL\n",
    "\n",
    "# Data Store env vars\n",
    "os.environ[\"NVIDIA_DATASETS_URL\"] = ENTITY_STORE_URL\n",
    "\n",
    "## Customizer env vars\n",
    "os.environ[\"NVIDIA_CUSTOMIZER_URL\"] = CUSTOMIZER_URL\n",
    "os.environ[\"NVIDIA_OUTPUT_MODEL_DIR\"] = CUSTOMIZED_MODEL_DIR\n",
    "\n",
    "# Evaluator env vars\n",
    "os.environ[\"NVIDIA_EVALUATOR_URL\"] = EVALUATOR_URL\n",
    "\n",
    "# Guardrails env vars\n",
    "os.environ[\"GUARDRAILS_SERVICE_URL\"] = GUARDRAILS_URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d60defd-6835-477a-9d79-490b083d47f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack.core.library_client import LlamaStackAsLibraryClient\n",
    "\n",
    "client = LlamaStackAsLibraryClient(\"nvidia\")\n",
    "client.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb75ce-6ccd-4305-bf13-583620f262d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack.apis.common.job_types import JobStatus\n",
    "from llama_stack.core.datatypes import Api\n",
    "\n",
    "async def wait_eval_job(benchmark_id: str, job_id: str, polling_interval: int = 10, timeout: int = 6000):\n",
    "    start_time = time()\n",
    "    \n",
    "    # Access eval through impls\n",
    "    eval_impl = client.async_client.impls[Api.eval]\n",
    "    \n",
    "    job_status = await eval_impl.job_status(benchmark_id=benchmark_id, job_id=job_id)\n",
    "\n",
    "    print(f\"Waiting for Evaluation job {job_id} to finish.\")\n",
    "    print(f\"Job status: {job_status.status} after {time() - start_time} seconds.\")\n",
    "\n",
    "    while job_status.status in [JobStatus.scheduled, JobStatus.in_progress]:\n",
    "        await asyncio.sleep(polling_interval)\n",
    "        job_status = await eval_impl.job_status(benchmark_id=benchmark_id, job_id=job_id)\n",
    "\n",
    "        print(f\"Job status: {job_status.status} after {time() - start_time} seconds.\")\n",
    "\n",
    "        if time() - start_time > timeout:\n",
    "            raise RuntimeError(f\"Evaluation Job {job_id} took more than {timeout} seconds.\")\n",
    "\n",
    "    return job_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d6200-197c-4269-a00d-2c842020b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data Store endpoint: {DATA_STORE_URL}\")\n",
    "print(f\"Entity Store endpoint: {ENTITY_STORE_URL}\")\n",
    "print(f\"Customizer endpoint: {CUSTOMIZER_URL}\")\n",
    "print(f\"Evaluator endpoint: {EVALUATOR_URL}\")\n",
    "print(f\"NIM endpoint: {NIM_URL}\")\n",
    "print(f\"Namespace: {NMS_NAMESPACE}\")\n",
    "print(f\"Base Model: {BASE_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e99a72c-3e01-4d69-bf93-2dd2f92405ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOMIZED_MODEL = \"nvidia-tool-calling-tutorial/test-llama-stack@v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0619ee-18ea-4203-b67d-4d856ee50d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = client.models.list()\n",
    "model_ids = [model.identifier for model in models]\n",
    "\n",
    "assert f\"nvidia/{CUSTOMIZED_MODEL}\" in model_ids, \\\n",
    "    f\"Model {CUSTOMIZED_MODEL} not registered\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673aab19-c14d-4411-a6a1-b4e5835b924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(f\"{NIM_URL}/v1/models\")\n",
    "\n",
    "models = resp.json().get(\"data\", [])\n",
    "model_names = [model[\"id\"] for model in models]\n",
    "\n",
    "assert CUSTOMIZED_MODEL in model_names, \\\n",
    "    f\"Model {CUSTOMIZED_MODEL} not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8741fc-7378-466c-8c20-d3592487b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names, model_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74cb326-17ed-4b85-bb28-677a90863b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = f\"{NMS_NAMESPACE}/{DATASET_NAME}\"\n",
    "print(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b4ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url=f\"{ENTITY_STORE_URL}/v1/datasets/{repo_id}\")\n",
    "assert response.status_code in (200, 201), \\\n",
    "    f\"Dataset {repo_id} not found in Entity Store (status {response.status_code}): {response.text}\"\n",
    "\n",
    "dataset_info = response.json()\n",
    "print(f\"✓ Dataset '{DATASET_NAME}' exists in Entity Store\")\n",
    "print(f\"  Full ID: {repo_id}\")\n",
    "print(f\"  Files URL: {dataset_info['files_url']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e85fd85-c91a-4a47-81a9-072a1c846def",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = client.datasets.list()\n",
    "dataset_ids = [dataset.identifier for dataset in datasets]\n",
    "assert DATASET_NAME in dataset_ids, \\\n",
    "    f\"Dataset {DATASET_NAME} not registered\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4816ae-336b-4871-b6c8-a732450f10e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219566f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"✓ Using dataset: {repo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2087354-f185-4b1d-8bcf-a199f4659621",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url=f\"{ENTITY_STORE_URL}/v1/datasets/{repo_id}\")\n",
    "assert response.status_code in (200, 201), f\"Status Code {response.status_code} Failed to fetch dataset {response.text}\"\n",
    "\n",
    "print(\"Files URL:\", response.json()[\"files_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8a183-d692-47d7-95c5-9413aa59c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_id = \"simple-tool-calling-1\"\n",
    "simple_tool_calling_eval_config = {\n",
    "    \"type\": \"custom\",\n",
    "    \"tasks\": {\n",
    "        \"custom-tool-calling\": {\n",
    "            \"type\": \"chat-completion\",\n",
    "            \"dataset\": {\n",
    "                \"files_url\": f\"hf://datasets/{NMS_NAMESPACE}/{DATASET_NAME}/testing/xlam-test-single.jsonl\",\n",
    "                \"limit\": 50\n",
    "            },\n",
    "            \"params\": {\n",
    "                \"template\": {\n",
    "                    \"messages\": \"{{ item.messages | tojson}}\",\n",
    "                    \"tools\": \"{{ item.tools | tojson }}\",\n",
    "                    \"tool_choice\": \"auto\"\n",
    "                }\n",
    "            },\n",
    "            \"metrics\": {\n",
    "                \"tool-calling-accuracy\": {\n",
    "                    \"type\": \"tool-calling\",\n",
    "                    \"params\": {\"tool_calls_ground_truth\": \"{{ item.tool_calls | tojson }}\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813d994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = client.benchmarks.register(\n",
    "        benchmark_id=benchmark_id,\n",
    "        dataset_id=repo_id,\n",
    "        scoring_functions=[],\n",
    "        metadata=simple_tool_calling_eval_config\n",
    "    )\n",
    "    print(f\"✓ Registered benchmark '{benchmark_id}'\")\n",
    "except Exception as e:\n",
    "    if \"409\" in str(e) or \"Conflict\" in str(e) or \"already exists\" in str(e):\n",
    "        print(f\"✓ Benchmark '{benchmark_id}' already registered\")\n",
    "    else:\n",
    "        print(f\"Error registering benchmark: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784ba7f9-3009-4653-99ee-ecb88578aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.benchmarks.register(\n",
    "    benchmark_id=benchmark_id,\n",
    "    dataset_id=repo_id,\n",
    "    scoring_functions=[],\n",
    "    metadata=simple_tool_calling_eval_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59823075-26f2-443f-8843-93cb09df75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Try registering with colon instead of slash (meta:llama-3.2-1b-instruct)\n",
    "# Or see if we can create an alias\n",
    "model_payload = {\n",
    "    \"namespace\": \"meta\",  # Use meta as the namespace\n",
    "    \"name\": \"llama-3.2-1b-instruct\",\n",
    "    \"description\": \"Base Llama 3.2 1B Instruct model\",\n",
    "    \"type\": \"llm\",\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(\n",
    "        f\"{ENTITY_STORE_URL}/v1/models\",\n",
    "        json=model_payload\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    print(\"✓ Registered model in 'meta' namespace!\")\n",
    "    print(json.dumps(response.json(), indent=2))\n",
    "except requests.HTTPError as e:\n",
    "    print(f\"Status: {e.response.status_code}\")\n",
    "    print(f\"Response: {e.response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8115509-a179-4046-8179-b1892599938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack.core.datatypes import Api\n",
    "from llama_stack.apis.eval import BenchmarkConfig, EvalCandidate\n",
    "\n",
    "# Access eval through impls\n",
    "eval_impl = client.async_client.impls[Api.eval]\n",
    "\n",
    "# Create the benchmark config using proper data types\n",
    "from llama_stack.apis.eval import ModelCandidate, SamplingParams\n",
    "\n",
    "benchmark_config = BenchmarkConfig(\n",
    "    eval_candidate=ModelCandidate(\n",
    "        type=\"model\",\n",
    "        model=BASE_MODEL,\n",
    "        sampling_params=SamplingParams()\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create evaluation job\n",
    "response = await eval_impl.run_eval(\n",
    "    benchmark_id=benchmark_id,\n",
    "    benchmark_config=benchmark_config\n",
    ")\n",
    "\n",
    "job_id = response.job_id\n",
    "print(f\"Created evaluation job: {job_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136cfa9c-2693-4c42-967e-3399aa087c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = await wait_eval_job(benchmark_id=benchmark_id, job_id=job_id, polling_interval=5, timeout=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25e6950-ac12-4fcd-97a6-00558bc38fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Get the full job details to see the error\n",
    "response = requests.get(f\"{EVALUATOR_URL}/v1/evaluation/jobs/{job_id}\")\n",
    "response.raise_for_status()\n",
    "job_details = response.json()\n",
    "\n",
    "print(\"Job status:\", job_details.get(\"status\"))\n",
    "print(\"\\nStatus details:\")\n",
    "if \"status_details\" in job_details:\n",
    "    print(json.dumps(job_details[\"status_details\"], indent=2))\n",
    "    \n",
    "# print(\"\\nFull job details:\")\n",
    "# print(json.dumps(job_details, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcc1d3b-cfcb-4fa4-b6f2-6b42f4e50653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack.core.datatypes import Api\n",
    "\n",
    "# Access eval through impls\n",
    "eval_impl = client.async_client.impls[Api.eval]\n",
    "\n",
    "# Retrieve job results\n",
    "job_results = await eval_impl.job_result(benchmark_id=benchmark_id, job_id=job_id)\n",
    "print(f\"Job results: {json.dumps(job_results.model_dump(), indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2834a430-b868-46b4-b0d4-c0c75d22d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results = job_results.scores[benchmark_id].aggregated_results\n",
    "base_function_name_accuracy_score = aggregated_results[\"tasks\"][\"custom-tool-calling\"][\"metrics\"][\"tool-calling-accuracy\"][\"scores\"][\"function_name_accuracy\"][\"value\"]\n",
    "base_function_name_and_args_accuracy = aggregated_results[\"tasks\"][\"custom-tool-calling\"][\"metrics\"][\"tool-calling-accuracy\"][\"scores\"][\"function_name_and_args_accuracy\"][\"value\"]\n",
    "\n",
    "print(f\"Base model: function_name_accuracy: {base_function_name_accuracy_score}\")\n",
    "print(f\"Base model: function_name_and_args_accuracy: {base_function_name_and_args_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7607ad7-b97d-421a-a0ef-c5a3a4e34898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack.core.datatypes import Api\n",
    "from llama_stack.apis.eval import BenchmarkConfig, EvalCandidate\n",
    "\n",
    "# Access eval through impls\n",
    "eval_impl = client.async_client.impls[Api.eval]\n",
    "\n",
    "# Create the benchmark config using proper data types\n",
    "from llama_stack.apis.eval import ModelCandidate, SamplingParams\n",
    "\n",
    "benchmark_config = BenchmarkConfig(\n",
    "    eval_candidate=ModelCandidate(\n",
    "        type=\"model\",\n",
    "        model=CUSTOMIZED_MODEL,\n",
    "        sampling_params=SamplingParams()\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create evaluation job\n",
    "response = await eval_impl.run_eval(\n",
    "    benchmark_id=benchmark_id,\n",
    "    benchmark_config=benchmark_config\n",
    ")\n",
    "\n",
    "job_id = response.job_id\n",
    "print(f\"Created evaluation job: {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19435e23-ed8e-4ff5-bcb2-8c68ee82972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = await wait_eval_job(benchmark_id=benchmark_id, job_id=job_id, polling_interval=5, timeout=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15416460-f616-463e-8031-7b3b29d1d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack.core.datatypes import Api\n",
    "\n",
    "# Access eval through impls\n",
    "eval_impl = client.async_client.impls[Api.eval]\n",
    "\n",
    "# Retrieve job results\n",
    "job_results = await eval_impl.job_result(benchmark_id=benchmark_id, job_id=job_id)\n",
    "print(f\"Job results: {json.dumps(job_results.model_dump(), indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968db57e-01e7-450f-b6e2-812310eb4bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results_custom = job_results.scores[benchmark_id].aggregated_results\n",
    "custom_function_name_accuracy_score = aggregated_results_custom[\"tasks\"][\"custom-tool-calling\"][\"metrics\"][\"tool-calling-accuracy\"][\"scores\"][\"function_name_accuracy\"][\"value\"]\n",
    "custom_function_name_and_args_accuracy = aggregated_results_custom[\"tasks\"][\"custom-tool-calling\"][\"metrics\"][\"tool-calling-accuracy\"][\"scores\"][\"function_name_and_args_accuracy\"][\"value\"]\n",
    "\n",
    "print(f\"Custom model: function_name_accuracy: {custom_function_name_accuracy_score}\")\n",
    "print(f\"Custom model: function_name_and_args_accuracy: {custom_function_name_and_args_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac36b2e-bdf9-455d-a532-dc275cf6f7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
