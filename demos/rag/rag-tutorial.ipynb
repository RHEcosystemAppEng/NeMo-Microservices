{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG (Retrieval-Augmented Generation) Tutorial\n",
        "\n",
        "This notebook demonstrates how to build a RAG pipeline using NVIDIA NeMo Microservices on OpenShift.\n",
        "\n",
        "Full documentation: [NeMo Data Store](https://docs.nvidia.com/nemo/microservices/latest/datastore/overview.html), [NeMo Entity Store](https://docs.nvidia.com/nemo/microservices/latest/entity-store/overview.html)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This example implements a complete RAG workflow:\n",
        "1. **Document Ingestion**: Upload documents to NeMo Data Store\n",
        "2. **Embedding Generation**: Create embeddings using NeMo Embedding NIM\n",
        "3. **Vector Storage**: Store embeddings in NeMo Entity Store\n",
        "4. **Query Processing**: Retrieve relevant documents based on user queries\n",
        "5. **Response Generation**: Generate answers using NeMo Chat NIM with retrieved context\n",
        "6. **Optional Guardrails**: Apply safety guardrails to responses\n",
        "\n",
        "**No API keys required!** The notebook uses your deployed NIM endpoints for both chat and embedding models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "### Deployed Services\n",
        "- NeMo Data Store service deployed\n",
        "- NeMo Entity Store service deployed\n",
        "- NeMo Guardrails service deployed (optional but recommended)\n",
        "- **Chat NIM**: `meta/llama-3.2-1b-instruct` model (service name may vary, e.g., `meta-llama3-1b-instruct`)\n",
        "- **Embedding NIM**: `nv-embedqa-1b-v2` service\n",
        "\n",
        "### üîí Security Setup (REQUIRED FIRST STEP)\n",
        "\n",
        "**IMPORTANT**: This notebook uses `env.donotcommit` file for sensitive configuration (tokens, API keys). \n",
        "\n",
        "**Before running this notebook:**\n",
        "1. Copy the template: `cp env.donotcommit.example env.donotcommit`\n",
        "2. Edit `env.donotcommit` and add your `NMS_NAMESPACE` (and other values as needed)\n",
        "3. The `env.donotcommit` file is git-ignored and will NOT be committed to version control\n",
        "\n",
        "**Find your namespace:**\n",
        "```bash\n",
        "oc projects\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION: Load Environment Variables from env.donotcommit file\n",
        "# ============================================================================\n",
        "# üîí SECURITY: Never hardcode secrets in notebooks!\n",
        "# All sensitive values (tokens, API keys) should be in env.donotcommit file\n",
        "# \n",
        "# SETUP INSTRUCTIONS:\n",
        "# 1. Copy env.donotcommit.example to env.donotcommit: cp env.donotcommit.example env.donotcommit\n",
        "# 2. Edit env.donotcommit and fill in your values (especially NMS_NAMESPACE)\n",
        "# 3. env.donotcommit is git-ignored and will NOT be committed to version control\n",
        "#\n",
        "# IMPORTANT: Run this cell FIRST before importing config!\n",
        "# If you get connection errors, restart the kernel and run cells in order.\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Load env.donotcommit file from the notebook directory\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    # Find env.donotcommit file in the same directory as this notebook\n",
        "    notebook_dir = Path().resolve()  # Current working directory (where notebook is run from)\n",
        "    env_file = notebook_dir / \"env.donotcommit\"\n",
        "    \n",
        "    if env_file.exists():\n",
        "        load_dotenv(env_file, override=False)  # override=False: don't overwrite existing env vars\n",
        "        print(f\"‚úÖ Loaded env.donotcommit file from: {env_file}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  env.donotcommit file not found at: {env_file}\")\n",
        "        print(f\"   Looking for env.donotcommit.example template...\")\n",
        "        # Check if env.donotcommit.example exists\n",
        "        env_example = notebook_dir / \"env.donotcommit.example\"\n",
        "        if env_example.exists():\n",
        "            print(f\"   ‚ÑπÔ∏è  env.donotcommit.example exists at: {env_example}\")\n",
        "            print(f\"   üìù Please copy it to env.donotcommit and fill in your values:\")\n",
        "            print(f\"      cp env.donotcommit.example env.donotcommit\")\n",
        "            print(f\"      # Then edit env.donotcommit and add your NMS_NAMESPACE\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  env.donotcommit.example not found - creating template...\")\n",
        "            env_example_content = \"\"\"# NeMo Microservices Configuration\n",
        "# Copy this file to env.donotcommit and fill in your values\n",
        "# env.donotcommit is git-ignored and will NOT be committed\n",
        "\n",
        "# REQUIRED: Namespace for cluster services\n",
        "# Replace with your actual OpenShift namespace/project name\n",
        "# Find your namespace: oc projects\n",
        "NMS_NAMESPACE=your-namespace\n",
        "\n",
        "# REQUIRED: Set to \"false\" when running in Workbench (uses cluster URLs)\n",
        "# Set to \"true\" only if you're running locally with port-forwards\n",
        "RUN_LOCALLY=false\n",
        "\n",
        "# OPTIONAL: NeMo Data Store token\n",
        "# Default is \"token\" - update if your deployment uses a different token\n",
        "NDS_TOKEN=token\n",
        "\n",
        "# OPTIONAL: Dataset name for RAG tutorial documents\n",
        "DATASET_NAME=rag-tutorial-documents\n",
        "\n",
        "# OPTIONAL: RAG Configuration\n",
        "# Number of documents to retrieve\n",
        "RAG_TOP_K=5\n",
        "# Similarity threshold for retrieval\n",
        "RAG_SIMILARITY_THRESHOLD=0.3\n",
        "\n",
        "# OPTIONAL: API Keys (only needed if using external APIs as fallback)\n",
        "# OPENAI_API_KEY=\n",
        "# NVIDIA_API_KEY=\n",
        "# HF_TOKEN=\n",
        "\"\"\"\n",
        "            env_example.write_text(env_example_content)\n",
        "            print(f\"   ‚úÖ Created env.donotcommit.example template at: {env_example}\")\n",
        "            print(f\"   üìù Please copy it to env.donotcommit and fill in your values:\")\n",
        "            print(f\"      cp env.donotcommit.example env.donotcommit\")\n",
        "            print(f\"      # Then edit env.donotcommit and add your NMS_NAMESPACE\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  python-dotenv not installed - install with: pip install python-dotenv\")\n",
        "    print(\"   Will use system environment variables only (not recommended)\")\n",
        "\n",
        "# Clear any cached config module to force reload\n",
        "if 'config' in sys.modules:\n",
        "    del sys.modules['config']\n",
        "    print(\"‚ö†Ô∏è  Cleared cached config module - will reload with new env vars\")\n",
        "\n",
        "# Set defaults (will be overridden by env.donotcommit file if present)\n",
        "# These are fallback values - prefer setting them in env.donotcommit file\n",
        "os.environ.setdefault(\"NMS_NAMESPACE\", \"anemo-rhoai\")\n",
        "os.environ.setdefault(\"RUN_LOCALLY\", \"false\")\n",
        "os.environ.setdefault(\"NDS_TOKEN\", \"token\")\n",
        "os.environ.setdefault(\"DATASET_NAME\", \"rag-tutorial-documents\")\n",
        "os.environ.setdefault(\"RAG_TOP_K\", \"5\")\n",
        "os.environ.setdefault(\"RAG_SIMILARITY_THRESHOLD\", \"0.3\")\n",
        "# NIM_SERVICE_ACCOUNT_TOKEN should come from env.donotcommit file, not hardcoded here\n",
        "\n",
        "print(\"\\n‚úÖ Environment variables loaded\")\n",
        "print(f\"   NMS_NAMESPACE: {os.environ.get('NMS_NAMESPACE')}\")\n",
        "print(f\"   RUN_LOCALLY: {os.environ.get('RUN_LOCALLY')} (cluster mode for Workbench)\")\n",
        "print(f\"   DATASET_NAME: {os.environ.get('DATASET_NAME')}\")\n",
        "print(f\"\\nüí° If you see connection errors, restart the kernel and run cells in order!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install llama-stack-client from GitHub main (same as llamastack demo)\n",
        "# This ensures compatibility with the latest server version\n",
        "%pip install --upgrade git+https://github.com/meta-llama/llama-stack-client-python.git@main\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "# Note: langchain is not needed - we use LlamaStack client for chat completions and direct HTTP requests for embeddings\n",
        "%pip install requests jupyterlab python-dotenv numpy pandas llama-stack-client\n",
        "\n",
        "# If running locally (outside cluster), set RUN_LOCALLY before importing config\n",
        "# Uncomment the line below if you're running this notebook locally:\n",
        "# import os; os.environ[\"RUN_LOCALLY\"] = \"true\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "from config import (\n",
        "    NDS_URL, ENTITY_STORE_URL, GUARDRAILS_URL,\n",
        "    NIM_CHAT_URL, NIM_EMBEDDING_URL,\n",
        "    NIM_CHAT_URL_CLUSTER, NIM_EMBEDDING_URL_CLUSTER,\n",
        "    NMS_NAMESPACE, DATASET_NAME, NDS_TOKEN,\n",
        "    RAG_TOP_K, RAG_SIMILARITY_THRESHOLD,\n",
        "    RUN_LOCALLY, LLAMASTACK_URL, NIM_SERVICE_ACCOUNT_TOKEN\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Configuration loaded\")\n",
        "print(f\"Mode: {'Local (port-forward)' if RUN_LOCALLY else 'Cluster'}\")\n",
        "print(f\"Data Store: {NDS_URL}\")\n",
        "print(f\"Entity Store: {ENTITY_STORE_URL}\")\n",
        "print(f\"Chat NIM: {NIM_CHAT_URL}\")\n",
        "print(f\"Embedding NIM: {NIM_EMBEDDING_URL}\")\n",
        "print(f\"LlamaStack: {LLAMASTACK_URL}\")\n",
        "print(f\"Namespace: {NMS_NAMESPACE}\")\n",
        "print(f\"Dataset: {DATASET_NAME}\")\n",
        "\n",
        "# Quick connectivity test\n",
        "import requests\n",
        "try:\n",
        "    r = requests.get(f\"{NDS_URL}/v1/datastore/namespaces\", timeout=2)\n",
        "    print(f\"‚úÖ Data Store connectivity: OK\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Data Store connectivity: FAILED - {e}\")\n",
        "    if RUN_LOCALLY:\n",
        "        print(f\"\\nüì° Port-forward setup required for local mode:\")\n",
        "        print(f\"   Run this in a terminal:\")\n",
        "        print(f\"   ./port-forward.sh\")\n",
        "        print(f\"\\n   Or manually:\")\n",
        "        print(f\"   oc port-forward -n {NMS_NAMESPACE} svc/nemodatastore-sample 8001:8000 &\")\n",
        "        print(f\"   oc port-forward -n {NMS_NAMESPACE} svc/nemoentitystore-sample 8002:8000 &\")\n",
        "        print(f\"   oc port-forward -n {NMS_NAMESPACE} svc/nemoguardrails-sample 8005:8000 &\")\n",
        "        print(f\"   oc port-forward -n {NMS_NAMESPACE} svc/meta-llama3-1b-instruct 8006:8000 &\")\n",
        "        print(f\"   oc port-forward -n {NMS_NAMESPACE} svc/nv-embedqa-1b-v2 8007:8000 &\")\n",
        "        print(f\"   oc port-forward -n {NMS_NAMESPACE} svc/llamastack 8321:8321 &\")\n",
        "    else:\n",
        "        print(f\"   If running from outside cluster, set RUN_LOCALLY=true environment variable\")\n",
        "        print(f\"   Or ensure you're running this notebook from within the cluster\")\n",
        "\n",
        "# Initialize LlamaStack client\n",
        "try:\n",
        "    from llama_stack_client import LlamaStackClient\n",
        "    import logging\n",
        "    \n",
        "    # Suppress httpx INFO logs (500 errors and connection attempts are expected during fallback)\n",
        "    logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
        "    \n",
        "    client = LlamaStackClient(base_url=LLAMASTACK_URL)\n",
        "    # Test connectivity\n",
        "    # Note: 404 on root endpoint is expected - it just means the service is reachable\n",
        "    try:\n",
        "        server_info = client._client.get(\"/\")\n",
        "        print(f\"‚úÖ LlamaStack connectivity: OK\")\n",
        "        try:\n",
        "            client_version = client._client._version\n",
        "            print(f\"   LlamaStack client version: {client_version}\")\n",
        "        except:\n",
        "            pass\n",
        "    except Exception as e:\n",
        "        # 404 is OK - it means service is reachable but root endpoint doesn't exist\n",
        "        if \"404\" in str(e) or \"Not Found\" in str(e):\n",
        "            print(f\"‚úÖ LlamaStack connectivity: OK (service reachable)\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  LlamaStack connectivity: FAILED - {e}\")\n",
        "            if RUN_LOCALLY:\n",
        "                print(f\"   Make sure port-forward is active: oc port-forward -n {NMS_NAMESPACE} svc/llamastack 8321:8321\")\n",
        "            else:\n",
        "                print(f\"   Make sure LlamaStack is deployed: oc get pods -n {NMS_NAMESPACE} | grep llamastack\")\n",
        "            client = None\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  LlamaStack client not available - install with: %pip install --upgrade git+https://github.com/meta-llama/llama-stack-client-python.git@main\")\n",
        "    print(\"   Continuing without LlamaStack integration...\")\n",
        "    client = None\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  LlamaStack initialization failed: {e}\")\n",
        "    print(\"   Continuing without LlamaStack integration...\")\n",
        "    client = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Document Ingestion\n",
        "\n",
        "First, we'll upload sample documents to NeMo Data Store. These documents will be used for retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample documents for RAG tutorial\n",
        "documents = [\n",
        "    {\n",
        "        \"id\": \"doc1\",\n",
        "        \"title\": \"Introduction to NeMo Microservices\",\n",
        "        \"content\": \"NVIDIA NeMo Microservices is a platform for deploying AI models at scale. It provides infrastructure for training, inference, and evaluation of large language models. The platform includes components like Data Store, Entity Store, Customizer, Evaluator, and Guardrails.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc2\",\n",
        "        \"title\": \"RAG Architecture\",\n",
        "        \"content\": \"Retrieval-Augmented Generation (RAG) combines information retrieval with language generation. The process involves: 1) Storing documents in a vector database, 2) Embedding user queries, 3) Retrieving relevant documents, 4) Generating responses using retrieved context.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc3\",\n",
        "        \"title\": \"OpenShift Deployment\",\n",
        "        \"content\": \"NeMo Microservices can be deployed on OpenShift using Helm charts. The deployment includes infrastructure components (PostgreSQL, MLflow, Argo Workflows) and instance components (NeMo services, NIM services). All components are namespace-scoped for multi-tenant safety.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc4\",\n",
        "        \"title\": \"NIM Services\",\n",
        "        \"content\": \"NVIDIA Inference Microservices (NIM) provide optimized inference for AI models. NIM services support chat models, embedding models, and reranking models. They are containerized and can be deployed on Kubernetes/OpenShift clusters with GPU support.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc5\",\n",
        "        \"title\": \"Vector Databases\",\n",
        "        \"content\": \"Vector databases store embeddings for similarity search. NeMo Entity Store provides vector storage capabilities. Milvus is also available as an alternative vector database. Both support efficient similarity search for RAG applications.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"‚úÖ Prepared {len(documents)} sample documents\")\n",
        "for doc in documents:\n",
        "    print(f\"  - {doc['title']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload documents to NeMo Data Store\n",
        "import json\n",
        "\n",
        "# Create namespace if it doesn't exist\n",
        "namespace_url = f\"{NDS_URL}/v1/datastore/namespaces/{NMS_NAMESPACE}\"\n",
        "try:\n",
        "    response = requests.get(namespace_url, headers={\"Authorization\": f\"Bearer {NDS_TOKEN}\"})\n",
        "    if response.status_code == 404:\n",
        "        # Create namespace\n",
        "        response = requests.post(\n",
        "            f\"{NDS_URL}/v1/datastore/namespaces\",\n",
        "            json={\"name\": NMS_NAMESPACE},\n",
        "            headers={\"Authorization\": f\"Bearer {NDS_TOKEN}\"}\n",
        "        )\n",
        "        print(f\"‚úÖ Created namespace: {NMS_NAMESPACE}\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Namespace exists: {NMS_NAMESPACE}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Error checking namespace: {e}\")\n",
        "\n",
        "# Upload documents\n",
        "uploaded_docs = []\n",
        "for doc in documents:\n",
        "    try:\n",
        "        # Create dataset entry\n",
        "        file_url = f\"hf://datasets/{NMS_NAMESPACE}/{DATASET_NAME}/{doc['id']}.json\"\n",
        "        doc_data = {\n",
        "            \"id\": doc['id'],\n",
        "            \"title\": doc['title'],\n",
        "            \"content\": doc['content']\n",
        "        }\n",
        "        \n",
        "        # In a real scenario, you would upload to Data Store\n",
        "        # For this tutorial, we'll store locally and use for embedding\n",
        "        uploaded_docs.append(doc_data)\n",
        "        print(f\"‚úÖ Prepared document: {doc['title']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error uploading {doc['id']}: {e}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Prepared {len(uploaded_docs)} documents for embedding\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Generate Embeddings\n",
        "\n",
        "Now we'll generate embeddings for each document using the NeMo Embedding NIM service.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate embeddings using NeMo Embedding NIM\n",
        "# Note: We use direct NIM calls for embeddings as LlamaStack may not expose embeddings API directly\n",
        "# Future enhancement: If LlamaStack adds embeddings API support, we can use client.embeddings.create()\n",
        "def get_embedding(text, embedding_url, input_type=\"passage\"):\n",
        "    \"\"\"Generate embedding for text using NeMo Embedding NIM\"\"\"\n",
        "    try:\n",
        "        headers = {\"Content-Type\": \"application/json\"}\n",
        "        # Add Authorization header if token is provided\n",
        "        if NIM_SERVICE_ACCOUNT_TOKEN:\n",
        "            headers[\"Authorization\"] = f\"Bearer {NIM_SERVICE_ACCOUNT_TOKEN}\"\n",
        "        \n",
        "        response = requests.post(\n",
        "            f\"{embedding_url}/v1/embeddings\",\n",
        "            json={\n",
        "                \"input\": text,\n",
        "                \"model\": \"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
        "                \"input_type\": input_type\n",
        "            },\n",
        "            headers=headers,\n",
        "            timeout=30\n",
        "        )\n",
        "        if response.status_code == 200:\n",
        "            return response.json()[\"data\"][0][\"embedding\"]\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  Error getting embedding: {response.status_code} - {response.text}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Exception getting embedding: {e}\")\n",
        "        return None\n",
        "\n",
        "# Generate embeddings for all documents\n",
        "print(\"Generating embeddings...\")\n",
        "documents_with_embeddings = []\n",
        "\n",
        "for doc in uploaded_docs:\n",
        "    # Combine title and content for embedding\n",
        "    text_to_embed = f\"{doc['title']}\\n{doc['content']}\"\n",
        "    embedding = get_embedding(text_to_embed, NIM_EMBEDDING_URL)\n",
        "    \n",
        "    if embedding:\n",
        "        doc['embedding'] = embedding\n",
        "        documents_with_embeddings.append(doc)\n",
        "        print(f\"‚úÖ Generated embedding for: {doc['title']}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Failed to generate embedding for: {doc['title']}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Generated embeddings for {len(documents_with_embeddings)} documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Store Embeddings Locally\n",
        "\n",
        "For this tutorial, we'll store embeddings in memory for local similarity search.\n",
        "In production, you can use a vector database like Milvus or Pinecone.\n",
        "\n",
        "**Note**: NeMo Entity Store is primarily designed for managing models, datasets, and namespaces,\n",
        "not for storing arbitrary document embeddings. For production RAG, consider using a dedicated vector database.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embeddings are already stored in documents_with_embeddings list\n",
        "# For this tutorial, we use in-memory storage for simplicity\n",
        "print(f\"‚úÖ Stored {len(documents_with_embeddings)} documents with embeddings in memory\")\n",
        "print(f\"   Documents ready for local similarity search\")\n",
        "print(f\"   In production, use a vector database like Milvus or Pinecone\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Query and Retrieve\n",
        "\n",
        "Now we'll process a user query: embed it, find similar documents, and retrieve the most relevant ones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# User query\n",
        "user_query = \"What is RAG and how does it work?\"\n",
        "\n",
        "print(f\"User Query: {user_query}\\n\")\n",
        "\n",
        "# Generate embedding for the query\n",
        "query_embedding = get_embedding(user_query, NIM_EMBEDDING_URL, input_type=\"query\")\n",
        "\n",
        "if query_embedding:\n",
        "    print(f\"‚úÖ Generated query embedding (dimension: {len(query_embedding)})\\n\")\n",
        "    \n",
        "    # Use local similarity search\n",
        "    import numpy as np\n",
        "    retrieved_docs = []\n",
        "    similarities = []\n",
        "    \n",
        "    for doc in documents_with_embeddings:\n",
        "        similarity = np.dot(query_embedding, doc['embedding']) / (\n",
        "            np.linalg.norm(query_embedding) * np.linalg.norm(doc['embedding'])\n",
        "        )\n",
        "        similarities.append((similarity, doc))\n",
        "    \n",
        "    # Sort by similarity and get top_k\n",
        "    similarities.sort(reverse=True, key=lambda x: x[0])\n",
        "    \n",
        "    print(f\"‚úÖ Found {len(similarities)} documents, showing top {RAG_TOP_K}:\\n\")\n",
        "    \n",
        "    for i, (similarity, doc) in enumerate(similarities[:RAG_TOP_K], 1):\n",
        "        if similarity >= RAG_SIMILARITY_THRESHOLD:\n",
        "            print(f\"{i}. {doc['title']} (similarity: {similarity:.3f})\")\n",
        "            retrieved_docs.append({\n",
        "                'title': doc['title'],\n",
        "                'content': doc['content'],\n",
        "                'id': doc['id']\n",
        "            })\n",
        "    \n",
        "    print(f\"\\n‚úÖ Retrieved {len(retrieved_docs)} documents above threshold ({RAG_SIMILARITY_THRESHOLD})\\n\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Failed to generate query embedding\")\n",
        "    retrieved_docs = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Generate Response\n",
        "\n",
        "Now we'll use the retrieved documents as context to generate a response using the Chat NIM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build context from retrieved documents\n",
        "context = \"\\n\\n\".join([\n",
        "    f\"Document: {doc['title']}\\n{doc['content']}\"\n",
        "    for doc in retrieved_docs\n",
        "])\n",
        "\n",
        "print(\"Retrieved Context:\")\n",
        "print(\"=\" * 80)\n",
        "print(context[:500] + \"...\" if len(context) > 500 else context)\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Generate response using LlamaStack client only (no fallback)\n",
        "def generate_response(query, context):\n",
        "    \"\"\"Generate response using LlamaStack client with retrieved context\"\"\"\n",
        "    # Validate LlamaStack client is available\n",
        "    if client is None:\n",
        "        raise ValueError(\n",
        "            \"LlamaStack client not available. \"\n",
        "            \"Check LlamaStack deployment and connectivity.\"\n",
        "        )\n",
        "    \n",
        "    # Build prompt with context\n",
        "    system_prompt = \"You are a helpful assistant. Answer the question based on the provided context. If the context doesn't contain enough information, say so.\"\n",
        "    user_prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "    \n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            model=\"nvidia/meta/llama-3.2-1b-instruct\",\n",
        "            temperature=0.7,\n",
        "            max_tokens=500\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        print(f\"‚ùå LlamaStack error: {error_msg}\")\n",
        "        print(\"\\nüîç Troubleshooting steps:\")\n",
        "        print(f\"1. Check LlamaStack pod is running:\")\n",
        "        print(f\"   oc get pods -n {NMS_NAMESPACE} | grep llamastack\")\n",
        "        print(f\"2. Check LlamaStack pod status:\")\n",
        "        print(f\"   oc get pods -n {NMS_NAMESPACE} -l app=nemo-llamastack\")\n",
        "        print(f\"3. Check LlamaStack logs for errors:\")\n",
        "        print(f\"   oc logs -n {NMS_NAMESPACE} deployment/llamastack -c llamastack-ctr --tail=50\")\n",
        "        print(f\"4. Verify token secret exists and is populated:\")\n",
        "        print(f\"   oc get secret -n {NMS_NAMESPACE} | grep token-secret\")\n",
        "        print(f\"   oc get secret <sa-name>-token-secret -n {NMS_NAMESPACE} -o jsonpath='{{.data.token}}' | base64 -d | head -c 20\")\n",
        "        print(f\"5. Check service account exists:\")\n",
        "        print(f\"   oc get sa -n {NMS_NAMESPACE} | grep model\")\n",
        "        print(f\"6. Verify LlamaStack can reach NIM (check initContainer logs):\")\n",
        "        print(f\"   oc logs -n {NMS_NAMESPACE} <pod-name> -c wait-for-token\")\n",
        "        \n",
        "        # Provide specific guidance based on error type\n",
        "        if \"500\" in error_msg or \"Internal server error\" in error_msg:\n",
        "            print(f\"\\nüí° This is a 500 error - LlamaStack is having internal issues.\")\n",
        "            print(f\"   Most likely causes:\")\n",
        "            print(f\"   - Token secret not populated (check step 4)\")\n",
        "            print(f\"   - LlamaStack can't authenticate to NIM\")\n",
        "            print(f\"   - Check LlamaStack logs (step 3) for detailed error\")\n",
        "        elif \"401\" in error_msg or \"Unauthorized\" in error_msg:\n",
        "            print(f\"\\nüí° This is a 401 error - Authentication failed.\")\n",
        "            print(f\"   Most likely causes:\")\n",
        "            print(f\"   - Token secret missing or empty\")\n",
        "            print(f\"   - Service account doesn't exist\")\n",
        "            print(f\"   - Check token secret (step 4)\")\n",
        "        elif \"404\" in error_msg or \"Not Found\" in error_msg:\n",
        "            print(f\"\\nüí° This is a 404 error - Endpoint not found.\")\n",
        "            print(f\"   Most likely causes:\")\n",
        "            print(f\"   - LlamaStack not fully started\")\n",
        "            print(f\"   - Wrong URL configured\")\n",
        "            print(f\"   - Check pod status (step 2)\")\n",
        "        \n",
        "        raise RuntimeError(f\"LlamaStack request failed: {error_msg}\")\n",
        "\n",
        "# Generate response\n",
        "print(\"Generating response...\")\n",
        "response_text = generate_response(user_query, context)\n",
        "\n",
        "if response_text:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Generated Response:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(response_text)\n",
        "    print(\"=\" * 80)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Failed to generate response\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Validate RAG with Test Queries\n",
        "\n",
        "Let's test the RAG pipeline with multiple questions to validate it's working correctly.\n",
        "We'll ask questions about different topics from our documents and verify the responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test queries covering different topics from our documents\n",
        "test_queries = [\n",
        "    \"What is NeMo Microservices?\",\n",
        "    \"How does RAG work?\",\n",
        "    \"What are vector databases used for?\",\n",
        "    \"What components does NeMo Microservices include?\"\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"RAG VALIDATION TESTS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Testing {len(test_queries)} queries against {len(documents_with_embeddings)} documents\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to run a complete RAG query\n",
        "def run_rag_query(query, show_context=True):\n",
        "    \"\"\"Run a complete RAG query and return the response\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Generate query embedding\n",
        "    query_embedding = get_embedding(query, NIM_EMBEDDING_URL, input_type=\"query\")\n",
        "    \n",
        "    if not query_embedding:\n",
        "        print(\"‚ö†Ô∏è  Failed to generate query embedding\")\n",
        "        return None\n",
        "    \n",
        "    # Local similarity search\n",
        "    import numpy as np\n",
        "    retrieved_docs = []\n",
        "    similarities = []\n",
        "    \n",
        "    for doc in documents_with_embeddings:\n",
        "        similarity = np.dot(query_embedding, doc['embedding']) / (\n",
        "            np.linalg.norm(query_embedding) * np.linalg.norm(doc['embedding'])\n",
        "        )\n",
        "        similarities.append((similarity, doc))\n",
        "    \n",
        "    # Sort by similarity and get top_k\n",
        "    similarities.sort(reverse=True, key=lambda x: x[0])\n",
        "    \n",
        "    print(f\"\\nüìä Top {min(RAG_TOP_K, len(similarities))} retrieved documents:\")\n",
        "    for i, (similarity, doc) in enumerate(similarities[:RAG_TOP_K], 1):\n",
        "        if similarity >= RAG_SIMILARITY_THRESHOLD:\n",
        "            print(f\"  {i}. {doc['title']} (similarity: {similarity:.3f})\")\n",
        "            retrieved_docs.append({\n",
        "                'title': doc['title'],\n",
        "                'content': doc['content'],\n",
        "                'id': doc['id']\n",
        "            })\n",
        "    \n",
        "    if not retrieved_docs:\n",
        "        print(f\"‚ö†Ô∏è  No documents found above threshold ({RAG_SIMILARITY_THRESHOLD})\")\n",
        "        return None\n",
        "    \n",
        "    # Build context\n",
        "    context = \"\\n\\n\".join([\n",
        "        f\"Document: {doc['title']}\\n{doc['content']}\"\n",
        "        for doc in retrieved_docs\n",
        "    ])\n",
        "    \n",
        "    if show_context:\n",
        "        print(f\"\\nüìÑ Retrieved Context (first 300 chars):\")\n",
        "        print(f\"{context[:300]}...\")\n",
        "    \n",
        "    # Generate response using LlamaStack client (via generate_response function)\n",
        "    print(f\"\\nü§ñ Generating response...\")\n",
        "    response_text = generate_response(query, context)\n",
        "    \n",
        "    if response_text:\n",
        "        print(f\"\\n‚úÖ Response:\")\n",
        "        print(f\"{response_text}\")\n",
        "        return response_text\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Failed to generate response\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run all test queries\n",
        "results = {}\n",
        "\n",
        "for query in test_queries:\n",
        "    response = run_rag_query(query, show_context=True)\n",
        "    results[query] = response\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"VALIDATION SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nTotal queries tested: {len(test_queries)}\")\n",
        "print(f\"Successful responses: {sum(1 for r in results.values() if r is not None)}\")\n",
        "print(f\"Failed responses: {sum(1 for r in results.values() if r is None)}\")\n",
        "\n",
        "if all(r is not None for r in results.values()):\n",
        "    print(\"\\n‚úÖ All queries returned responses! RAG pipeline is working correctly.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Some queries failed. Check the error messages above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This tutorial demonstrated a complete RAG pipeline:\n",
        "1. ‚úÖ Document ingestion into NeMo Data Store\n",
        "2. ‚úÖ Embedding generation using NeMo Embedding NIM\n",
        "3. ‚úÖ Vector storage (local in-memory for this tutorial)\n",
        "4. ‚úÖ Query processing with similarity search\n",
        "5. ‚úÖ Response generation using NeMo Chat NIM\n",
        "6. ‚úÖ Optional guardrails validation\n",
        "7. ‚úÖ RAG validation with test queries\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Add more documents to improve retrieval quality\n",
        "- Experiment with different embedding models\n",
        "- Adjust retrieval parameters (top_k, similarity threshold)\n",
        "- Integrate with your own document sources\n",
        "- Add multi-turn conversation support\n",
        "- Use a production vector database (Milvus, Pinecone, etc.) for larger document sets"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
