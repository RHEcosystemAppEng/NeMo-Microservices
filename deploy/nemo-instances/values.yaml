# NeMo Samples Helm Chart Values
# Following quickstart Step 6 (lines 152-164)
# Deploys custom resources: NemoCustomizer, NemoDatastore, NemoEntitystore,
# NemoEvaluator, NemoGuardrail, NIMCache (embedding), NIMPipeline (embedding)
# Note: Chat models are deployed via RHOAI/OpenShift AI NIM Serving Runtime Model (KServe InferenceService)

# Installation namespace
namespace:
  name: anemo-rhoai

# Component installation flags
install:
  nemoOperator: true
  nimOperator: true

# Storage configuration
storage:
  storageClass: ""  # Empty string uses default storage class
  volumeAccessMode: ReadWriteOnce

# PostgreSQL Secrets Configuration
# These passwords are used as fallbacks if infrastructure PostgreSQL secrets cannot be read.
# The Helm template will automatically read passwords from infrastructure secrets if available.
# Only override these if infrastructure secrets don't exist or you need different passwords.
postgresqlSecrets:
  customizer:
    password: "ncspassword"  # Fallback if nemo-infra-customizer-postgresql secret not found
  datastore:
    password: "ndspass"  # Fallback if nemo-infra-datastore-postgresql secret not found
  entitystore:
    password: "nespass"  # Fallback if nemo-infra-entity-store-postgresql secret not found
  evaluator:
    password: "evalpass"  # Fallback if nemo-infra-evaluator-postgresql secret not found
  guardrail:
    password: "guardrailpass"  # Fallback if nemo-infra-guardrail-postgresql secret not found

# ===== NemoCustomizer Configuration =====
# Image: nvcr.io/nvidia/nemo-microservices/customizer-api:25.08 (matches latest samples)
customizer:
  enabled: true
  name: nemocustomizer-sample
  image:
    repository: nvcr.io/nvidia/nemo-microservices/customizer-api
    tag: "25.08"
    pullPolicy: IfNotPresent
  trainingImage:
    repository: nvcr.io/nvidia/nemo-microservices/customizer
    tag: "25.08"
  datastoreToolsImage:
    repository: nvcr.io/nvidia/nemo-microservices/nds-v2-huggingface-cli
    tag: "25.08"
  scheduler:
    type: "volcano"
  wandb:
    # Note: CRD requires wandb section, so it's always included in the spec
    # The secret is always created (with empty values by default)
    # WandB functionality is optional - application should handle empty values gracefully
    secretName: wandb-secret
    apiKeyKey: apiKey
    encryptionKey: encryptionKey
    # Values for wandb-secret (empty by default, wandb is optional)
    # For production with WandB, override via --set flags:
    # --set customizer.wandb.apiKey=<your-api-key> --set customizer.wandb.encryptionKeyValue=<your-encryption-key>
    apiKey: ""
    encryptionKeyValue: ""
  otel:
    enabled: true
    # exporterOtlpEndpoint will be templated in the template file
  database:
    # host will be templated in the template file
    port: 5432
    databaseName: ncsdb
    user: ncsuser
    secretName: customizer-pg-existing-secret
    passwordKey: password
  modelPVC:
    create: true
    name: finetuning-ms-models-pvc
    size: 50Gi
  workspacePVC:
    storageClass: "gp3-csi"  # Changed from "local-path" to match cluster storage class
    size: 10Gi
    mountPath: /pvc/workspace
  modelConfig:
    name: nemo-model-config
  trainingConfig:
    name: nemo-training-config
  ngcAPISecret:
    name: ngc-api-secret
    key: "NGC_API_KEY"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
    - key: "g5-gpu"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
  # Resource requests/limits for training job pods
  # Ephemeral storage is critical - the customizer image is ~62GB
  resources:
    requests:
      ephemeral-storage: "80Gi"  # Enough for image pull + container filesystem
      memory: "32Gi"
      cpu: "4"
      nvidia.com/gpu: "1"
    limits:
      ephemeral-storage: "100Gi"  # Upper limit to prevent node issues
      memory: "64Gi"
      cpu: "8"
      nvidia.com/gpu: "1"

# ===== NemoDatastore Configuration =====
# Image: nvcr.io/nvidia/nemo-microservices/datastore:25.08 (exact from quickstart)
datastore:
  enabled: true
  name: nemodatastore-sample
  image:
    repository: nvcr.io/nvidia/nemo-microservices/datastore
    tag: "25.08"
    pullPolicy: IfNotPresent
  database:
    # host will be templated in the template file
    port: 5432
    databaseName: ndsdb
    user: ndsuser
    secretName: datastore-pg-existing-secret
    passwordKey: password
  pvc:
    name: "pvc-shared-data"
    create: true
    size: "10Gi"
  replicas: 1
  resources:
    requests:
      memory: "256Mi"
      cpu: "500m"
    limits:
      memory: "512Mi"
      cpu: "1"
  secrets:
    datastoreConfigSecret: "nemo-ms-nemo-datastore"
    datastoreInitSecret: "nemo-ms-nemo-datastore-init"
    datastoreInlineConfigSecret: "nemo-ms-nemo-datastore-inline-config"
    giteaAdminSecret: "gitea-admin-credentials"
    lfsJwtSecret: "nemo-ms-nemo-datastore--lfs-jwt"
  # Gitea admin credentials (DEV/TEST ONLY - override for production)
  # These defaults match the quickstart for development/testing environments
  # For production: Override via --set or environment variables
  gitea:
    adminUsername: "datastore_admin"  # DEV DEFAULT - override for production
    adminPassword: "s3aJPHD9!bt6d0I"  # DEV DEFAULT - override for production
  # LFS JWT secret (DEV/TEST ONLY - override for production)
  # Default matches quickstart for development/testing
  # For production: Generate with: openssl rand -base64 32 | base64
  lfsJwtSecret: "VjNwcWQxSlViMnR5YkRSbFVuSjZlVUpTY0d0R2IxSldUM0UyZWpsUGRuUQ=="  # DEV DEFAULT - override for production

# ===== NemoEntitystore Configuration =====
# Image: nvcr.io/nvidia/nemo-microservices/entity-store:25.08 (exact from quickstart)
entitystore:
  enabled: true
  name: nemoentitystore-sample
  image:
    repository: nvcr.io/nvidia/nemo-microservices/entity-store
    tag: "25.08"
    pullPolicy: IfNotPresent
  database:
    # host will be templated in the template file
    port: 5432
    databaseName: nesdb
    user: nesuser
    secretName: entity-store-pg-existing-secret
    passwordKey: password

# ===== NemoEvaluator Configuration =====
# Image: nvcr.io/nvidia/nemo-microservices/evaluator:25.06 (exact from quickstart)
evaluator:
  enabled: true
  name: nemoevaluator-sample
  image:
    repository: nvcr.io/nvidia/nemo-microservices/evaluator
    tag: "25.06"
    pullPolicy: IfNotPresent
  evaluationImages:
    bigcodeEvalHarness: "nvcr.io/nvidia/nemo-microservices/eval-tool-benchmark-bigcode:0.12.21"
    lmEvalHarness: "nvcr.io/nvidia/nemo-microservices/eval-tool-benchmark-lm-eval-harness:0.12.21"
    similarityMetrics: "nvcr.io/nvidia/nemo-microservices/eval-tool-benchmark-custom-eval:0.12.21"
    llmAsJudge: "nvcr.io/nvidia/nemo-microservices/eval-tool-benchmark-llm-as-a-judge:0.12.21"
    mtBench: "nvcr.io/nvidia/nemo-microservices/eval-tool-benchmark-llm-as-a-judge:0.12.21"
    retriever: "nvcr.io/nvidia/nemo-microservices/eval-tool-benchmark-retriever:0.12.21"
    rag: "nvcr.io/nvidia/nemo-microservices/eval-tool-benchmark-rag:0.12.21"
    bfcl: "nvcr.io/nvidia/nemo-microservices/eval-factory-benchmark-bfcl:25.6.1"
    agenticEval: "nvcr.io/nvidia/nemo-microservices/eval-factory-benchmark-agentic-eval:25.6.1"
  argoWorkflows:
    # endpoint will be templated in the template file
    serviceAccount: argo-workflows-executor
  vectorDB:
    # endpoint will be templated in the template file
  database:
    # host will be templated in the template file
    port: 5432
    databaseName: evaldb
    user: evaluser
    secretName: evaluator-pg-existing-secret
    passwordKey: password
  otel:
    enabled: true
    # exporterOtlpEndpoint will be templated in the template file
  # Environment variables for the evaluator pod
  # NIM_PROXY_URL points to the NIM service for evaluation jobs
  # Configure via evaluator.nimProxy section below (for NIM deployed via RHOAI/OpenShift AI)
  env:
    - name: NIM_PROXY_URL
      # Will be templated in nemoevaluator.yaml using evaluator.nimProxy configuration
      value: ""  # Will be templated in nemoevaluator.yaml
  # NIM Proxy configuration (for NIM deployed via RHOAI/OpenShift AI KServe InferenceService)
  # If not set, defaults to meta-llama3-1b-instruct service
  nimProxy:
    # Service name (e.g., meta-llama3-1b-instruct-predictor for KServe InferenceService)
    serviceName: ""
    # Service suffix (e.g., "-predictor" for KServe InferenceService)
    serviceSuffix: ""
    # Namespace (defaults to namespace.name if not set)
    namespace: ""
    # Path (e.g., "/v1" for KServe InferenceService)
    path: ""
  replicas: 1

# ===== NemoGuardrail Configuration =====
# Image: nvcr.io/nvidia/nemo-microservices/guardrails:25.08 (exact from quickstart)
guardrail:
  enabled: true
  name: nemoguardrails-sample
  image:
    repository: nvcr.io/nvidia/nemo-microservices/guardrails
    tag: "25.08"
    pullPolicy: IfNotPresent
  database:
    # host will be templated in the template file
    port: 5432
    databaseName: guardraildb
    user: guardrailuser
    secretName: guardrail-pg-existing-secret
    passwordKey: password
  configStorePVC:
    name: "pvc-guardrail-config"
    create: true
    size: "1Gi"
  # NIM endpoint configuration (for NIM deployed via RHOAI/OpenShift AI KServe InferenceService)
  # If not set, defaults to meta-llama3-1b-instruct service
  nimEndpoint:
    # Service name (e.g., meta-llama3-1b-instruct-predictor for KServe InferenceService)
    serviceName: ""
    # Service suffix (e.g., "-predictor" for KServe InferenceService)
    serviceSuffix: ""
    # Namespace (defaults to namespace.name if not set)
    namespace: ""
    # Port (defaults to 80 for KServe InferenceService)
    port: ""
  replicas: 1
  resources:
    limits:
      cpu: "1"
      ephemeral-storage: 10Gi

# ===== NIMCache Configuration =====
# REMOVED: Chat model NIMCache is no longer deployed via Helm chart
# Chat models (e.g., meta/llama-3.2-1b-instruct) are now deployed via RHOAI/OpenShift AI 
# NIM Serving Runtime Model (KServe InferenceService), which handles model caching internally.
# Only embedding model NIMCache (nimCacheEmbedding) is deployed via Helm chart.
# If you need to deploy chat model NIMCache via Helm, configure it separately.

# ===== NIMPipeline Configuration =====
# REMOVED: NIM Pipeline is no longer deployed via Helm chart
# Models are now deployed via RHOAI/OpenShift AI NIM Serving Runtime Model (KServe InferenceService)
# If you need to deploy NIM models via Helm, use the embedding pipeline (nimPipelineEmbedding) or
# configure your own NIMPipeline resources separately.

# ===== NIMCache Embedding Configuration =====
# Model: nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2:1.3.1 (embedding model for RAG)
# Using official model from: deploy-on-openshift/config/samples/nim/pipelines/rag/nimcache-embedding.yaml
# This model supports NIMCache properly (has model_manifest.yaml)
nimCacheEmbedding:
  enabled: true
  name: nv-embedqa-1b-v2
  modelPuller: nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2:1.3.1
  engine: tensorrt  # Embedding models use tensorrt, not tensorrt_llm
  # Note: tensorParallelism is NOT used for embedding models (only for LLM with tensorrt_llm)
  pullSecret: ngc-secret
  authSecret: ngc-api-secret
  # OpenShift-compatible UIDs (required for nonroot SCC)
  # Range: 1000790000-1000799999
  userID: 1000790000
  groupID: 1000790000
  pvc:
    create: true
    size: "50Gi"  # Official sample uses 50Gi
  tolerations:
    - key: "g5-gpu"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"

# ===== NIMPipeline Embedding Configuration =====
# Model: nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2:1.3.1 (embedding model for RAG)
# Using official model that supports NIMCache
nimPipelineEmbedding:
  enabled: true
  name: embedding-1b-v2-pipeline
  serviceName: nv-embedqa-1b-v2
  image:
    repository: nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2
    tag: 1.3.1
    pullPolicy: IfNotPresent
  pullSecret: ngc-secret
  authSecret: ngc-api-secret
  nimCacheName: nv-embedqa-1b-v2
  replicas: 1
  resources:
    limits:
      gpu: 1
  tolerations:
    - key: "g5-gpu"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"

# ===== LlamaStack Configuration =====
# LlamaStack provides a unified abstraction layer over NeMo Microservices
# Image: quay.io/ecosystem-appeng/llamastack-server-distribution:latest
llamastack:
  enabled: true
  name: llamastack
  configMapName: llamastack-config
  image:
    repository: quay.io/ecosystem-appeng/llamastack-server-distribution
    tag: "latest"
    pullPolicy: Always
  pullSecret: ngc-secret
  ngcAPISecret:
    name: ngc-api-secret
    key: "NGC_API_KEY"
  replicas: 1
  port: 8321
  strategy:
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 0
  # LlamaStack configuration
  datasetNamespace: "nvidia-e2e-tutorial"
  projectId: "llamastack-project"
  guardrailsConfigId: "demo-self-check-input-output"
  # Inference model identifier (optional - defaults to a common model name)
  # This is used by LlamaStack for model identification in its API
  # If not specified, defaults to "meta/llama-3.2-1b-instruct"
  # Only override if you need a different model identifier
  inferenceModel: "meta/llama-3.2-1b-instruct"
  # Safety model identifier (for guardrails)
  safetyModel: "meta/llama-3.2-1b-instruct"
  outputModelDir: "nvidia-e2e-tutorial/test-messages-model@v1"
  # REQUIRED: KServe InferenceService name (without -predictor suffix)
  # This is the name of your InferenceService resource
  # Example: anemo-rhoai-model1
  # The template will automatically construct:
  #   - Service account name: <inferenceServiceName>-sa
  #   - Predictor service name: <inferenceServiceName>-predictor
  # Example: If inferenceServiceName is "anemo-rhoai-model1", then:
  #   - serviceAccountName will be "anemo-rhoai-model1-sa"
  #   - inferenceServicePredictor will be "anemo-rhoai-model1-predictor"
  inferenceServiceName: "anemo-rhoai-model1"
  # InferenceService namespace (optional)
  # If not set, defaults to the main namespace (namespace.name)
  # Only set this if your InferenceService is in a different namespace
  inferenceServiceNamespace: ""
  # Service account name for LlamaStack (typically created by InferenceService)
  # Format: <inferenceservice-name>-sa
  # If not set, will be auto-constructed from inferenceServiceName as: <inferenceServiceName>-sa
  # Only override this if your service account has a different naming convention
  serviceAccountName: ""
  # Whether to create the service account (set to false if InferenceService creates it)
  # Default: false (InferenceService typically creates the service account)
  createServiceAccount: false
  # KServe InferenceService predictor service name (cluster-internal)
  # This is the stable service name that routes to the latest revision
  # Format: <inferenceservice-name>-predictor
  # If not set, will be auto-constructed from inferenceServiceName as: <inferenceServiceName>-predictor
  # Only override this if your service has a different naming convention
  inferenceServicePredictor: ""
  # Direct override for NVIDIA_BASE_URL (optional, advanced)
  # If set, this will be used instead of auto-constructing from inferenceServicePredictor
  # Format: http://<service-name>.<namespace>.svc.cluster.local:<port>
  # Example: http://my-model-predictor.my-namespace.svc.cluster.local:80
  # If not set, will be auto-constructed from inferenceServicePredictor and namespace
  nvidiaBaseURL: ""
  # Use Bearer token authentication instead of API key for KServe InferenceService
  # Set to true when using KServe InferenceService that requires service account token
  useBearerToken: true
  resources:
    requests:
      memory: "512Mi"
      cpu: "500m"
    limits:
      memory: "1Gi"
      cpu: "1"

# ===== NeMo Operator Subchart =====
nemo-operator:
  # NeMo Operator chart configuration
  # Following quickstart Step 4 (lines 114-133):
  # - Helm install with resource limits (lines 122-126)
  # - ServiceAccount configured with ngc-secret from the start (no patching needed)
  # 
  # Prerequisites (quickstart lines 43-47):
  # - ngc-secret must be created manually BEFORE installation:
  #   oc create secret docker-registry ngc-secret \
  #     --docker-server=nvcr.io \
  #     --docker-username='$oauthtoken' \
  #     --docker-password=$NGC_API_KEY \
  #     -n <namespace>
  
  # Image pull secrets configuration (uses ngc-secret directly, no patching required)
  imagePullSecrets:
    - name: ngc-secret
  
  # Manager resource configuration (exact from quickstart lines 124-125)
  manager:
    resources:
      limits:
        memory: 512Mi
      requests:
        memory: 256Mi

# ===== NIM Operator Subchart =====
nim-operator:
  # NIM Operator chart configuration
  # Following quickstart Step 5 (lines 137-150):
  # - Uses local OpenShift-specific Helm chart (NOT the official NVIDIA repo chart)
  # - Image: ghcr.io/nvidia/k8s-nim-operator:release-3.0 (exact from chart values.yaml)
  # - Resource limits: 512Mi limit, 256Mi request (exact from quickstart lines 143-144)
  # 
  # CRITICAL: OpenShift requires the local Helm chart from deploy-on-openshift/deployments/helm/k8s-nim-operator
  # The official NVIDIA Helm repository chart (nvidia/k8s-nim-operator) is NOT compatible with OpenShift
  # due to security context and permission differences (quickstart line 150)
  
  # Operator resource configuration (exact from quickstart lines 143-144)
  operator:
    resources:
      limits:
        memory: 512Mi
      requests:
        memory: 256Mi
    # Image configuration (exact from chart values.yaml)
    image:
      repository: ghcr.io/nvidia/k8s-nim-operator
      tag: release-3.0
      pullPolicy: Always

